\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, se puede decir que esta tesis ha englobado dos vertientes principales dada su naturaleza dual. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado dataset. Y por otro lado, de forma más extensa, se han realizado las tareas de investigación e implementación necesarias para poder llevar estos métodos teóricos a la práctica. Se ha investigado sobre tres campos diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada uno de ellos se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado. Asimismo, se hace un especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico, como el mercado empresarial.

En esta tesis se puede apreciar el hecho de que contar con una sólida teoría basada en la lógica y las matemáticas concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. De esta forma se ha comprobado que existe una gran cantidad de información implícita en los datos que se suelen utilizar. El descubrimiento de toda esta información y su gestión inteligente es sin duda un claro tema de investigación con fuerte actividad y repercusión en la actualidad. Esta ha sido la intención principal a la hora de trabajar con FCA, los conjuntos de implicaciones y los operadores de cierre. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos tanto de esta tesis como lo es para el propio FCA si se pretende que se convierta en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Antes de entrar plenamente en el apartado de conclusiones, es necesario indicar la manera de certificar la validez de los resultados obtenidos a lo largo de la tesis. Como se ha mostrado en los capítulos anteriores, la labor de investigación se ha centrado en actuar sobre conjuntos de implicaciones, y en ese sentido, para los experimentos realizados se ha contado con unos ficheros de entrada que contenían la información necesaria, y sobre ellos se han obtenido unos resultados. 

Ahora bien, la forma de verificar que esos resultados son correctos es la siguiente. En primer lugar y con respecto a los resultados de claves y generadores minimales, se han realizado y corregido numerosos ejercicios en papel intentando buscar casos límites donde la implementación pudiera no ser precisa y se ha comprobado que los resultados obtenidos en papel coincidían exactamente con los calculados por las implementaciones en la máquina. Además, dado que para muchos de los ejemplos probados en los que se llegaban a calcular millones de nodos de un árbol no era posible comprobar si cada uno de esos cálculos era correcto, para el caso concreto de los experimentos relacionados con claves minimales, la validez de los experimentos viene dada al haber cotejado los resultados con aquellos obtenidos sobre un amplio abanico de ficheros utilizados en trabajos anteriores \cite{Benito-Picazo2013PFC,Benito-Picazo2014TFM} donde su validez quedó demostrada. Básicamente, la validez de los ejercicios más grandes se ha extrapolado de los resultados correctos obtenidos para los ejercicios más pequeños. Pero además, los resultados se corroboran igualmente al alcanzar las mismas soluciones para diferentes métodos cuando cada uno de ellos hace un tratamiento de la información diferente con respecto al otro. En relación a los experimentos con SR conversacionales, dado que los experimentos no alcanzan números tan costosos de verificar, la validez puede demostrarse de forma más asequible siguiendo un desarrollo explícito en papel.

Y el otro punto que se quiere destacar es el siguiente. A lo largo de toda la tesis se ha hablado de trabajar sobre sistemas de implicaciones. No obstante, queda fuera del ámbito de esta tesis el procedimiento mediante el cual se obtiene esos elementos para un sistema de datos. Para ello, se recomienda al lector a visitar \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. En el caso particular de la tesis, el cometido comienza con el tratamiento de la información una vez de ha extraído el conjunto de implicaciones de un sistema concreto.

Aclarados estos puntos, se presenta ahora el último capítulo de la tesis, en el cual se recopilan las conclusiones más importantes que alcanzadas como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se introducen como trabajos futuros.


\section*{Conclusiones}
\noindent
Como se ha mostrado anteriormente, conocer las claves es fundamental en cualquier modelo de datos. En este sentido, se ha presentado una serie de métodos que permiten averiguar el conjunto de claves a partir del conjunto de implicaciones y haciendo uso de métodos de razonamiento automatizado, en concreto, la \slfd. Se ha investigado, pasando desde la teoría a la práctica, los diferentes métodos implementados haciendo uso del paradigma de Tableaux y se ha comprobado como, hasta donde se conoce, los resultados obtenidos superan las aproximaciones anteriores.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento y una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Para abordar esta tarea, se han presentado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello se ha hecho un uso intensivo de la lógica \slfde sobre conjuntos de implicaciones. Finalmente, se han creado, analizado y probado enfoques diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras alcanzadas por cada uno.

En ambas situaciones, es decir, tanto para claves minimales como para generadores minimales, se han desarrollado los códigos necesarios para poder actuar sobre grandes cantidades de información. No obstante, para resolver problemas reales donde la cantidad de información de entrada sea considerable, es incuestionable la necesidad de unos recursos enormes tales como los que ha proporcionado el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; sin ellos habría sido inviable haber podido realizar gran parte de las pruebas. De acuerdo con esto, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de recursos de memoria. Incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse sustancialmente.

Asimismo, el parecido y la cualidad de independencia que tiene cada nodo del árbol tanto para los métodos de claves minimales como para los de generadores minimales, ponen de manifiesto que la utilización de estrategias paralelas sea la mejor opción para abordar estos problemas. No obstante, el primer punto que se desea aclarar es que el concepto de paralelismo que se ha utilizado se refiere a un paralelismo de tipo \textit{hardware}. Esto se refiere a que los beneficios obtenidos del paralelismo se deben a la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, no estamos ante un caso de desarrollo de código paralelo desde una visión más purista, sino que es más acertado considerarlo como una aplicación basada en una estrategia \textit{MapReduce}\cite{Dean2004}.

Para la mayoría de los casos, en relación a claves y generadores minimales, existe un serio inconveniente. En primera instancia es prácticamente imposible, por el momento, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a obligar a realizar una serie de experimentos previos de forma que se pueda aproximar las necesidades que va a tener un determinado experimento. 

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que permitieran evaluar el rendimiento de las pruebas de forma que se pudieran comparar unos métodos con otros. En el caso de los experimentos relacionados con claves y generadores minimales, cuando se plantea la idea de la comparación de resultados, lo primero que se pensó fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, se advirtió que este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar magnitud del árbol y la cantidad de resultados redundantes que se obtienen. De esta forma, si alguien hiciera otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre se puede atender al tamaño del árbol pudiendo defender si realmente es una mejora en el método o en la ejecución debido a la arquitectura.

En cuanto a los SR conversacionales, también son varias las conclusiones alcanzadas. La más importante es que, efectivamente, el tratamiento de la información realizado por medio de implicaciones y la lógica \slfde puede aplicarse con éxito en este campo de conocimiento. Esto ya quedaba patente por la existencia de multitud de trabajos en la literatura de SR que utilizan conceptos de FCA; el trabajo en esta tesis viene a reforzar este hecho.

Concretamente, se presenta una novedosa aplicación del algoritmo de cierre \cierree para afrontar el problema de la sobrecarga de la información que a menudo presentan los SR. La solución propuesta propone un proceso conversacional de selección de atributos por parte del usuario. Este trabajo combina características de sistemas basados en conteido con sistemas badados en conocimiento mediante una gestión inteligente de las implicaciones a través del cierre \cierre. El sistema constituye un marco que puede integrarse en diversos datasets y los resultados siguen siendo admisibles. Esto es, ciertamente, una gran contribución de este trabajo, pues ofrece la posibilidad de aplicarse a aplicaciones en diferentes áreas.

Otra conclusión importante es que existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR para evitar los diferentes problemas que pueden aparecer, por tanto, hay que tener presente con qué expectativas queremos que actúe. Además, hay que tener en cuenta que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación; lo habitual es que sean sistemas híbridos con la intención de poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. La propuesta de esta tesis es un claro ejemplo; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional.

Por otro lado, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esta es una conclusión razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SR es igualmente alto. Es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que queramos evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. En consecuencia, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, aplicarle las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Finalmente, se quiere remarcar que el sistema desarrollado es capaz de ir más allá del ámbito académico o de investigación. Las pruebas realizadas sobre \textit{datasets} reales demuestran la viabilidad y la utilidad que el sistema puede aportar en entornos empresariales. De esta forma, se apuesta de nuevo por una eficaz transmisión de conocimiento que acerque la empresa a la academia.



\section*{Trabajos Futuros}
\noindent
Finalmente, los resultados obtenidos motivan una serie de direcciones importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En primera instancia es prácticamente imposible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a constituir en la mayoría de los casos un serio inconveniente.

Tanto en claves minimales como en generadores minimales se ha podido apreciar que existe un elemento fundamental en el diseño de las implementaciones paralelas: el valor de corte o BOV. Recordemos que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. Aparece entonces el hecho de que establecer un valor de corte adecuado se convierte en una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el Tableaux del experimento que estemos realizando y esto sólo se puede averiguar empíricamente. En definitiva, hay que buscar estrategias para encontrar un valor de corte tal que el aprovechamiento de los recursos computacionales sea óptimo.

Es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se puedan dirigir las búsquedas de forma más eficiente.

Hay que profundizar en el hecho de que aumentar el número de cores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}.

La intención en el futuro es aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda valorar el rendimiento alcanzado gracias al paralelismo.

Otra tarea por la que continuar el trabajo consiste en investigar cómo realizar un nuevo diseño de los códigos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo es muy importante, pues serviría para obtener una versión paralela del mejor método secuencial estudiado (GenMinGen). 

En relación a los SR conversacionales, aparecen dos aspectos interesantes a tener en cuenta en el futuro próximo. En primer lugar, sería muy valioso poder identificar desde un primer momento qué elementos del dataset presentan una importancia superior entre los demás, ya sea por aparecer con mayor frecuencia, ser único, tener relación con un mayor número de elementos del conjunto, etc. En segundo lugar y siguiendo la misma idea, sería sin duda crucial saber qué características del dataset (tamaño, escasez, etc.) son las más influyentes en el rumbo que toman las conversaciones con el usuario. Tener un mayor conocimiento de estos dos factores permitiría poder influir de manera positiva en la conversación del sistema con el usuario y de esta forma, deparar en una mejor experiencia de usuario.

Asimismo, hay aspectos de los SR que sería recomendable investigar si sería acertado incluir en el sistema conversacional; tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario ha recibido. Este es un aspecto importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}.


%-BOV
%-becnhmark
%-no saber de principio nada
%-real data sets
%-numer of cores
%-sw and hw para paralelismo sw
%-calculos redundantes
%
%-Trying to discover which characteristics concerning the dataset (dimensionality, sparsity, etc.) are relevant
%-identifying elements within the dataset, which play a more significant role









