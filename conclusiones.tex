\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, se puede decir que esta tesis, dada su naturaleza dual, ha conllevado dos grandes grupos de tareas. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado conjunto de datos. Y por otro lado, se han realizado las tareas de investigación e implementación necesarias para poder contrastar la validez de estos métodos teóricos en la práctica. 

Se ha investigado sobre tres áreas diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada una de ellas se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado. Asimismo, se hace un especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico, como el mercado empresarial.

A lo largo de la tesis se puede apreciar el hecho de que contar con una sólida teoría basada en la Lógica y las Matemáticas concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. De esta forma, se ha comprobado que existe una gran cantidad de información implícita en los datos que se suelen utilizar en dichas aplicaciones. El descubrimiento de toda esta información y su gestión inteligente es sin duda una clara oportunidad de investigación con una fuerte actividad y repercusión en la actualidad. Esta ha sido la intención principal a la hora de trabajar con FCA, los conjuntos de implicaciones y los conjuntos cerrados. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos de esta tesis al pretender que estos conceptos se conviertan en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Se alcanza ahora el último capítulo de la tesis, en el cual se recopilan las conclusiones más importantes alcanzadas como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se presentan como trabajos futuros.


\section*{Conclusiones}
\noindent
En primer lugar, y dada la relación manifiesta en términos de algoritmos y computación paralela, se muestran las conclusiones referentes a claves y generadores minimales. A continuación y para terminar la sección, se muestran aquellas conclusiones obtenidas en torno a SR conversacionales.

Como se ha mencionado con anterioridad, conocer las claves es una tarea crucial para muchas áreas de gestión de la información. Se recuerda que una clave es un conjunto de atributos de un esquema relacional que nos permite distinguir inequívocamente cada objeto del conjunto de datos. El problema surge debido a la complejidad exponencial que conlleva averiguar estas claves a partir de un conjunto de FD que se cumplen en un esquema del modelo relacional \cite{Lucchesi78}.

Para abordar este problema, a partir de los métodos que los autores presentaron en \cite{Cordero2013}, se han diseñado una serie de nuevos métodos que permiten averiguar el conjunto de claves a partir del conjunto de implicaciones haciendo uso de métodos de razonamiento automático basados en la lógica \slfd. Se ha investigado e implementado, pasando desde la teoría a la práctica, los diferentes métodos basados en el paradigma de tableaux, y se ha verificado como, hasta donde se ha podido comprobar, los resultados obtenidos superan las aproximaciones anteriores reduciendo tres aspectos fundamentales: el tiempo de cómputo, los cálculos redundantes y el tamaño del problema \cite{Benito-Picazo2016}. Además, en ese trabajo se manifiesta como la introducción del paralelismo y los recursos de supercomputación han permitido que las limitaciones que se encontraban en el trabajo original \cite{Cordero2013} hayan podido solventarse, abriendo la puerta a poder trabajar con cantidades mayores de información.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento y una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Junto con los conjuntos cerrados, los generadores minimales, son esenciales
para obtener una representación completa del conocimiento en FCA \cite{Poelmans2013}.

El punto de partida para trabajar sobre generadores minimales en esta tesis ha sido el método presentado en \cite{Cordero2012}, donde se utilizó la lógica \slfde\index{\slfde} como herramienta para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. La propuesta que se ha realizado ha consistido en el diseño y la implementación de métodos que nos permitan identificar los generadores minimales como representaciones canónicas de cada conjunto cerrado para un conjunto de datos. 

Desafortunadamente, la dificultad que aparece al utilizar estos métodos es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial.

Con la intención de afrontar esta tarea, se han diseñado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello, se ha hecho un uso intensivo de la lógica \slfde sobre conjuntos de implicaciones. Finalmente, se han diseñado, analizado y probado algoritmos diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras aportadas por cada uno. Así, se han alcanzado reducciones de más del 50\% en el número de nodos del árbol de búsqueda que dibuja el método más básico, MinGen, frente al resto, MinGenPr y GenMinGen, como se puede ver en \cite{Benito-Picazo2018}.

Asimismo, la utilización de estrategias paralelas se alzan como la mejor alternativa en la resolución tanto de los métodos de claves minimales como de generadores minimales. Este hecho se sustenta en la cualidad de independencia que tiene cada nodo del árbol del tableaux que permite que puedan ser tratados de forma independiente.  

No obstante, el primer punto que es necesario aclarar es que el concepto de \textit{paralelismo} que se ha utilizado se refiere a un paralelismo de tipo \textit{hardware}. Esto quiere decir que el tipo de paralelismo que se ha aplicado ha consistido en la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, estas implementaciones no son un caso de desarrollo de código paralelo desde una visión más purista de programación, sino que es más acertado considerarlas como aplicaciones basadas en una estrategia \textit{MapReduce}\cite{Dean2004}, que se ejecutan de forma paralela con la ayuda de recursos \textit{hardware}.

Al igual que para el problema de las claves minimales, se ha desarrollado el código necesario para poder trabajar con grandes cantidades de información para identificar los generadores minimales. Para resolver los problemas de tiempo de ejecución cuando la cantidad de información de entrada sea considerable, el desarrollo se ha optimizado para ejecuciones que sean capaces de aprovechar grandes recursos computacionales, tales como los proporcionados por el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; gracias a ellos ha sido viable realizar la gran mayoría de las pruebas. Aun contando con dichos recursos, se ha llegado a la conclusión de que, para ambos casos, claves y generadores minimales, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de los recursos de memoria; incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse sustancialmente.

No obstante, para la mayoría de los casos, existe un serio inconveniente. En primera instancia es prácticamente imposible, por el momento, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a obligar a realizar una serie de pruebas previas de forma que se pueda aproximar las necesidades que va a tener un determinado experimento (véase \cite{Benito-Picazo2014TFM}). 

Para evaluar las implementaciones realizadas no es suficiente con la comparación de los tiempos de ejecución de los métodos ya que este parámetro va a venir condicionado por la arquitectura \textit{hardware} utilizada para llevar a cabo los experimentos. En este sentido, es necesario utilizar métricas adicionales que reflejen el tamaño del problema o la cantidad de cálculo redundante que se lleva a cabo como muestran los experimentos presentados en \cite{Benito-Picazo2014,Benito-Picazo2013PFC,Benito-Picazo2014TFM}.

En cuanto a los SR conversacionales, el trabajo realizado se ha enfocado en el uso de implicaciones y la Lógica para subsanar determinados problemas que aparecen en este tipo de SR. En concreto, se ha abordado el denominado problema de la dimensionalidad que surge cuando la información sobre la que trabaja el SR contiene una cantidad muy elevada de características que dificultad la interacción del sistema para con el usuario.

En este campo también son varias las conclusiones alcanzadas. La más importante es que, efectivamente, el tratamiento de la información realizado por medio de implicaciones y la lógica \slfde puede aplicarse con éxito al campo de los SR. Este hecho ya quedaba auspiciado por la existencia de trabajos en la literatura de SR que utilizan conceptos de FCA \cite{Senatore2013,LeivaERCMG13,LeivaERCMG13a,Zou2017}; el trabajo en esta tesis refuerza este hecho, en concreto para los SR conversacionales, proponiendo nuevos métodos con los que abordar problemas comunes de esos sistemas y mejorar las aproximaciones existentes.

Concretamente, se ha aportado una novedosa aplicación del algoritmo del cierre \cierree para afrontar el problema de la sobrecarga de la información. La solución propuesta propone un proceso conversacional de selección de elementos por parte del usuario a partir de los atributos que contengan. Este trabajo combina además características de sistemas basados en contenido con sistemas basados en conocimiento mediante una gestión inteligente de las implicaciones teniendo como base el cierre \cierre.

%Otra conclusión es que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación, lo habitual es que sean sistemas híbridos buscando poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. %La propuesta de esta tesis es un claro ejemplo de ello; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional, consiguiendo una beneficiosa sinergia.

%existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, se ha comprobado que es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR con la intención de evitar los diferentes problemas que pueden aparecer; es por tanto crucial, tener presente qué expectativas se pretenden cumplir. Del mismo modo, hay que tener en cuenta que 

Por otro lado, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esto es razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SR es igualmente alto. Por tanto, es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que se desee evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. 

Dada la naturaleza del sistema desarrollado, las métricas que se han utilizado para evaluar el rendimiento están directamente relacionadas con el proceso de diálogo, como son el número de pasos de la conversación o el filtrado de atributos que se produce. Estas métricas devuelven resultados muy prometedores en ambos casos: 
\begin{enumerate}
	\item En cuanto al número de pasos del diálogo, se puede ver como en gran parte de los experimentos realizados, la conversación necesita menos de 3 ó 4 pasos para alcanzar recomendaciones adecuadas aun cuando el número total de atributos sea considerable (más de 30 atributos). Esta afirmación se puede observar en \cite{Benito-Picazo2017}, donde se trabaja con 100 atributos diferentes y la mayoría de las conversaciones finalizan en 2 ó 3 pasos. 
	\item En cuanto al filtrado de atributos, los resultados son igualmente notables. Las pruebas mostradas en ese mismo artículo, demuestran cómo el uso del sistema conversacional ha evitado que el usuario tenga que interactuar a lo largo del diálogo con cerca del 5-20\% de atributos disponibles en la mayoría de los casos, reduciendo de esta forma la sobrecarga de información y mejorando la experiencia de usuario.
\end{enumerate}

Estos resultados superan con creces estudios previos como \cite{TrabelsiWBR11} donde, incluso contando con un menor número de atributos, se puede ver que por un lado, el número de pasos necesarios es mayor, y por otro, que la reducción del número de atributos en la conversación es menor.
  

%En consecuencia, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas que existen. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, discernir las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Para finalizar, se quiere destacar que las aplicaciones y métodos desarrollados para los tres campos son capaces de ir más allá del ámbito académico o de investigación. Las pruebas realizadas sobre datasets reales demuestran la viabilidad y la utilidad que las propuestas pueden aportar en entornos empresariales. Como muestras de ello, se pueden considerar como referencias los experimentos satisfactorios llevados a cabo sobre datos reales, como el caso de MovieLens y los repositorios de la UCI\footnote{Universidad de California, Irvine (https://archive.ics.uci.edu/ml/index.php)} para claves y generadores minimales, o el caso de la información real sobre enfermedades y fenotipos extraída de HPO\footnote{Human Phenotype Ontology Consortium (https://hpo.jax.org/app/)} y OMIM\footnote{Online Mendelian Inheritance in Man (https://https://www.omim.org)}.De esta forma, en el caso de los sistemas de recomendación conversacional, se apuesta por una eficaz transmisión de conocimiento que acerque la empresa a la academia.



\section*{Trabajos Futuros}
\noindent
Los resultados obtenidos motivan una serie de líneas de trabajo importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En el momento actual no es posible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada, en términos de tiempo de cómputo y de recursos computacionales. Esto va a constituir en la mayoría de los casos un serio inconveniente ya que, para la realización de experimentos, esta circunstancia no permite adelantar los requisitos de tiempo y recursos computacionales que serán necesarios. A pesar de ello, gracias a la gran cantidad de pruebas realizadas, es posible vislumbrar ciertas características, como el cardinal de la premisa o la conclusión en la implicación, que suelen vaticinar patrones de comportamiento similares. En este sentido, el trabajo que se está llevando a cabo es continuar con el diseño y realización de nuevas baterías de pruebas modificando esas características mencionadas, con la intención de poder identificar patrones que pronostiquen la complejidad que alcanzará una determinada prueba.

Otro camino muy importante para continuar la investigación tanto para claves minimales como para generadores minimales es ahondar en la optimización del valor de corte o BOV. Recuérdese que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. El hecho de establecer un valor de corte adecuado es una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el tableaux del experimento que se esté realizando y esto sólo se puede averiguar empíricamente. En definitiva, hay que buscar estrategias para encontrar un valor de corte tal que el aprovechamiento de los recursos computacionales sea óptimo. Es este sentido, se están elaborando baterías de pruebas con las que se están investigando diferentes situaciones (casos límite, casos más comunes), de forma que se pueda aproximar el valor más adecuado en cada situación. 

%Como consecuencia de lo anterior, es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se pueda optimizar el funcionamiento de los métodos.

Otra línea de investigación sobre la que se está trabajando busca profundizar en el hecho de que aumentar el número de procesadores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}. En otras palabras, los tiempos de ejecución de los experimentos pueden incrementarse al aumentar los recursos \textit{hardware}. Esta desafortunada circunstancia se debe, generalmente, a que el problema original se disemina de manera excesiva entre los procesadores disponibles provocando que el tiempo requerido por las comunicaciones para alcanzar el resultado final contrarreste la capacidad de cómputo adicional. 

Para abordar este problema, se está trabajando en descubrir aquellas cotas de recursos a partir de las cuales el beneficio decrece. Así, se están llevando a cabo nuevos experimentos generando de manera sistemática pruebas con diferentes caracterizaciones para intentar inferir correlaciones entre características de los datos de entrada y las configuraciones \textit{hardware} necesarias de cara a optimizar el rendimiento. Para ello, se están investigando estrategias de optimización de recursos \textit{hardware} en entornos de HPC \cite{Wienke2016,AlAli2016}.

%También se pretende en el futuro aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda maximizar el rendimiento alcanzado gracias al paralelismo. Para ello, se va a utilizar información de repositorios conocidos, como el comentado de la Universidad de California, sobre los que aplicar los algoritmos, comparar resultados y verificar la viabilidad en tales situaciones reales.

Otra tarea por la que continuar consiste en investigar cómo realizar un nuevo diseño de los algoritmos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo es muy importante y la razón es la siguiente. Como se ha mencionado en el capítulo \ref{cap:introduccion}, el método que mejores resultados obtiene en referencia a número de nodos y tiempos de ejecución es GenMinGen, sin embargo, hay que recordar que, por el momento, es un método secuencial debido al requisito de establecer comunicación entre las diferentes soluciones parciales. Consecuentemente, las nuevas tareas de paralelización que se están llevando a cabo abren la puerta a poder obtener una versión paralela de GenMinGen. Según el curso actual de la investigación, va a ser necesario considerar la aplicación del paralelismo a nivel de \textit{software} y ver cómo puede combinarse con la estrategia actual, centrada en el paralelismo \textit{hardware}. Por tanto, se ha comenzado a investigar 
la literatura sobre este tema \cite{Moraes2016}, además de otros aspectos de más bajo nivel como lenguajes de programación, librerías, etc.

En relación a los SR conversacionales, aparecen varios aspectos interesantes a tener en cuenta en el futuro próximo. 

En primer lugar, sería muy valioso poder identificar desde un primer momento qué objetos del \textit{dataset} presentan una importancia superior entre los demás teniendo en cuenta que aparezcan con mayor frecuencia en las recomendaciones (tienen una mayor popularidad). Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen una menor presencia en las recomendaciones, lo cual puede deberse a diversas circunstancias como: ser más antiguos, estar descritos en menor detalle, etc. Ejemplos de este tipo de características ya están siendo investigadas en el campo de los SR \cite{Nguyen2014,Arnaboldi2016,Bressan2016,Abdollahpouri2017}; el reto consiste en introducirlas en el modelo concreto de SR conversacional propuesto en esta tesis, y para ello, por el momento se está investigando la literatura existente \cite{Llorente2012,Felfernig2015,Salem2014} en aras de encontrar un nexo que permita tener introducir estas características en la propuesta actual.

En segundo lugar y siguiendo la misma idea, sería esencial saber qué características del \textit{dataset} (tamaño, escasez, etc.) son las más influyentes en el rendimiento del proceso de recomendación. Tener un mayor conocimiento de la influencia de dichas características permitiría poder influir de manera positiva en la conversación de numerosas formas. En busca de este objetivo se ha comenzado a investigar en la aplicación de nuevas métricas que ayuden al descubrimiento de estos objetos o características más notables del \textit{dataset}.

%Entre ellas, serviría para incrementar la reducción de los pasos del diálogo o proponer un determinado orden en que se muestran los resultados en la lista de recomendación \cite{Shalom2016}. En definitiva, deparar en una mejor experiencia de usuario. 

Para finalizar, existen más aspectos de los SR que sería recomendable investigar como vía de futuras investigaciones para el sistema conversacional desarrollado. Tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario recibe. Este es un aspecto muy importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}. De hecho, la aceptación de un SR mejora cuando los usuarios comprenden las fortalezas y limitaciones del SR \cite{Papadimitriou2012}. Por todo ello, en este momento se está investigando a partir de la literatura los tres diferentes estilos de explicaciones habituales en los SR actuales como son las basadas en: usuario y sus preferencias, como es el caso de Recommender Widget \cite{Guy2009} en el que los elementos recomendados se eligen de acuerdo con la similitud en la navegación web del usuario \cite{Yulong2011}; en objetos, donde las recomendaciones están basadas en las valoraciones que el usuario ha proporcionado al sistema; en características, donde se utiliza los atributos de los objetos como guía para realizar las recomendaciones \cite{Mooney2000}. 


% =====================================================================
% =====================================================================
% =====================================================================








