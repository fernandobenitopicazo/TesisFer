\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, se puede decir que esta tesis, dada su naturaleza dual, ha conllevado dos grandes grupos de tareas. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado conjunto de datos. Y por otro lado, se han realizado las tareas de investigación e implementación necesarias para poder contrastar la validez de estos métodos teóricos en la práctica. 

Se ha investigado sobre tres áreas diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada una de ellas se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado. Asimismo, se hace un especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico, como el mercado empresarial.

A lo largo de la tesis se puede apreciar el hecho de que contar con una sólida teoría basada en la Lógica y las Matemáticas concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. De esta forma, se ha comprobado que existe una gran cantidad de información implícita en los datos que se suelen utilizar en dichas aplicaciones. El descubrimiento de toda esta información y su gestión inteligente es sin duda una clara oportunidad de investigación con una fuerte actividad y repercusión en la actualidad. Esta ha sido la intención principal a la hora de trabajar con FCA, los conjuntos de implicaciones y los conjuntos cerrados. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos de esta tesis al pretender que estos conceptos se conviertan en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Se alcanza ahora el último capítulo de la tesis, en el cual se recopilan las conclusiones más importantes alcanzadas como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se presentan como trabajos futuros.


\section*{Conclusiones}
\noindent
Como se ha mostrado anteriormente, conocer las claves es una tarea crucial para muchas áreas de gestión de la información. Se recuerda que un clave es un conjunto de atributos de un esquema relacional que nos permite distinguir inequívocamente cada objeto del conjunto de datos. El problema surge a la hora de averiguar estas claves a partir de un conjunto de FD que se cumplen en un esquema del modelo relacional.

Para ello, se han presentado una serie de métodos que permiten averiguar el conjunto de claves a partir del conjunto de implicaciones haciendo uso de métodos de razonamiento automático basados en la lógica \slfd. Se ha investigado, pasando desde la teoría a la práctica, los diferentes métodos implementados haciendo uso del paradigma de Tableaux y se ha verificado como, hasta donde se ha podido comprobar, los resultados obtenidos superan las aproximaciones anteriores \cite{Benito-Picazo2016}. Además, en ese trabajo se manifiesta como la introducción del paralelismo y los recursos de supercomputación ha permitido que las limitaciones que se encontraban en trabajos anteriores \cite{Cordero2013} hayan podido solventarse, abriendo la puerta a poder trabajar sobre cantidades mayores de información.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento y una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Junto con los conjuntos cerrados, los generadores minimales, son esenciales
para obtener una representación completa del conocimiento en FCA \cite{Poelmans2013}.

El problema sobre el que se ha trabajado en esta tesis al respecto de los generadores ha consistido en el diseño y la implementación de métodos que nos permitan identificar los generadores minimales como representaciones canónicas de cada conjunto cerrado para un conjunto de datos. 

Para abordar esta tarea, se han presentado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello, se ha hecho un uso intensivo de la lógica \slfde sobre conjuntos de implicaciones. Finalmente, se han diseñado, analizado y comprobado algoritmos diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras aportadas por cada uno. Así, es posible alcanzar reducciones de más del 50\% en el número de nodos del árbol de búsqueda que dibuja el método más básico, MinGen, frente al resto, MinGenPr y GenMinGen, como se puede ver en \cite{Benito-Picazo2018}.

Asimismo, el parecido y la cualidad de independencia que tiene cada nodo del árbol del Tableaux tanto para los métodos de claves minimales como para los de generadores minimales, ponen de manifiesto que la utilización de estrategias paralelas es la mejor opción para abordar estos problemas. No obstante, el primer punto que se desea aclarar es que el concepto de \textit{paralelismo} que se ha utilizado se refiere a un paralelismo de tipo \textit{hardware}. Esto quiere decir que los beneficios obtenidos gracias al paralelismo se deben a la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, estas implementaciones no son un caso de desarrollo de código paralelo desde una visión más purista de programación, sino que es más acertado considerarlas como aplicaciones basadas en una estrategia \textit{MapReduce}\cite{Dean2004}, que se ejecutan de forma paralela con la ayuda de recursos \textit{hardware}.

Al igual que para el problema de las claves minimales, se han desarrollado los códigos necesarios para poder actuar sobre grandes cantidades de información para identificar los generadores minimales. Para resolver estos problemas donde la cantidad de información de entrada sea considerable, el desarrollo se ha optimizado para ejecuciones que sean capaces de aprovechar grandes recursos computacionales, tales como los proporcionados por el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; gracias a ellos ha sido viable realizar la gran mayoría de las pruebas. De acuerdo con esto, para ambos casos, claves y generadores minimales, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de recursos de memoria; incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse sustancialmente.

No obstante, para la mayoría de los casos, existe un serio inconveniente. En primera instancia es prácticamente imposible, por el momento, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a obligar a realizar una serie de pruebas previas de forma que se pueda aproximar las necesidades que va a tener un determinado experimento. 

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que permitieran evaluar el rendimiento de las pruebas de forma que se pudieran comparar unos métodos con otros. En el caso de los experimentos con claves y generadores minimales, cuando se plantea la idea de la comparación de resultados, lo primero que se pensó fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, se advirtió que este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar la magnitud del árbol y la cantidad de resultados redundantes que se obtienen. De esta forma, en el momento de que exista otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre se puede atender al tamaño del árbol y al número de cálculos redundantes, pudiendo defender si realmente es una mejora en el método o bien, en la ejecución debido a la arquitectura.

En cuanto a los SR conversacionales, el trabajo realizado se ha enfocado en el uso de implicaciones y la Lógica para subsanar determinados problemas que aparecen en este tipo de SR. En concreto, se ha abordado el denominado problema de la dimensionalidad que surge cuando la información sobre la que trabaja el SR contiene una cantidad muy elevada de características que dificultad la interacción del sistema para con el usuario.

En este campo también son varias las conclusiones alcanzadas. La más importante es que, efectivamente, el tratamiento de la información realizado por medio de implicaciones y la lógica \slfde puede aplicarse con éxito en este campo de conocimiento. Este hecho ya quedaba patente por la existencia de trabajos en la literatura de SR que utilizan conceptos de FCA; el trabajo en esta tesis refuerza este hecho proponiendo además nuevos métodos con los que abordar problemas comunes en el campo y mejorar las aproximaciones existentes.

Concretamente, se presenta una novedosa aplicación del algoritmo del cierre \cierree para afrontar el problema de la sobrecarga de la información. La solución propuesta propone un proceso conversacional de selección de atributos por parte del usuario. Este trabajo combina características de sistemas basados en contenido con sistemas basados en conocimiento mediante una gestión inteligente de las implicaciones teniendo como base el cierre \cierre.

Otra conclusión es que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación, lo habitual es que sean sistemas híbridos buscando poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. La propuesta de esta tesis es un claro ejemplo de ello; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional, consiguiendo una beneficiosa sinergia.

%existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, se ha comprobado que es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR con la intención de evitar los diferentes problemas que pueden aparecer; es por tanto crucial, tener presente qué expectativas se pretenden cumplir. Del mismo modo, hay que tener en cuenta que 

Por otro lado, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esta es una conclusión razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SR es igualmente alto. Por tanto, es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que se desee evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. 

Dada la naturaleza del sistema desarrollado las métricas que se han utilizado para evaluar el rendimiento están directamente relacionadas con el proceso de diálogo, como son el número de pasos de la conversación o el filtrado de atributos que se produce. Estas métricas devuelven resultados muy prometedores en ambos casos: en cuanto al número de pasos del diálogo, se puede ver como en gran parte de los experimentos realizados, la conversación necesita menos de 3 ó 4 pasos para alcanzar recomendaciones adecuadas \cite{Benito-Picazo2017}; y en cuanto al filtrado de atributos, las pruebas demuestran cómo el sistema conversacional ha evitado al usuario tener que trabajar con cerca del 5-20\% de atributos disponibles en la mayoría de los casos. 

%En consecuencia, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas que existen. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, discernir las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Para finalizar, se quiere destacar que las aplicaciones y métodos desarrollados para los tres campos son capaces de ir más allá del ámbito académico o de investigación. Las pruebas realizadas sobre datasets reales demuestran la viabilidad y la utilidad que las propuestas pueden aportar en entornos empresariales. Como muestras de ello, se pueden considerar como referencias los experimentos satisfactorios llevados a cabo sobre datos reales, como el caso de MovieLens y los repositorios de la UCI\footnote{https://archive.ics.uci.edu/ml/index.php} para claves y generadores minimales, o el caso de la información real sobre enfermedades y fenotipos extraída de HPO\footnote{Human Phenotype Ontology Consortium (https://hpo.jax.org/app/)} y OMIM\footnote{Online Mendelian Inheritance in Man (https://https://www.omim.org)} . De esta forma, se apuesta por una eficaz transmisión de conocimiento que acerque la empresa a la academia.



\section*{Trabajos Futuros}
\noindent
Los resultados obtenidos motivan una serie de líneas de trabajo importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En el momento actual es prácticamente imposible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a constituir en la mayoría de los casos un serio inconveniente. A pesar de ello, gracias a la gran cantidad de pruebas realizadas, sí es posible aventurar que hay ciertas características, como el cardinal de la premisa o la conclusión en la implicación, que suelen vaticinar patrones de comportamiento similares. 

Tanto en claves minimales como en generadores minimales se ha podido apreciar que existe un elemento fundamental en el diseño de las implementaciones paralelas: el valor de corte o BOV. Recuérdese que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. El hecho de establecer un valor de corte adecuado se convierte entonces en una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el Tableaux del experimento que estemos realizando y esto sólo se puede averiguar empíricamente. En definitiva, hay que buscar estrategias para encontrar un valor de corte tal que el aprovechamiento de los recursos computacionales sea óptimo. Es este sentido, se están elaborando baterías de pruebas con las que se están investigando diferentes situaciones (casos más límites, casos más comunes), de forma que se pueda atisbar la forma de acertar con el valor más adecuado en cada situación. 

Como consecuencia de lo anterior, es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se pueda optimizar el funcionamiento de los métodos.

Otra línea de investigación sobre la que se está trabajando busca profundizar en el hecho de que aumentar el número de cores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}. Este hecho se refiere a que los tiempos de ejecución de los experimentos pueden aumentar al aumentar recursos. Esta desafortunada circunstancia se debe, generalmente, a que el problema original se disemina de manera excesiva entre los cores disponibles provocando que el tiempo requerrido por las comunicaciones para alcanzar el resultado final malogren la capacidad de cómputo adicional. Para abordar este problema se está trabajando, a través de nuevos experimentos, en descubrir aquellas cotas de recursos a partir de las cuales el beneficio decrece.

También se pretende en el futuro aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda maximizar el rendimiento alcanzado gracias al paralelismo. Para ello, se va a utilizar información de repositorios conocidos, como el comentado de la Universidad de California, sobre los que aplicar los algoritmos, comparar resultados y verificar la viabilidad en tales situaciones reales.

Otra tarea por la que continuar consiste en investigar cómo realizar un nuevo diseño de los códigos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo es muy importante, pues serviría para obtener una versión paralela del mejor método secuencial estudiado (GenMinGen). Según el curso actual de la investigación, va a ser necesario considerar la aplicación del paralelismo a nivel de \textit{software} y ver cómo puede combinarse con la estrategia actual, centrada en el paralelismo \textit{hardware}. Por tanto, se ha comenzado a investigar aspectos como el lenguaje de programación a utilizar, las librerías necesarias, trabajos anteriores \cite{Moraes2016}, etc.

En relación a los SR conversacionales, aparecen dos aspectos interesantes a tener en cuenta en el futuro próximo. En primer lugar, sería muy valioso poder identificar desde un primer momento qué objetos del dataset presentan una importancia superior entre los demás, ya sea por aparecer con mayor frecuencia en las recomendaciones (tienen una mayor popularidad), tener relación con un mayor número de objetos del conjunto, etc. Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen un menor impacto, bien por ser más antiguos, por estar descritos en menor detalle, etc. Ejemplos de este tipo de características, como por ejemplo la popularidad, ya están siendo investigadas \cite{Arnaboldi2016,Bressan2016}; el reto consiste en introducirlas en el modelo propuesto en esta tesis.

En segundo lugar y siguiendo la misma idea, sería esencial saber qué características del dataset (tamaño, escasez, etc.) son las más influyentes en el camino que toman las conversaciones con el usuario. Tener un mayor conocimiento de estos dos factores permitiría poder influir de manera positiva en la conversación de numerosas formas. Entre ellas, serviría para mejorar la reducción que se hace actualmente de los pasos del diálogo o proponer un determinado orden en que se muestran los resultados en la lista de recomendación \cite{Shalom2016}. En definitiva, deparar en una mejor experiencia de usuario. En busca de este objetivo se ha comenzado a investigar en la aplicación de nuevas métricas que ayuden al descubrimiento de estos objetos o características más notables del dataset.

Para finalizar, existen más aspectos de los SR que sería recomendable investigar como vía de futuras investigaciones para el sistema conversacional desarrollado. Tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario recibe. Este es un aspecto muy importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}. De hecho, la aceptación de un SR mejora cuando los usuarios comprenden las fortalezas y limitaciones del SR \cite{Papadimitriou2012}. Por todo ello, se están investigando los tres diferentes estilos de explicaciones habituales en los SR actuales (basadas en usuario, en objetos, en atributos)\cite{Tintarev2011} y considerando las opciones de cómo incluir esta nueva característica dentro del sistema conversacional. 




% =====================================================================
% =====================================================================
% =====================================================================








