\pagestyle{empty}
\chapter{Introducción}\label{cap:introduccion}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}La gestión de la información es uno de los pilares esenciales de la Ingeniería Informática. No es de extrañar, por tanto, que conforme un amplio campo de investigación y conocimiento donde diversas disciplinas como las Matemáticas, la Lógica y la Computación actúen conjuntamente para alcanzar mejores sinergias.

Dentro de este ámbito y con la intención de hacer aportaciones en campos de la Ingeniería Informática como son las bases de datos\index{bases de datos} y los sistemas de recomendación\index{sistemas de recomendación}, esta tesis doctoral toma como principal base teórica el Análisis Formal de Conceptos \index{Análisis Formal de Conceptos} (FCA, por sus siglas en inglés: \textit{Formal Concept Analysis}), y más concretamente, una de sus herramientas fundamentales: los conjuntos de implicaciones\index{conjuntos de implicaciones}. La gestión inteligente de estos elementos mediante técnicas lógicas y computacionales confieren una alternativa para superar obstáculos en los campos mencionados.

FCA es una teoría matemática y una metodología para derivar una jerarquía de conceptos a partir de una colección de objetos y las relaciones que verifican. De esta forma, el propósito es poder representar y organizar la información de manera más cercana al pensamiento humano sin perder rigor científico. En este sentido se enmarca la cita de Rudolf Wille: ``\textit{El objetivo y el significado del FCA como teoría matemática sobre conceptos y sus jerarquías es apoyar la comunicación racional entre seres humanos mediante el desarrollo matemático de estructuras conceptuales apropiadas que se puedan manipular con la lógica.}'' \cite{Wille2005}.

El término FCA fue acuñado por Wille en 1984 culminando años más tarde con la publicación más citada al respecto en colaboración con Bernhard Ganter \cite{Ganter1997}. Desde entonces, FCA se ha aplicado con éxito en diferentes disciplinas, como por ejemplo: minería de datos, biología celular \cite{Endres2012}, genética \cite{Kaytoue2011}, economía, ingeniería del software \cite{Snelting1998}, medicina \cite{Motameny2008}, derecho \cite{Mimouni2015}, gestión de la información \cite{Priss2006}, etc.

FCA parte de una representación de conjuntos de objetos y atributos por medio de tablas de datos. Estas tablas se denominan contextos formales y representan las relaciones binarias entre esos objetos y atributos. A partir de ahí, se generan dos herramientas básicas para representar el conocimiento: los retículos de conceptos y los conjuntos de implicaciones. 

Desde hace años, existen en la literatura estudios \cite{Kuznetsov2002} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir del conjunto de datos (en adelante \textit{dataset}\index{dataset} por su nomenclatura habitual en el campo). Sin embargo, debido a que el tamaño del retículo de conceptos es, en el peor de los casos, $2^{min(|G|,|M|)}$, siendo $G$ el conjunto de objetos y $M$ el conjunto de atributos, el coste computacional de los métodos encargados de su construcción constituye una limitación para la aplicación de FCA.

Por otro lado está el conjunto de implicaciones. Las implicaciones pueden considerarse \textit{grosso modo} como reglas del tipo \textit{si-entonces} (en inglés: \textit{if-then rules}\index{if-then rules}), que representan un concepto muy intuitivo: cuando se verifica una premisa, entonces se cumple una conclusión. Esta idea básica se utiliza con diferentes interpretaciones en numerosos campos de conocimiento. Así, en la teoría relacional se interpretan como dependencias funcionales\index{dependencias funcionales} \cite{Codd1970}, en FCA como implicaciones\index{implicaciones} \cite{Ganter1997}, en programación lógica como reglas lógicas de programación \cite{Niemela1999}, etcétera.

No obstante, al igual que en el caso del retículo de conceptos, también existen ciertas desventajas a la hora de trabajar con implicaciones, de hecho, la propia extracción del conjunto completo de implicaciones de un dataset es una tarea que presenta una complejidad exponencial como demostraron los autores en \cite{Cohen2001}. 

Ahora bien, trabajar con conjuntos de implicaciones permite utilizar técnicas de razonamiento automático basadas en la lógica. Este hecho fundamenta el objetivo a alto nivel de esta tesis doctoral, que principalmente consiste en, utilizando los conjuntos de implicaciones, aplicar mecanismos lógicos para realizar un tratamiento eficiente de la información.

La aproximación a través de la lógica, como veremos en el Capítulo \ref{cap:preliminares} es posible gracias a sistemas axiomáticos válidos y completos como los Axiomas de Armstrong\index{Axiomas de Armstrong} \cite{Armstrong74} y la Lógica de Simplificación\index{Lógica de Simplificación} \cite{Enciso2002} (SL, por sus siglas en inglés: \textit{Simplification Logic}). Estos métodos aplicados sobre conjuntos de implicaciones se utilizan en esta tesis doctoral sobre tres áreas de investigación: claves minimales\index{clave!claves minimales}, generadores minimales\index{generadores minimales} y sistemas de recomendación conversacionales.

Se anticipa que, en cada uno de los casos, se va a aprovechar la información subyacente al conjunto de implicaciones para realizar novedosas aproximaciones que permitan abordar problemas presentes en esos ámbitos. De esta forma, con los métodos desarrollados basados en implicaciones, se ha conseguido obtener resultados favorables para todos esos campos. Tales resultados se sustentan por una amplia gama de experimentos, en los cuales se ha utilizado tanto información real como conjuntos autogenerados (utilizando información generada de forma aleatoria) y donde la computación paralela llevada a cabo en entornos de supercomputación\index{supercomputación} ha desempeñado un papel crucial.

Antes de describir con mayor detalle las tres áreas anteriores, es necesario hacer una importante declaración previa. Tal y como se ha mencionado anteriormente, en esta tesis se trabaja con el conjunto de implicaciones que se deriva de un almacén de datos (en adelante referido como \textit{dataset} por su denominación habitual en el campo). Sin embargo, es conveniente destacar que no es competencia de este trabajo el estudio de técnicas de extracción de implicaciones (lo cual es más una tarea de minería de datos), sino que la intención es partir del conjunto de implicaciones para trabajar con él. A este respecto, se pueden consultar los trabajos más citados en la literatura en relación a la extracción de implicaciones a partir de datasets \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Son trabajos de suma importancia desde el punto de vista teórico, pero también desde el punto de visto práctico, pues incluyen las implementaciones de las aplicaciones que realizan la extracción de las implicaciones. 

Dicho esto, se retoma el texto pasando a introducir los tres campos de aplicación donde se han utilizado los conjuntos de implicaciones.

%Sin perjuicio de lo anterior, dado que hemos llegado a crear datasets propios sobre los que poder realizar experimentos como veremos más adelante, si bien no entramos en las técnicas de extracción de implicaciones que utilizan esas aplicaciones, sí hemos tenido que convertirnos en usuarios de estas aplicaciones para poder obtener el conjunto atributos e implicaciones que se verifican. Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

\section{Claves Minimales}
\noindent
El concepto de clave\index{clave} es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}. Una clave de un esquema relacional está compuesta por un subconjunto de atributos\index{atributo} (que verifican un conjunto de objetos) que representan el \textit{dominio} de una determinada función cuya \textit{imagen} es la totalidad del conjunto de atributos. 

Estas funciones se pueden representar por medio de Dependencias Funcionales \cite{Ullman1997} (FD, por sus siglas en inglés: \textit{Functional Dependencies}) que especifican una relación entre dos subconjuntos de atributos, e.g. $A$ y $B$, asegurando que para cualesquiera dos tuplas\index{tupla} de una tabla de datos, si verifican $A$, entonces también verifican $B$, es decir, si un elemento de la tabla contiene un atributo \textit{A}, entonces también contendrá un atributo \textit{B}. Nótese el cambio de denominación de implicación a dependencia funcional por estar en el entorno de los sistemas de bases de datos relacionales. Éste cambio y las diferencias entre ambos conceptos se verá con mayor detalle en la Sección \ref{sec:basesDatosRelacionales}. 

La identificación de las claves de una determinada relación es una tarea crucial para muchas áreas de tratamiento de la información: modelos de datos \cite{Simsion2005}, optimización de consultas \cite{Kemper1991}, indexado \cite{Manolopoulos1999}, etc. El problema consiste en el descubrimiento de todos los subconjuntos de atributos que componen una clave a partir de un conjunto de FD que se cumplen en un esquema de una tabla del modelo relacional.

Las claves no sólo son parte fundamental a considerar durante la fase de diseño de sistemas de base de datos relacionales, sino que también se consideran una poderosa herramienta para resolver multitud de problemas referentes a diversos aspectos del tratamiento de la información. Como muestras de la relevancia de este problema, es posible encontrar numerosas citas en la literatura, entre las que se pueden destacar las siguientes. En \cite{Sismanis2006}, los autores afirman que: ``\textit{identification of keys is a crucially important task in many areas of modern data management, including data modeling, query optimization (provide a query optimizer with new access paths that can lead to substantial speedups in query processing), indexing (allow the database administrator to improve the efficiency of data access via physical design techniques such as data partitioning or the creation of indexes and materialized views), anomaly detection, and data integration}''. Más recientemente, se pueden encontrar estudios en referencia al uso de claves minimales en áreas emergentes como el \textit{linked-data} \cite{Benito-PicazoCMMSE2015}, o también en \cite{Pernelle2013}, donde los autores delimitan el problema manifiestando: \textit{``establishing semantic links between data items can be really useful, since it allows crawlers, browsers and applications to combine information from different sources.''}.

Para ilustrar el concepto de clave, sirva el siguiente Ejemplo básico \ref{ejemplo:basicoClaves}.

\begin{ejemplo}
\label{ejemplo:basicoClaves}
Supongamos que disponemos de la Tabla \ref{tabla:ejemploPeliculas}. Es una pequeña tabla con información que relaciona títulos de películas, actores, países, directores, nacionalidad y años de estreno. 

De esta información, utilizando los métodos comentados anteriormente, podemos extraer el siguiente conjunto de FDs: 

$\Gamma = \{Titulo, A\tilde{n}o \rightarrow Pais$;  $Titulo, A\tilde{n}o \rightarrow Director$; $Director\rightarrow Nacionalidad$\}. 

\begin{table*}[htbp]
\caption{Tabla de películas}
\label{tabla:ejemploPeliculas}
\centering
{\scriptsize
\begin{tabular}{cccccc}
 \hline
 Título & Año & País & Director & Nacionalidad & Actor\\
 \hline
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & John Travolta\\
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & Uma Thurman\\
  Pulp Fiction & 1994 & USA & Quentin Tarantino & USA &Samuel Jackson \\
 King Kong & 2005 & NZ & Peter Jackson& NZ & Naomi Watts\\
 King Kong & 2005 & NZ & Peter Jackson & NZ & Jack Black\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jessica Lange\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jeff Bridges\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Jamie Foxx\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Samuel Jackson\\\hline
\end{tabular}
}
\end{table*}

Esta tabla tiene una única clave: $\{Titulo, A\tilde{n}o, Actor\}$ que corresponde con el conjunto de atributos necesario para identificar cualquier tupla de la relación.
\end{ejemplo}

 %Formalmente:

%\begin{definicion}[Clave minimal]
%Dada un tabla $R$ sobre un conjunto de atributos $\Omega$, decimos que $K$ es una clave minimal de $R$ si se verifica la dependencia funcional $K\rightarrow \Omega$ en $R$ y $\not\exists a \in K$ tal que $K \smallsetminus \{a\} \rightarrow \Omega$.
%\end{definicion}



% El problema de la búsqueda de claves
\section*{El problema de la búsqueda de claves}
\label{sec:problemaBusquedaClaves}
% Problema de la búsqueda de claves
El problema de la búsqueda de claves consiste en encontrar todos los subconjuntos de atributos que componen una clave minimal\footnote{Se acuña el término \textit{minimal} para referirnos a una clave en la que todos y cada uno de los atributos que la forman son imprescindibles para mantener su naturaleza de clave, es decir, no contiene ningún atributo superfluo.} a partir de un conjunto de FD que se verifican en un esquema de una tabla de de datos relacional. Es un campo de estudio con décadas de antigüedad como puede observarse en \cite{Fadous75}, donde las claves se estudiaron dentro del ámbito de la matriz de implicaciones u otros tantos trabajos como \cite{Sali2004,Giannella99} que se centran en averiguar estas claves minimales.

% Claves como elemento crucial
La dificultad al enfrentarse en el problema de la búsqueda de claves surge debido a que, dado un conjunto de atributos $A$, la cardinalidad del conjunto $2^A$ hace que haya que abordar el problema aplicando técnicas que guíen la búsqueda de los conjuntos candidatos a ser claves minimales de forma que se pueda subsanar la complejidad exponencial de este tipo de problemas. 

% problema NP
El cálculo de todas las claves minimales representa un problema complejo. En \cite{Lucchesi78,Yu76} se incluyen resultados interesantes acerca de la complejidad del problema; los autores demuestran que el número de claves está limitado por el factorial del número de dependencias, por tanto, no existe un algoritmo que resuelva el problema en tiempo polinómico. En definitiva, es un problema NP-completo decidir si existe una clave de tamaño a lo sumo $k$ dado un conjunto de FD \cite{Lucchesi78}.

Por otro lado, en \cite{CorderoEMG14}, los autores muestran cómo el problema de las claves minimales en las bases de datos tiene su análogo en FCA, donde el papel de las FD se trata como implicaciones de atributos. En ese artículo, el problema de las claves mínimales se presentó desde un punto de vista lógico y para ello, se empleó un sistema axiomático, que los autores denominaron \slfde\index{\slfde} (por sus siglas en inglés: Simplification Logic for Functional Dependencies) \cite{Enciso2002}, para gestionar las FD y las implicaciones.

% referencias generales
Las principales referencias sobre este problema apuntan al trabajo de Lucchesi y Osborn en \cite{Lucchesi78} que presenta un algoritmo para calcular todas las claves candidatas. Por otro lado, Saiedian y Spencer \cite{Saiedian1996} presentaron un algoritmo usando grafos con atributos para encontrar todas las claves posibles de un esquema de base de datos relacional. No obstante, demostraron que sólo podía aplicarse cuando el grafo de FD no estuviera fuertemente conectado. Es reseñable también el trabajo de Zhang \cite{Zhang09} en el cual se utilizan mapas de Karnaugh \cite{Karnaugh1953} para calcular todas las claves. Existen  más trabajos sobre el problema del cálculo de las claves minimales como son \cite{Sismanis2006,Worland2004}, destacando también una contribución actual que aborda el problema en un estilo lógico \cite{CorderoEMG14}. Asimismo, en \cite{Levy2005,Valtchev03,Valtchev08} los autores propusieron el uso de FCA \cite{Ganter1997} para abordar problemas relacionados con la búsqueda y la gestión de las implicaciones, que pueden considerarse complementarios a nuestro trabajo.




\section*{Algoritmos para el cálculo de claves}
\label{sec:algoritmosCalculoClaves}
% referencias de tableaux
En este ámbito, el objetivo de esta tesis se centra en los algoritmos de búsqueda de claves basados en la lógica, y más específicamente, en aquellos que utilizan el paradigma de tableaux\index{Tableaux} \cite{Morgan1992,Risch1992} como sistema de inferencia. 

De forma muy general, se puede decir que los métodos tipo tableaux representan el espacio de búsqueda como un árbol, donde sus hojas contienen las soluciones (claves). El proceso de construcción del árbol comienza con una raíz inicial y desde allí, mediante la utilización de unas reglas de inferencia del método que se esté aplicando, generan nuevas ramas del árbol etiquetadas con nodos que representan instancias más simples del nodo padre. La mayor ventaja de este proceso es su versatilidad, ya que el desarrollo de nuevos métodos se reduce a cambiar las reglas de inferencia. Debido a esta característica, las comparaciones entre estos métodos se pueden realizar fácilmente ya que su eficiencia va de la mano del tamaño del árbol de búsqueda generado.

Esto conduce a un punto de partida fundamental, los estudios de R. Wastl (Universidad de Wurzburg, Alemania) \cite{Wastl98a,Wastl98} donde se introduce por primera vez un sistema de inferencia de tipo Hilbert para averiguar todas las claves de un esquema relacional. A modo de ejemplo básico, en la Figura \ref{figura:ejemploTaleaux} se presenta un árbol de búsqueda según el paradigma de tableaux desarrollado según las reglas de inferencia del sistema de inferencia $\mathbb{K}$ de Wastl. Básicamente, se parte de una raíz para cuyo cálculo se ha aplicado una regla de inferencia \uno y a partir de ahí, se van construyendo las diferentes ramas del árbol mediante la aplicación de una segunda regla de inferencia \dos al conjunto de FD (véase \cite{Wastl98a,Wastl98} para más detalles).

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=.75\textwidth,height=.3\textheight]{arbol897.png}
	\end{center}
	\caption{Ejemplo de tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
	\label{figura:ejemploTaleaux}
\end{figure}

Siguiendo esta línea, en \cite{Cordero2013} los autores abordan el problema de la búsqueda de claves utilizando un sistema de inferencia basado en la lógica \slfde \cite{Enciso2002}, demostrando como el árbol del espacio de búsqueda que se genera lleva a sobrepasar las capacidades computacionales de ordenadores corrientes hoy día, incluso para problemas pequeños. En \cite{Mora2012} los autores muestran la equivalencia entre \slfde\index{\slfde} y los axiomas de Armstrong \cite{Armstrong74} junto con un algoritmo para calcular el cierre de un conjunto de atributos. Más tarde, en \cite{CorderoEMG14}, los autores introdujeron el método SST\index{SST}, basado en la introducción del test de minimalidad que evita la apertura de ramas adicionales del árbol, por lo que el espacio de búsqueda se vuelve más reducido, logrando un gran rendimiento en comparación con sus predecesores.

% con el paralelo
Una propiedad muy interesante de los métodos basados en tableaux es la generación de subproblemas independientes los unos de los otros a partir del problema original. De esta forma, se alcanza otro objetivo fundamental de esta tesis de utilizar las técnicas lógicas sobre una implementación paralela de los métodos de búsqueda de claves que, mediante el uso de recursos de supercomputación\index{supercomputación}, permitan alcanzar resultados en un tiempo razonable.

En esta línea, son varios los trabajos que han utilizado la paralelización para afrontar problemas relacionados con implicaciones o FCA. Un algoritmo paralelo para el tratamiento de implicaciones enmarcado en el campo de los hipergrafos lo podemos encontrar en \cite{Sridhar1990}. A su vez, Krajca et al. \cite{Krajca2008} presentan un algoritmo paralelo para el cálculo de conceptos formales. Por nuestra parte, en \cite{Benito-Picazo2014} se prsentó una primera aproximación a la paralelización del método de Wastl \cite{Wastl98a,Wastl98} y el algoritmo de claves \cite{Cordero2013}, donde se muestra cómo el paralelismo\index{paralelismo} puede integrarse de forma natural en los métodos basados en tableaux.

Para la labor de computación de alto rendimiento, se ha trabajado fervientemente con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{http://www.scbi.uma.es/}. La posibilidad de tratar con este centro ha proporcionado dos beneficios fundamentales: por un lado, se ha alcanzado una elevada pericia para trabajar en entornos de computación de alto rendimiento (HPC, por sus siglas en inglés: \textit{High Performance Computing}) y para realizar implementaciones que aprovechen una alta cantidad de recursos, y por otro lado, ha permitido obtener resultados empíricos sobre experimentos utilizando estrategias paralelas\index{paralelismo} que han desembocado en contribuciones científicas \cite{Benito-Picazo2014,Benito-Picazo2016,Benito-Picazo2018} y que habría sido imposible conseguir en la actualidad sin contar con tales recursos computacionales.


\section*{Métodos \textit{SST} y \textit{CK}}
\label{sec:metodosSSTyCK}
En \cite{CorderoEMG14} se presentó un nuevo algoritmo, denominado SST\index{SST}, para calcular todas las claves minimales usando una estrategia de estilo tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. SST se basa en la noción de cierre de conjunto; una noción básica en la teoría de bases de datos que, utilizando el sistema axiomático\index{sistema axiomático}, permite caracterizar el conjunto máximo de atributos que se puede alcanzar, desde un determinado conjunto de atributos $A$ con respecto a un conjunto de FD. Por lo tanto, si el cierre de $A$ se denota como $A^+_\Gamma$, el sistema de inferencia para FD permite inferir la FD $A\to A^+_\Gamma$. En este sentido, el enfoque con estilo lógico para el problema de las claves minimales consiste en la enumeración de todos los conjuntos de atributos $A$ tales que se verifique la FD: $A\to\Omega$.

SST muestra un gran rendimiento en comparación con sus predecesores como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}. El beneficio principal en la reducción del espacio de búsqueda se debe a la introducción de un test de inclusión para evitar la apertura de ramas extra. Gracias a ello, SST no abre aquellas ramas de las que se tiene el conocimiento de que van a producir las mismas claves que se calculan en otra rama.

Tomando como base esos trabajos y con el apoyo del sistema axiómatico de la lógica \slfde\index{\slfde} (véase \ref{sec:logicaSimplificacion}), en esta tesis se presenta un nuevo método llamado \textit{Closure Keys (CK)}\index{CK} que incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde para mejorar el método SST. El nuevo operador de cierre\index{operador de cierre} definido en \cite{Mora2012} permite reducir el espacio de búsqueda realizando reducciones en el camino hacia las hojas, donde finalmente se obtienen las claves. El método CK tiene una característica fundamental que lo convierte en una novedosa alternativa frente a los métodos clásicos y es la siguiente. Además del conjunto de atributos que se deriva de la aplicación del operador de cierre al conjunto de implicaciones, el método proporciona un subconjunto $\Gamma$ de implicaciones del conjunto $\Sigma$ original que engloba la información que ha quedado fuera del cierre. 

Más formalmente, el método CK recibe un conjunto de implicaciones $\Sigma$ y un subconjunto de atributos $X \subseteq \Omega$, calcula el conjunto cierre $X^+$ respecto a $\Sigma$, y además, un nuevo conjunto $\Gamma$ que contiene el conjunto de implicaciones que guarda la semántica que queda fuera del cierre $X^+$. Si $\Gamma = \varnothing$, entonces $X^+ = \Omega$ (véase \cite{Mora2012} para más detalles).

Con esto, se tienen los elementos para presentar la principal contribución de la tesis en la resolución del problema de la búsqueda de claves. Se han diseñado e implementado algoritmos para las versiones paralelas tanto del método SST como del método CK basándose en el paradigma \textit{MapReduce}\index{MapReduce} \cite{Dean2004}.

Básicamente, el algoritmo paralelo de búsqueda de claves se divide en dos partes principales. Utiliza una primera fase en la que se realiza una expansión del árbol de búsqueda trabajando sobre el problema original y aplicando sucesivamente las reglas de inferencia y el algoritmo del cierre lógico, pero llegando únicamente hasta un cierto nivel de árbol, es decir sin alcanzar todavía las claves en las hojas del árbol. A partir de ese momento, se tiene un árbol de búsqueda parcial en el que cada nodo constituye un problema equivalente al original pero simplificado. A continuación interviene la segunda etapa del algoritmo y la computación paralela en la que cada nodo de ese nivel del árbol, se resuelve en paralelo mediante el uso de un elevado número de cores, es decir, aplica el mismo algoritmo de búsqueda de claves, pero ahora ya sí, hasta alcanzar las hojas del árbol, es decir, las soluciones del problema.

Existen numerosos factores a tener en cuenta a la hora de aplicar el algoritmo paralelo, de entre los cuales, el más importante es el valor de corte o parada de la primera etapa (en adelante \textit{BOV} por sus siglas en inglés, \textit{Break-Off Value}). Determinar este valor es un punto muy sensible del problema, pues de él depende el aprovechamiento general de los recursos en la aplicación del paralelismo \cite{Benito-Picazo2016}. 

Para contrastar la aportación del algoritmo, se han realizado multitud de experimentos, los cuales necesitan llevarse a cabo en entornos de supercomputación\index{supercomputación} y cuyos resultados pueden consultarse en \cite{Benito-Picazo2016}. Así, se ha demostrado que el algoritmo diseñado es claramente adecuado susceptible de ejecutarse utilizando una implementación paralela. De esta forma, se consiguen resultados en tiempos razonables incluso en los casos en los que la cantidad de información de entrada es considerable y en los que los métodos secuenciales no son capaces de finalizar.


\section{Generadores Minimales}
Como se ha mencionado anteriormente, una forma de representar en FCA el conocimiento es el retículo de conceptos. Esta representación otorga una visión global de la información con un formalismo muy sólido, abriendo la puerta para utilizar la teoría de retículos como una metateoría para gestionar la información \cite{Bertet2016}.

Los conjuntos cerrados\index{retículo de conceptos!conjuntos cerrados} son la base para la generación del retículo de conceptos ya que éste puede ser construido a partir de aquellos, considerando la relación de subconjuntos como la relación de orden. En este punto nace el concepto de generadores minimales como aquellas representaciones de los conjuntos cerrados que no contengan información superflua, es decir, representaciones canónicas de cada conjunto cerrado \cite{Ganter1997}.

Los generadores minimales junto con los conjuntos cerrados son esenciales para obtener una representación completa del conocimiento en FCA, pero no sólo son interesantes desde un punto de visto teórico. La importancia de los generadores minimales puede apreciarse claramente a través de citas tales como: \textit{``Minimal generators have some nice properties: they have relatively small size and they form an order ideal''}, existente en importantes estudios como el de Poelmans et al. \cite{Poelmans2013} o \cite{Qu2007}. Además, los generadores minimales se han usado como punto clave para generar bases\index{bases}, las cuales constituyen una representación compacta del conocimiento que facilita un mejor rendimiento de los métodos de razonamiento basados en reglas. Missaoui et al. \cite{Missaoui2010,Missaoui2012} presentan el uso de generadores minimales para calcular bases que impliquen atributos positivos y negativos cuyas premisas son generadores minimales.

Cualquier operador de cierre $c$ sobre un conjunto de atributos $M$ puede asociarse con un sistema de implicaciones. Esta conexión establece una forma de gestionar el trabajo del operador de cierre $c$ por medio de su derivación sintáctica y, como consecuencia, se puede elaborar un método para realizar esta gestión. Pero, ¿qué ocurre con la conexión inversa? Es decir, dado un conjunto de implicaciones, ¿es posible generar el operador de cierre $c$ asociado a él? Tal pregunta es el núcleo de esta parte de la tesis y su solución implica enumerar todos los conjuntos cerrados.

Si se tiene que $X,Y \subseteq M$ satisfacen que $X = Y^+_\Sigma$, es habitual decir que $Y$ es un generador del conjunto cerrado $X$. Obsérvese que cualquier subconjunto de $X$ que contiene $Y$ es también un generador de $X$. Dado que se trabaja con conjuntos finitos de atributos, el conjunto de los generadores de un conjunto cerrado se pueden caracterizar por sus generadores minimales.


\section*{Métodos para el cálculo de generadores minimales}
\label{seccion:metodosGeneradoresMinimales}
En este aspecto, centrándose una vez más en el tratamiento inteligente de implicaciones, se utilizan como los elementos para describir la información y además, como base del diseño de métodos para enumerar todos los conjuntos cerrados y sus generadores minimales. El proceso se desarrolla a partir de esta información, y no del dataset\index{dataset} original, lo cual, hasta donde se ha investigado, no se había hecho previamente.

El método propuesto en esta tesis es una evolución del presentado en \cite{Cordero2012}, donde se utilizó la lógica \slfde\index{\slfde} como herramienta para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. Este método trabaja sobre el conjunto de implicaciones aplicando unas reglas de inferencia\index{reglas de inferencia} y construyendo árbol de búsqueda de aspecto similar a los árboles del caso de las claves minimales.

De manera específica, dado un conjunto de atributos $M$ y un sistema de implicaciones $\Sigma$, el método realiza un mapeo $mg_\Sigma\colon 2^M\to 2^{2^M}$\index{mapeo $mg_\Sigma$} que satisface la siguiente condición.

$\forall X,Y\subseteq M$, $X\in mg_\Sigma(C)$ si y sólo si $C$ es cerrado para $(\ )^+_\Sigma$ y $X$ es un generador minimal para $C$.

\begin{ejemplo}
Sea $\Sigma=\{a\to c, bc\to d, c\to ae, d\to e\}$, el mapeo $mg_\Sigma$ se describe como:
\begin{center}
\begin{tabular}{p{1.6cm}|p{.4cm}p{.3cm}p{.3cm}p{.5cm}p{.5cm}p{.6cm}p{.6cm}p{.8cm}p{.9cm}}
$X$ & $\varnothing$ & $b$ & $e$ & $be$ & $de$ & $ace$ & $bde$ &  $acde$ &   $abcde$   \\
\hline
$mg_\Sigma(X)$ &$\varnothing$ & $b$ & $e$ & $be$ & $d$  & $a$ & $bd$ & $ad$ & $ab$ 
\\
& & & & & & $c$& &$cd$ & $bc$    
\end{tabular}
\end{center}

En otro caso, $X$ no es cerrado y $mg_\Sigma(X)=\varnothing$. Nótese que $\varnothing$ es cerrado y $mg_{\Sigma}(\varnothing)=\{\varnothing\}$, i.e. $\varnothing$ es un generador minimal del conjunto cerrado $\varnothing$.
\end{ejemplo}

Tras el método presentado en \cite{Cordero2012} (que los autores denominaron MinGen)\index{MinGen} se presenta ahora un nuevo método, MinGenPr\index{MinGenPr}, que aplica una importante mejora con respecto al anterior. Fundamentalmente, consiste en incorporar un mecanismo de poda, basada en un test de inclusión de conjuntos, que involucra a todos los nodos del mismo nivel, para evitar la generación de generadores minimales y cierres redundantes. El propósito de esta poda es verificar la información de cada nodo en el espacio de búsqueda, evitando la apertura de una rama completa. 

Finalmente, se propone un último método, GenMinGen\index{GenMinGen}, que generaliza la estrategia de poda anterior al considerar el test de inclusión del subconjunto no sólo con la información de los nodos del mismo nivel, sino también con todos los generadores minimales calculados antes de la apertura de cada rama.

En definitiva, se han estudiado, diseñado e implementado cada uno de estos métodos en su versión secuencial. Para evaluar el rendimiento e ilustrar las mejoras obtenidas al pasar de un método a otro, se han realizado un gran número de pruebas utilizando información autogenerada (i.e. información creada de forma aleatoria para la realización de las pruebas) e información real procedente de repositorios de datos utilizados comúnmente en investigación como son los de la Universidad de California, Irvine (UCI)\footnote{https://archive.ics.uci.edu/ml/datasets.html}.

A la luz de los resultados obtenidos en \cite{Benito-Picazo2018}, se aprecia claramente como la estrategia de poda del método MinGenPr\index{MinGenPr} hace que su rendimiento supere con creces al anterior MinGen. Estas mejoras pueden verse reflejadas en la reducción del número de nodos del árbol de búsqueda y su consiguiente disminución de los tiempos de ejecución del algoritmo. Respecto al último método, GenMinGen, los resultados de los experimentos son aún más notables, alcanzando reducciones superiores al 75\% en ambas métricas (i.e. número de nodos y tiempos de ejecución) en muchos de los casos. 

La contrapartida que aparece al utilizar estos métodos es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial. Sin embargo, dado que nuestro objetivo es explotar las posibilidades de operar con conjuntos de implicaciones, en este trabajo se va a combinar ambos aspectos enumerando todos los conjuntos cerrados a partir de un conjunto dado de implicaciones. De hecho, se va a ir un paso más allá, calculando no sólo todos los conjuntos cerrados, sino que para cada uno de ellos se producirán sus respectivos generadores minimales.



\section*{Generadores minimales y paralelismo}
\label{seccion:generadoresMinimalesParalelismo}
Si se pretende trabajar sobre conjuntos de datos con una cantidad de información substancial, surge el mismo problema que en la enumeración de las claves minimales, la capacidad computacional de una máquina convencional actual no es suficiente para solucionar estos problemas en un tiempo razonable. Por tanto, se vuelve a utilizar el paralelismo como estrategia para abordar el problema, lo cual conduce directamente a la siguiente sección.

%No obstante, aparecerá un problema similar al que sucedía con las claves minimales, y es que, al tratar con grandes cantidades de información, los métodos realizados para producir los generadores minimales, conllevan unas necesidades de cómputo que sobrepasan los límites de las máquinas convencionales actualmente. Por tanto, una vez más, hay que trasladar las implementaciones secuenciales de los métodos a versiones paralelas que permitan funcionar bajo arquitecturas de supercomputación\index{supercomputación}. 

En este sentido, en esta tesis, se han diseñado los métodos de búsqueda de generadores minimales y se han realizado tanto sus las implementaciones secuenciales como paralelas (MinGen\index{MinGen}, MinGenPr\index{MinGenPr}, GenMinGen\index{GenMinGen}, MinGenPar\index{MinGenPar}). Para las versiones paralelas se va a utilizar la misma filosofía de implementación que en el caso de las claves minimales, es decir, se han desarrollado unos códigos, basándonse en el esquema \textit{MapReduce}\index{MapReduce} \cite{Dean2004}, que se ejecutarán en dos etapas. Al igual que en las claves minimales, la primera etapa divide el problema de entrada en varios subproblemas equivalentes pero reducidos de forma que puedan ser tratados de forma independiente. En la segunda etapa, cada uno de estos subproblemas se resuelve en paralelo usando múltiples cores.

Una vez más, para verificar el rendimiento y la idoneidad de los métodos para aplicar estrategias paralelas, se ha realizado una amplia batería de pruebas tanto sobre información autogenerada como información real, tal y como se ha explicado anteriormente para el tema de las claves minimales. Además, las pruebas han incluido tareas de estimación del número óptimo de cores a utilizar así como del valor de corte más apropiado en la etapa primera de los métodos paralelos. Los resultados obtenidos respecto a esta parte de la investigación pueden consultarse en \cite{Benito-PicazoCMMSE2017} y, especialmente en \cite{Benito-Picazo2018}, que constituye uno de los trabajos que avalan esta tesis doctoral. 




\section{Sistemas de Recomendación Conversacionales}
\noindent
La tercera aportación de esta tesis doctoral, haciendo uso de los conjuntos de implicaciones\index{conjuntos de implicaciones} y los conjuntos cerrados\index{conjuntos cerrados}, se enmarca en el campo de los sistemas de recomendación\index{sistemas de recomendación} (SR). 

%El objetivo principal de los SR es ayudar al usuario a elegir entre un número alto de alternativas.  

De forma muy resumida, se podría considerar que un SR es un sistema inteligente que proporciona a los usuarios una serie de sugerencias personalizadas (recomendaciones) seleccionadas de un conjunto de elementos (ítems). Comúnmente, los SR estudian las características de cada usuario e ítem del sistema, y a partir de ahí, mediante un procesamiento de los datos, encuentra un subconjunto de ítems que pueden resultar de interés para el usuario. Una de las referencias más notables en el campo de los SR la encontramos en el libro de Adomavicius y otros \cite{AdomaviciusBook11}.

% Historia
Desde los primeros trabajos sobre SR \cite{Hill1995,Resnick1997}, éstos han estado en continua evolución durante los últimos años \cite{Adomavicius2005}. Sin embargo, es con la expansión de las nuevas tecnologías cuando han tenido un acercamiento más directo a la mayor parte de la sociedad debido a su capacidad para realizar todo tipo de recomendaciones sobre productos muy populares (libros \cite{Crespo2011}, documentos \cite{Porcel2012}, música \cite{LampropoulosLT12}, turismo \cite{BorrasFPMVIORC11}, películas\footnote{https://www.movielens.org}, etc.).

% Importancia
En la actualidad, los SR constituyen un claro campo de investigación y estudio como demuestran el gran número de trabajos que se están realizando \cite{Son2018,Eirinaki2018} y cuya cantidad continúa aumentando día a día. Además, la relevancia de estos sistemas no se limita al ámbito investigador. Actualmente, muchos SR ya han sido implantados con éxito en fuertes entornos comerciales a nivel mundial. Este es el caso de empresas líderes en el sector como pueden ser Amazon \cite{linden2003}, LinkedIn \cite{Metaphor2012} o Facebook \cite{Tiroshi11}, que han realizado fuertes inversiones con el fin de generar mejores SR. Estas situaciones ponen de manifiesto la gran importancia de estos sistemas en ambas vertientes de la sociedad actual.

% FCA y SR
Abordar la generación de recomendaciones haciendo uso de FCA es una aproximación existente en la literatura desde hace años. En \cite{duBoucherRyan2006}, los autores utilizan FCA para agrupar elementos y usuarios en conceptos para posteriormente, realizar recomendaciones colaborativas según la afinidad con los elementos vecinos. Más tarde, en \cite{Senatore2013}, los autores introducen un modelo para el filtrado colaborativo basado en FCA para generar correlaciones entre datos a través de un diseño del retículo. Zhang et al. \cite{Zhang2015} propusieron un sistema basado en similitud agrupando la información contextual en grafos mediante el cual llevar a cabo recomendaciones sobre las interacciones sociales entre usuarios. En \cite{LeivaERCMG13,LeivaERCMG13a}, se utilizan relaciones difusas e implicaciones ponderadas para especificar el contexto y \slfde\index{\slfde} para desarrollar un proceso lineal de filtrado que permite a los SR podar el conjunto original de elementos y así mejorar su eficiencia. Recientemente, en \cite{Zou2017} se propone y utiliza un novedoso SR personalizado basado en el retículo de conceptos para descubrir información valiosa de acuerdo con los requisitos e intereses de los usuarios de forma rápida y eficiente. Todos estos trabajos subrayan claramente cómo FCA puede aplicarse con éxito en el campo de los SR.


\section*{Técnicas de recomendación}
\label{seccion:tecnicasRecomendacion}
% Tipos
Existen numerosos tipos de SR que normalmente se clasifican atendiendo a cómo se generan las recomendaciones. Los más conocidos y extendidos son los sistemas de filtrado colaborativo (en inglés: \textit{Collaborative Filtering})\index{sistemas de recomendación!colaborativos} que basan su funcionamiento fundamentalmente en las valoraciones que otros usuarios han otorgado a los elementos disponibles; y los sistemas basados en contenido (en inglés: \textit{Content-Based})\index{sistemas de recomendación!basados en contenido} que proporcionan resultados que tengan características similares a otros que han sido bien valorados anteriormente por el usuario.

En los últimos años ha habido un gran crecimiento de los SR contextuales\index{sistemas de recomendación!contextuales} \cite{BenSassi2017}, capaces de tener en cuenta información relevante para la recomendación como puede ser la hora, el lugar, la compañía, la ubicación, etc. Existen también los conocidos como SR demográficos\index{sistemas de recomendación!demográficos} \cite{BeelLNG13} que clasifican a los usuarios según diferentes parámetros personales (edad, localización, etc.) y de acuerdo con esto generan las recomendaciones. Por otro lado aparecen los denominados SR basados en conocimiento (en inglés: \textit{Knowledge-Based})\index{sistemas de recomendación!basados en conocimiento} \cite{Mandl2011}. Estos sistemas modelan y gestionan el conocimiento inherente a los datos y revelan cómo un ítem puede satisfacer la necesidad del usuario, es decir, utilizan un método de razonamiento para inferir la relación entre una necesidad y una posible recomendación. 

Finalmente, se introduce los SR más importantes desde el punto de vista de este trabajo, los denominados SR conversacionales\index{sistemas de recomendación!conversacionales} \cite{Griol2018,Lee2017}. Estos SR están estrechamente relacionados con los conceptos de recomendador basado en críticas (en inglés, \textit{Critiquing Recommender}) \cite{Chen2012} y recomendaciones de información \cite{TrabelsiWBR11}. Mientras que con los SR mencionados previamente las recomendaciones vienen determinadas por los datos de partida y el algoritmo que se utiliza, los SR conversacionales se diferencian en el sentido del procedimiento o flujo de trabajo que se sigue para generar la recomendación.

Se destaca especialmente este tipo de SR porque será la estrategia principal en la que se enmarca el desarrollo de SR realizado y que ha dado lugar a una de las contribuciones que avalan esta tesis \cite{Benito-Picazo2017}. Se puede consultar una clasificación más detallada en el libro de Adomavicius y Tuzhilin \cite{AdomaviciusBook11} que, junto con la contribución de Bobadilla et al. \cite{Bobadilla2013}, constituyen las referencias más citadas en el campo.

Se puede apreciar que existen diferentes tipos de estrategias para los SR, sin embargo, la historia ha demostrado ampliamente que la mejor alternativa consiste en combinar características de diferentes tipos de SR para generar híbridos que se beneficien de las ventajas de cada uno de ellos \cite{DeCampos2010}. Tal es nuestro caso, en el que nuestro trabajo ha culminado en un SR híbrido que combina las siguientes técnicas de recomendación:

\begin{itemize}
	\item \textbf{SR basados en conocimiento.\index{sistemas de recomendación!basados en conocimiento}} Ya que se utilizan mecanismos de extracción de conocimiento de los datos utilizando FCA, los conjuntos de implicaciones y los operadores de cierre.
	\item \textbf{SR basados en contenido.\index{sistemas de recomendación!basados en contenido}} Puesto que las técnicas lógicas empleadas se basan en informaciónsobre los ítems a recomendar y sus atributos. 
	\item \textbf{SR conversacionales.\index{sistemas de recomendación!conversacionales}} Ya que es precisamente un proceso de diálogo el que se utiliza para generar
las recomendaciones.
\end{itemize}

Pero además, no se tendrá en cuenta únicamente de la estrategia de recomendación sino también del proceso de cómo obtener una recomendación. En este sentido, se introduce el concepto de Recuperación de Información (en inglés, \textit{Information Retrieval})\index{Information Retrieval}. Este concepto se basa en obtener recursos relevantes del sistema de información a partir de la realización de consultas. Numerosos trabajos en la literatura relacionan el uso de FCA en modelos basados en Recuperación de Información \cite{Codocedo2015,Ignatov2015}.


\section*{Problemas comunes y la maldición de la dimensión}
\label{seccion:problemasRecomendacion}
Si bien es cierto que los SR están alcanzando una enorme importancia, existen numerosas dificultades que han de afrontarse a la hora de diseñarlos e implementarlos. En la lista de problemas\index{problemas SR} relacionados con los SR \cite{Shah2016} se puede destacar: el arranque en frío o \textit{cold-start}\index{problemas SR!arranque en frío} (aparece cuando un nuevo elemento se incluye en el sistema careciendo de la información necesaria para participar en las recomendaciones) \cite{Feil2016,Son201687,Benito-PicazoCMMSE2016}, privacidad (puede surgir un problema si el sistema necesita información sensible con respecto al usuario) \cite{Friedman2015}\index{problemas SR!privacidad}, oveja-negra \cite{Gras2016}\index{problemas SR!oveja-negra} (existen elementos que no manifiestan similitud suficiente con ningunos otros, estos elementos tan singulares pueden verse olvidados en la mayoría de las recomendaciones), escasez \cite{Guo2012}\index{problemas SR!escasez} (pueden existir elementos del SR que sean difíciles de recomendar si la información que se tiene sobre ellos es insuficiente), ataques maliciosos \cite{Zhou2015,Yang2016}\index{problemas SR!ataques maliciosos} (son acciones que se pueden realizar sobre un SR para romper su fiabilidad, rendimiento, confianza), sobreespecialización \cite{LopsGS11}\index{problemas SR!sobreespecialización}, escalabilidad \cite{Isinkaye2015}\index{problemas SR!escalabilidad}, postergación \cite{Sundaresan2011}\index{problemas SR!postergación} (elementos que no son muy populares quedan relegados por aquellos otros que gozan de mayor solicitud), dimensionalidad \cite{Salimi2017}\index{problemas SR!dimensionalidad} (aparece cuando el la cantidad de información y su nivel de detalle en el sistema es tal que dificulta la interacción con el usuario), etc.

En concreto, en esta tesis se ha orientado el trabajo a abordar este último problema de la dimensionalidad en los SR. Este problema, también conocido como \textit{the curse of dimensionality phenomenon} \cite{Salimi2017,Nagler2016} aparece cuando es necesario trabajar sobre datasets con un alto número de características (variables o atributos). De forma intuitiva, se puede describir de la siguiente manera: cuando hay pocas columnas de datos, los algoritmos de tratamiento inteligente de la información (aprendizaje automático, \textit{clustering}, clasificación, etc.) suelen tener un buen comportamiento. Sin embargo, a medida que aumentan las columnas o características de nuestros ítems, se vuelve más difícil hacer labores predictivas con un buen nivel de precisión. El número de filas de datos necesarias para realizar cualquier modelado útil aumenta exponencialmente a medida que agregamos más columnas a una tabla \cite{McEneaney2008}.

Para abordar este problema, se pueden encontrar numerosos trabajos en la literatura \cite{Salimi2018,KIM2018} sobre la reducción de la dimensión de la información, especialmente mediante selección de características \textit{(feature selection)}\index{feature selection}, que pueden ayudar a descartar aquellas características que no son relevantes de cara al objetivo buscado. De hecho, estas técnicas ya se aplican en otras áreas como son: algoritmos genéticos o redes neuronales, normalmente centrándose en la aplicación de un proceso automatizado que se aplique de una vez \textit{(batch mode)} \cite{Viegas2018}.

%Suele ser habitual que para realizar una selección de características por parte del SR, el usuario tenga que introducir y seleccionar información del sistema una y otra vez. Esto constituye un problema dado que pueden existir artículos para los cuales el número de características que los definen sea muy elevado y en consecuencia, incomode la correcta interacción del usuario con el sistema, poniendo de manifiesto de nuevo cómo la alta dimensionalidad constituye un problema para los SR.

En definitiva, el objetivo ha sido abordar el problema de la alta dimensionalidad\index{problemas SR!dimensionalidad} en los SR haciendo uso de los conjuntos de implicaciones\index{conjuntos de implicaciones}, a través de un proceso de selección de atributos por parte del usuario mediante un SR conversacional\index{sistemas de recomendación!conversacionales} que utiliza características de los SR basados en contenido\index{sistemas de recomendación!basados en contenido} y en conocimiento\index{sistemas de recomendación!basados en conocimiento}. De esta forma, se ha conseguido reducir el número de pasos necesarios en el diálogo para generar recomendaciones y al mismo tiempo la aproximación conseguida permite gestionar favorablemente el problema de la dimensionalidad.

Un trabajo interesante en esta área es \cite{Jannach2009}, que establece la idoneidad de los enfoques basados en el conocimiento para los procesos conversacionales.
En particular, estos autores utilizan el razonamiento basado en restricciones, en lugar de nuestro enfoque basado en la lógica. Además, este trabajo trata sobre
concepto de optimización de consultas, análogo al aplicado en la propuesta de esta tesis. Otro trabajo notable es \cite{TrabelsiWBR11}, que comparte el objetivo de disminuir el número de pasos de la conversación. Los autores proponen métricas acerca del número de pasos de la conversación y tasas de poda, ambos muy similares a los utilizados en este trabajo de tesis. Por otro lado, en \cite{Chen2007}, los autores demuestran cómo la posibilidad de que sea el usuario el encargado de la selección de atributos genera una ventaja con respecto al hecho de que sea el sistema mismo el encargado de dicha selección. Este hecho respalda el enfoque buscado en esta tesis, en el cual el experto humano guía la conversación y el proceso de selección de características.


\section*{Evaluación de los sistemas de recomendación}
\label{seccion:evaluacionSistemasRecomendacion}
La evaluación de las predicciones y recomendaciones se ha convertido en un aspecto muy importante \cite{Herlocker2004,Burke2010}. Los SR requieren medidas de calidad y métricas de evaluación \cite{Gunawardana2009} para conocer la calidad de las técnicas, métodos y algoritmos para las predicciones y recomendaciones. Las métricas de evaluación \cite{HernandezdelOlmo2008} y los \textit{frameworks} de evaluación \cite{Bobadilla2011} facilitan la comparación de varias soluciones para el mismo problema.

No obstante, hay que tener en cuenta que dependiendo del SR con el que estemos trabajando, la evaluación habrá que llevarla a cabo utilizando aquellas métricas, que por su naturaleza y significado, sean coherentes con el SR que se desea evaluar. En el caso de estudio de esta tesis, dado que se ha desarrollado un SR conversacional como se verá más adelante, son medidas adecuadas de rendimiento aquellas basadas en calcular el número de pasos que se producen en la conversación \cite{McSherry01}. Por contra, otras métricas tan populares como son \textit{Precision} y \textit{Recall} \cite{Gunawardana2015} no son adecuadas de aplicar en el trabajo de esta tesis porque se obtendría siempre valores máximos en ambas métricas, y la razón es la siguiente:

\begin{itemize}
	\item En primer lugar, cualquier ítem de la lista de resultados, verifica los atributos seleccionados ya que la consulta para obtener los ítems resultado contiene esas restricciones.
	\item Y en segundo lugar, a cada paso del diálogo, el sistema devuelve todos los ítems que verifiquen la selección de atributos establecida por el usuario.
\end{itemize}
 
Sucede una situación similar al hablar de otras métricas muy utilizadas como son MAE o RMSE. Estas métricas basadas en valoraciones no tienen cabida en el sistema desarrollado en esta tesis puesto que no existen valoraciones con la que el sistema trabaje. 

Por la misma razón, no existe necesidad de considerar métricas referentes a la exactitud de los resultados ya que el sistema desarrollado no es un modelo de predicción, su funcionamiento está basado en implicaciones y eso asegura un 100\% de exactitud en las respuestas..


\section*{Propuesta desarrollada}
\label{seccion:aplicacionDesarrollada}
La propuesta de solución realizada consiste en reducir los efectos del problema de la alta dimensionalidad mediante un proceso de selección de características guiado por el usuario (experto humano) dentro de un sistema conversacional \cite{Yu2016}. Por tanto, el sistema se va a centrar principalmente en la primera fase de la recomendación, el filtrado. Para ello, una vez más se hará uso de una gestión inteligente de las implicaciones y de los conjuntos cerrados que va a permitir reducir sustancialmente el número de pasos necesarios en el diálogo entre el usuario y la aplicación para conseguir una recomendación adecuada en tiempo y forma. Para ello, se cuenta con el apoyo de la lógica \slfde\index{\slfde} introducida por los autores en \cite{Enciso2002} y en particular, se utilizará el algoritmo del cierre de atributos \slfde \cite{Mora2012} como núcleo de un marco de selección de atributos que será el que permita reducir el número de etapas del diálogo.

Además, se han realizado numerosas pruebas de aplicación sobre la propuesta que se ha desarrollado para contrastar su validez. En concreto, se han realizado pruebas utilizando información real sobre enfermedades y fenotipos como se puede apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Además, la propuesta desarrollada, al igual que la gran mayoría de los SR, permite actuar sobre diferentes datasets de forma que se puede utilizar el mismo procedimiento para mejorar las recomendaciones conversacionales en diversos entornos. Esta versatilidad es una característica notoria, ya que permite libertad de maniobra en el caso de que se quieran introducir ciertos cambios, o simplemente que los datos sean diferentes. En ese sentido, la propuesta de este trabajo casa con los conceptos de adaptabilidad y longevidad de los SR ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo es necesario conocer el conjunto de atributos e implicaciones subyacente a los datos.


\section{Verificación de los Resultados}
\label{sec:verificacionResultados}
Antes de entrar plenamente en el cuerpo de la tesis es necesario hacer una importante aclaración previa con la intención de indicar la manera de certificar la validez de los resultados obtenidos a lo largo de la tesis. 

Como se ha verá en los siguientes capítulos, la labor de investigación se ha centrado en actuar sobre conjuntos de implicaciones, y en ese sentido, para los experimentos realizados se ha contado con unos ficheros de entrada que contenían la información necesaria, y sobre ellos se han obtenido unos resultados. Ahora bien, la forma de verificar que esos resultados son correctos es la siguiente. 

En primer lugar y con respecto a los resultados que se presentan en cuanto a claves y generadores minimales, se han realizado numerosos ejercicios en papel intentando buscar casos límites donde la implementación pudiera no ser precisa y se ha comprobado que los resultados obtenidos en papel coincidían exactamente con los calculados por la máquina. Además, dado que para muchos de los experimentos, en los que se llegaban a calcular millones de nodos de un árbol, no era posible comprobar si cada uno de esos cálculos era correcto, para el caso concreto de los experimentos relacionados con claves minimales, la validez de los experimentos viene dada al haber cotejado los resultados con aquellos obtenidos sobre un amplio abanico de experimentos en trabajos anteriores \cite{Benito-Picazo2013PFC,Benito-Picazo2014TFM} donde su validez quedó demostrada. Básicamente, la validez de los ejercicios más grandes se ha extrapolado de los resultados correctos obtenidos para los ejercicios más pequeños. Pero además, la validez de los resultados se corrobora igualmente al alcanzar las mismas soluciones para diferentes métodos cuando cada uno de ellos hace un tratamiento de la información diferente con respecto al otro. En relación a los experimentos con SR conversacionales, dado que los experimentos no alcanzan números tan costosos de verificar, la validez de los resultados puede demostrarse de forma más asequible siguiendo un desarrollo explícito en papel.

Adicionalmente, y con mayor énfasis en relación a los experimentos que han conllevado la utilización de recursos de supercomputación, cada uno de los experimentos se ha reproducido entre 30 y 50 veces de forma que los resultados mostrados son fruto de un estudio estadístico posterior más amplio que permite identificar los resultados más fiables, tal y como se sugiere en trabajos anteriores \cite{Zobel1998,Goh10,Yang2012}.

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que permitieran evaluar el rendimiento de las pruebas de forma que se pudieran comparar unos métodos con otros. En el caso de los experimentos con claves y generadores minimales, cuando se plantea la idea de la comparación de resultados, lo primero que se pensó fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, se advirtió que este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar la magnitud del árbol y la cantidad de resultados redundantes que se obtienen (véase \cite{Benito-Picazo2014,Benito-Picazo2013PFC,Benito-Picazo2014TFM}. De esta forma, en el momento de que exista otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre se puede atender al tamaño del árbol y al número de cálculos redundantes, pudiendo defender si realmente es una mejora en el método o bien, en la ejecución debido a la arquitectura.

%\vspace{0.4cm}
A modo de resumen gráfico, la Figura \ref{figura:esquemaConceptual} muestra un esquema del camino que investigación que se ha seguido en el desarrollo de esta tesis doctoral, apoyado por las principales nociones y referencias.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.75\textheight]{esquemaConceptual.png}
	\end{center}
	\caption{Esquema del estado del arte y las contribuciones generadas.}
	\label{figura:esquemaConceptual}
\end{figure}

Para finalizar este capítulo, se incluye un último apartado donde se describe brevemente la estructura que presenta el documento incluyendo las publicaciones que avalan esta tesis.

%\vspace{2cm}
%\newpage{}
\subsection*{Estructura de la Tesis}
\noindent
En este primer capítulo de introducción, se han fijado los puntos fundamentales de la tesis, como son: el marco de trabajo sobre el que se va a actuar, las técnicas que se utilizarán y los principales objetivos que se pretenden alcanzar. Concretamente, se ha estipulado la utilización de FCA y los conjuntos de implicaciones como base del estudio sobre la que aplicar técnicas basadas en la lógica para mejorar el tratamiento de la información.

A continuación, aparece el Capítulo \ref{cap:preliminares}, en el que se han recopilado un conjunto de conceptos previos necesarios relacionados con: FCA, la lógica de simplificación, los sistemas de implicaciones y los operadores de cierre. Tras estos dos primeros capítulos, ya se contará con el conocimiento previo necesario para abordar el resto de capítulos de la tesis. 

Tras la introducción y los preliminares, es turno del capítulo \ref{cap:clavesMinimales} en el que se presenta la primera contribución que avala este trabajo de investigación y que corresponde con el trabajo realizado en el campo de la búsqueda de las claves minimales. De forma general, este artículo presenta nuevos métodos para resolver el problema de la inferencia de claves minimales en esquema de datos basándose en la lógica \slfde y el uso de implicaciones. Además, se muestran las implementaciones y las ventajas obtenidas al aplicar técnicas de computación paralela para poder aplicar los métodos sobre conjuntos de datos de un tamaño tal que las técnicas secuenciales no son capaces de gestionar en cuanto a tiempo y recursos necesarios. Para ello se presentan los resultados obtenidos para los experimentos en entornos de supercomputación\index{supercomputación}.

A continuación, se presenta un capítulo análogo al anterior pero esta vez para el tema referente a los generadores minimales \ref{cap:clavesMinimales}. Este capítulo presenta un segundo artículo en el cual se lleva a cabo un estudio de los métodos de producción de generadores minimales basados en la lógica y el tratamiento de implicaciones. Se comprueba las mejoras de rendimiento de los métodos al aplicar reducciones en el espacio de búsqueda basadas en estrategias de poda. Al igual que en el caso de las claves minimales, se presentan las implementaciones paralelas de los métodos para poder tratar con conjuntos de datos de tamaño considerable y se incluyen las pruebas realizadas en entornos de supercomputación.

Como último capítulo dedicado a las aplicaciones desarrolladas mediante la gestión de implicaciones se presenta el capítulo \ref{cap:sistemasRecomendacion}. En este capítulo se incluye un novedoso trabajo en el que se desarrolla una aproximación al tratamiento del problema de la dimensionalidad en los sistemas de recomendación. En él, mediante el uso eficiente de la lógica \slfd, las implicaciones y los operadores de cierre, se consigue un modelo de sistema de recomendación conversacional que es capaz de gestionar el problema de la dimensionalidad reduciendo la sobrecarga de información con la que el usuario debe enfrentarse a la hora de obtener una recomendación por medio de un sistema conversacional. Así, se demuestra su buen comportamiento mediante su evaluación con conjunto de datos con información real.

Finalmente, la tesis se cierra con el capítulo \ref{cap:conclusiones} dedicado a recopilar las principales conclusiones obtenidas y a proponer caminos por los que seguir ahondando en la investigación en esta materia. Además, se incluye una relación de las referencias consultadas y los respectivos índices de términos, figuras y tablas.

En aras de la completitud, se incluyen como anexos finales aquellos artículos que han sido publicados a lo largo de este periodo de investigación, que si bien no se utilizan como respaldo para esta tesis doctoral, han sido la semilla y experiencia inicial a partir de la cual se han desarrollado los trabajos que actúan como aval.

En la Figura \ref{figura:esquemaTesis} se muestra de forma gráfica el contenido de la tesis y se contextualizan las contribuciones publicadas.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.85\textheight]{esquemaTesis.png}
	\end{center}
	\caption{Esquema de la estructura de la tesis y las publicaciones.}
	\label{figura:esquemaTesis}
\end{figure}


% =====================================================================
% =====================================================================
% =====================================================================