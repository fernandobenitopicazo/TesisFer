\pagestyle{empty}
\chapter{Introducción}\label{cap:introduccion}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}La gestión de la información es uno de los pilares fundamentales de la Ingeniería Informática. No es de extrañar, por tanto, que conforme un amplio campo de investigación y conocimiento donde diversas disciplinas como las Matemáticas, la Lógica y la Ingeniería actúen conjuntamente para alcanzar mejores sinergias.

Dentro de esta filosofía de la gestión de la información, en este trabajo de investigación nos vamos a centrar en el Análisis Formal de Conceptos (FCA, por sus siglas en inglés: \textit{Format Concept Analysis}). Podemos considerar FCA como una teoría matemática y una metodología para derivar una jerarquía de conceptos a partir de una colección de objetos y las relaciones que verifican. De esta forma, el propósito es poder representar y organizar la información de manera más cercana al pensamiento humano sin perder rigor científico. En este sentido se enmarca la cita de Rudolf Wille: ``\textit{El objetivo y el significado del FCA como teoría matemática sobre conceptos y sus jerarquías es apoyar la comunicación racional entre seres humanos mediante el desarrollo matemático de estructuras conceptuales apropiadas que se puedan manipular con la lógica.}''

El término FCA fue acuñado por Wille en 1984 culminando años más tarde con la publicación más citada al respecto en colaboración con Bernhard Ganter \cite{Ganter1997}. Desde entonces FCA se ha aplicado con éxito en diferentes disciplinas de la Ciencia, como por ejemplo: minería de datos, biología celular \cite{Endres2012}, genética \cite{Kaytoue2011}, economía, ingeniería del software \cite{Snelting1998}, medicina \cite{Motameny2008}, derecho \cite{Mimouni2015}, gestión de la información \cite{Priss2006}, etc.

La motivación principal de FCA aboga por representar conjuntos de objetos y atributos por medio de tablas de datos. Estas tablas se denominan contextos formales y representan las relaciones binarias entre esos objetos y atributos. Principalmente, existen dos formas básicas para representar el conocimiento: los retículos de conceptos y los conjuntos de implicaciones. 

Desde hace años, existen en la literatura estudios \cite{Kuznetsov2002} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir del conjunto de datos (en adelante \textit{dataset} por su nomenclatura habitual en el campo). Sin embargo, debido a que el tamaño del retículo de conceptos es, en el peor de los casos, $2^{min(|G|,|M|)}$, siendo $G$ el conjunto de objetos y $M$ el conjunto de atributos, el coste computacional de los métodos encargados de su construcción constituye una limitación de la aplicación de FCA.

Por otro lado tenemos el conjunto de implicaciones. Las implicaciones pueden considerarse \textit{grosso modo} como reglas del tipo \textit{si-entonces}, o en inglés: \textit{if-then rules}, cuya noción principal es un concepto muy intuitivo: cuando se verifica una premisa, entonces se cumple una conclusión. Esta idea básica se refleja en numerosos campos de conocimiento bajo diferentes nombres. Así, en base de datos relacionales se denominan dependencias funcionales \cite{Codd1970}, en FCA son implicaciones \cite{Ganter1997}, en programación lógica son reglas lógicas de programación \cite{Niemela1999}. No obstante, al igual que en el caso del retículo de conceptos, también existen ciertas desventajas a la hora de trabajar con implicaciones, de hecho, la propia extracción del conjunto completo de implicaciones de un dataset es una tarea que presenta una complejidad exponencial \cite{Cohen2001}. 

En cualquier caso, ambas formas (retículos e implicaciones) son capaces de representar la misma información contenida en un dataset y es posible pasar una representación a la otra. Ahora bien, trabajar con conjuntos de implicaciones nos permite aplicar técnicas automáticas basadas en la lógica. Con esta última mención, llegamos al momento donde podemos avanzar el propósito principal de esta tesis doctoral, que principalmente consiste en aplicar mecanismos lógicos sobre conjuntos de implicaciones que nos permitan desarrollar métodos para realizar un tratamiento eficiente de la información. 

En virtud de ello, en este trabajo de investigación nos vamos a centrar en la utilización del conjunto de implicaciones que se verifique en un dataset como semilla para los métodos de tratamiento eficiente de la información basados en la lógica que se han desarrollado. La aproximación a través de la lógica es posible gracias a sistemas axiomáticos válidos y completos como los Axiomas de Armstrong \cite{Armstrong74} y la Lógica de Simplificación \cite{Mora2004} (SL por sus siglas en inglés: \textit{Simplification Logic}), entre otros.

Estos métodos aplicados sobre conjuntos de implicaciones nos proporcionan la base para trabajar en esta tesis doctoral fundamentalmente sobre los siguientes tres campos de conocimiento: claves minimales, generadores minimales y sistemas de recomendación conversacionales.

Antes de entrar con mayor detalle en los anteriores tres campos, es necesario hacer una importante declaración previa. Tal y como se ha mencionado anteriormente, vamos a trabajar con el conjunto de implicaciones que se verifican en un dataset. Sin embargo, hay que dejar claro que no es competencia de este trabajo el estudiar técnicas para la extracción de estas implicaciones (lo cual es más una tarea de minería de datos), sino que la intención es partir del punto en el que ya contamos con el conjunto de implicaciones para trabajar con él. A este respecto, podemos mencionar los trabajos más citados en la literatura en relación a la extracción del conjunto de implicaciones a partir de datasets \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Son trabajos de suma importancia desde el punto de vista teórico pero también desde el punto de visto práctico, pues incluyen las implementaciones de las aplicaciones que realizan la extracción de las implicaciones.

Sin perjuicio de lo anterior, dado que hemos llegado a crear datasets propios sobre los que poder realizar experimentos como veremos más adelante, si bien no entramos en las técnicas de extracción de implicaciones que utilizan esas aplicaciones, sí hemos tenido que convertirnos en usuarios de estas aplicaciones para poder obtener el conjunto atributos e implicaciones que se verifican. Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

\section*{Claves Minimales}
\noindent
En primer lugar, se presenta el tema de las claves minimales. El concepto de clave es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}. Una clave de un esquema relacional está compuesta por un subconjunto de atributos que representan el \textit{dominio} de una determinada función cuya \textit{imagen} es la totalidad del conjunto de atributos. Estas funciones se pueden representar por medio de Dependencias Funcionales (FD, por sus siglas en inglés: \textit{Functional Dependencies}) que especifican una relación entre dos subconjuntos de atributos, e.g. $A$ y $B$, asegurando que para cualesquiera dos tuplas de una tabla de datos, si verifican $A$, entonces también verifican $B$. Nótese el cambio de denominación de implicación a dependencia funcional por estar en el entorno de los sistemas de base de datos relacionales. 

La identificación de las claves minimales de una determinada relación es una tarea crucial para muchas áreas de tratamiento de la información: modelos de datos \cite{Simsion2005}, optimización de consultas \cite{Kemper1991}, indexado \cite{Manolopoulos1999}, etc. El problema consiste en el descubrimiento de todos los subconjuntos de atributos que componen una clave minimal a partir de un conjunto de FD que se cumplen en un esquema de una tabla del modelo relacional.

Las claves no sólo son parte fundamental a considerar durante la fase de diseño de sistemas de base de datos relacionales, sino que también se consideran una poderosa herramienta para resolver multitud de problemas referentes a diversos aspectos del tratamiento de la información. Como muestras de la relevancia de este problema, encontramos numerosas citas en la literatura, entre las que podemos destacar las siguientes. En \cite{Sismanis2006}, los autores afirman que: ``\textit{identification of keys is a crucially important task in many areas of modern data management, including data modeling, query optimization (provide a query optimizer with new access paths that can lead to substantial speedups in query processing), indexing (allow the database administrator to improve the efficiency of data access via physical design techniques such as data partitioning or the creation of indexes and materialized views), anomaly detection, and data integration}''. Más recientemente, en referencia a áreas emergentes como el \textit{linked-data}, en \cite{Pernelle2013}, los autores delimitan el problema manifiestando: \textit{``establishing semantic links between data items can be really useful, since it allows crawlers, browsers and applications to combine information from different sources.''}

En la actualidad, existen diversos algoritmos capaces de resolver el problema de averiguar las claves minimales utilizando diferentes técnicas clásicas como veremos en el Capítulo \ref{cap:clavesMinimales}, sin embargo, ya adelantamos que para averiguar el conjunto de claves minimales, en este trabajo nos centraremos en aquellos algoritmos guiados por la lógica, y más concretamente, por aquellos basados en el paradigma de Tableaux \cite{Morgan1992,Risch1992}.

El problema de estos métodos de búsqueda de claves surge a la hora de tratar con grandes cantidades de información ya que el procedimiento de construcción del Tableaux produce una explosión tal del árbol del espacio de búsqueda que nos lleva a sobrepasar las capacidades de la máquina, incluso para problemas pequeños como se ha demostrado en \cite{Cordero2013}.

Sin embargo, una propiedad muy interesante de los métodos basados en Tableaux es su generación de subproblemas independientes los unos de los otros a partir del problema original. De esta forma, el uso del paradigma de Tableaux nos permite abordar estos problemas aplicando técnicas de supercomputación o de computación de alto rendimiento (HPC, por sus siglas en inglés: \textit{High Performance Computing}) mediante las cuales seremos capaces de tratar con grandes cantidades de información y obtener resultados en tiempo razonable. Entraremos más en detalle de estas técnicas, sus implementaciones y sus resultados en el Capítulo \ref{cap:clavesMinimales}.

Para esta labor de computación de alto rendimiento, hay que indicar que a lo largo de este trabajo de investigación se han adquirido y aplicado las competencias necesarias para poder trabajar en entornos de supercomputación; en concreto, durante los últimos años, se ha trabajado fervientemente con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{http://www.scbi.uma.es/}. La posibilidad de tratar con este Centro nos ha proporcionado dos beneficios fundamentales: por un lado, se ha alcanzado una elevada pericia para trabajar en entornos de HPC y para realizar implementaciones que aprovechen una alta cantidad de recursos, y por otro lado, se han podido obtener resultados empíricos sobre experimentos utilizando estrategias de paralelismo, que han desembocado en contribuciones científicas \cite{Benito-Picazo2014,Benito-Picazo2016} y que habría sido imposible conseguir en la actualidad sin contar con tales recursos computacionales.


\section*{Generadores Minimales}
Como se ha mencionado anteriormente, una forma de representar en FCA el conocimiento es el retículo de conceptos. Esta representación otorga una visión global de la información con un formalismo muy fuerte, abriendo el puerta para utilizar la teoría de retículos como una metateoría para gestionar la información \cite{Bertet2016}.

Los conjuntos cerrados son la base para la generación del retículo de conceptos ya que éste puede ser construido a partir de los conjuntos cerrados, considerando la relación de subconjuntos como la relación de orden. En este punto nace el concepto de generadores minimales como aquellas representaciones de los conjuntos cerrados que no contengan información supérflua, es decir, representaciones canónicas de cada conjunto cerrado \cite{Ganter1997}.

Los conjuntos cerrados junto con los generadores minimales son esenciales para obtener una representación completa del conocimiento en FCA, pero no sólo son interesantes desde un punto de visto teórico. La importancia de los generadores minimales puede apreciarse claramente a través de citas tales como: \textit{``Minimal generators have some nice properties: they have relatively small size and they form an order ideal''}, existente en importantes estudios como el de Poelmans et al. \cite{Poelmans2013} o \cite{Qu2007}. Además, los generadores minimales se han usado como punto clave para generar bases, las cuales constituyen una representación compacta del conocimiento que facilita un mejor rendimiento de los métodos de razonamiento basados en reglas. Missaoui et al. \cite{Missaoui2010,Missaoui2012} presentan el uso de generadores minimales para calcular bases que impliquen atributos positivos y negativos cuyas premisas son generadores minimales.

La contrapartida es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial. Sin embargo, dado que nuestro objetivo es explotar las posibilidades de operar con conjuntos de implicaciones, en este trabajo vamos a combinar ambos aspectos enumerando todos los conjuntos cerrados a partir de un conjunto dado de implicaciones. De hecho, iremos un paso más allá, calculando no sólo todos los conjuntos cerrados, sino que para cada uno de ellos produciremos sus respectivos generadores minimales. El método propuesto es una continuación del trabajo llevado a cabo en \cite{Cordero2012} sobre un método basado en la \slfd. Ese método opera sobre un conjunto de implicaciones aplicando un conjunto de reglas de inferencia construyendo un espacio de búsqueda en forma de árbol. 

Respecto a la producción de los generadores minimales, el problema de fondo que vamos a encontrar es similar al que sucedía con las claves minimales, y es que, al tratar con grandes cantidades de información, los métodos realizados para producir los generadores minimales, conllevan unas necesidades de cómputo que sobrepasan los límites de la máquina. Por tanto, una vez más, tenemos que trasladar las implementaciones de los métodos a versiones paralelas que nos permitan funcionar bajo arquitecturas de HPC. La consecución de este punto ha constituido otra fase del desarrollo de este trabajo de investigación que ha culminado con contribuciones científicas \cite{Benito-PicazoCMMSE2017}.


\section*{Sistemas de Recomendación Conversacionales}
\noindent
La última aplicación que se ha llevado a cabo en esta tesis doctoral haciendo uso de los conjuntos de implicaciones y los conjuntos cerrados se produce en el campo de los sistemas de recomendación (SR). 

El objetivo principal de los SR es ayudar al usuario a elegir entre un número alto de alternativas. Por tanto, es comprensible el tremendo crecimiento y la importancia de los SR en nuestra sociedad actual. De forma evidente, la tecnología y la ciencia han colaborado en el desarrollo y la expansión de los SR hasta el punto de llevar a estos sistemas a un lugar privilegiado en la comunidad científica estableciendo un claro campo de conocimiento respecto a la gestión de la información. 

De forma muy básica, podríamos considerar a los SR como herramientas que agrupan un amplio abanico de técnicas y aplicaciones de los sistemas de información con la intención de potenciar y favorecer la experiencia de usuario. Estos sistemas están presentes en muchas áreas diferentes de la sociedad actual (comercio electrónico, turismo, películas, música, noticias, etc.) en las que es bastante frecuente contar con gran cantidad de información. La mayoría de los SR basan sus recomendaciones en predecir cuán adecuado es un ítem para satisfacer la necesidad de un usuario. Para lograrlo, se aplican diferentes estrategias de recomendación que dan nombre a los diferentes tipos de SR que existen, entre los que destacamos los: basados en contenido, basados en conocimiento, filtrado colaborativo, basado en el contexto, conversacionales. Podemos apreciar una clasificación más detallada en el libro de Adomavicius y Tuzhilin \cite{AdomaviciusBook11} que, junto con la contribución de Bobadilla et al. \cite{Bobadilla2013}, constituyen las mayores referencias en el campo.

Sin embargo, si bien es cierto que los SR están alcanzando una enorme importancia en muchos de los aspectos relacionados con el tratamiento de la información para mejorar la experiencia de usuario, existen numerosas dificultades que han de afrontarse a la hora de diseñar e implementar un SR. En la lista de problemas relacionados con los SR \cite{Shah2016} podemos encontrar: el arranque en frío (\textit{cold-start}), privacidad, oveja-negra, escasez, etc. Cada SR puede adolecer de uno o varios de estos problemas de forma simultánea, y normalmente cada problema suele estar más emparejado con unos tipos de SR que con otros. Sin embargo, un problema común de los SR aparece cuando es necesario trabajar sobre conjuntos de datos con un alto número de características (variables o atributos). Esta situación se conoce como el fenómeno de la maldición de la dimensionalidad. En estos casos, tratar de aplicar técnicas de minería de datos (clasificación, regresión, agrupamiento, análisis de reglas de asociación, etc.) se convierte en una tarea muy compleja.

Abordar la generación de recomendaciones haciendo uso de FCA es un problema existente en la literatura desde hace años. En \cite{duBoucherRyan2006}, los autores utilizan FCA para agrupar elementos y usuarios en conceptos. Más recientemente, en \cite{Senatore2013}, los autores introducen un modelo para el filtrado colaborativo basado en FCA para generar correlaciones entre datos a través de un diseño del retículo. En \cite{LeivaERCMG13,LeivaERCMG13a}, se utilizan relaciones difusas e implicaciones ponderadas para especificar el contexto y \slfde para desarrollar un proceso lineal de filtrado que permite a los SR podar el conjunto original de elementos y así mejorar su eficiencia. Todos estos trabajos subrayan claramente cómo FCA puede usarse con éxito en el campo de los SR.

Desde el punto de vista de este trabajo, centramos nuestros esfuerzos en la aplicación de técnicas de tratamiento de implicaciones y conjuntos cerrados sobre la vertiente de SR denominada: SR conversacionales \cite{TRABELSI2011,Chen2012}. Pero no nos ocuparemos únicamente de la estrategia de recomendación sino también del proceso de cómo obtener una recomendación.

En estos sistemas, se aplica un proceso iterativo en el cual el usuario va seleccionando características que quiere que los ítems verifiquen. A través de este intercambio y usando diferentes técnicas, el sistema progresa eligiendo (o incluso prediciendo) subconjuntos de elementos que concuerdan con las preferencias del usuario, hasta alcanzar un resultado final con un número de opciones adecuado. El problema principal en estos sistemas es que, si el conjunto de datos presenta una alta dimensionalidad, el número de pasos hasta alcanzar una recomendación razonable puede ser muy alto.

La solución que hemos realizado consiste en gestionar el problema de la alta dimensionalidad mediante un proceso de selección de características guiado por el usuario (experto humano) dentro de un sistema conversacional \cite{Yu2016}. Para ello, una vez más haremos uso de una gestión inteligente de las implicaciones y de los conjuntos cerrados que nos va a permitir reducir sustancialmente el número de pasos necesarios en el diálogo entre el usuario y la aplicación para conseguir una recomendación adecuada en tiempo y forma. Para ello contaremos con el apoyo de la \slfde, presentada en varios trabajos anteriores, que fue diseñada para desarrollar métodos de deducción. En particular, utilizaremos el algoritmo del cierre de atributos \slfde \cite{Mora2012} como núcleo de un marco de selección de atributos que será el que nos permita reducir el número de etapas del diálogo.

Además, se han realizado numerosas pruebas de aplicación sobre la propuesta que se ha desarrollado. En concreto, hemos realizado pruebas en las que se han utilizado información real sobre enfermedades y fenotipos como podemos apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Pero además, el punto principal del trabajo es que la propuesta que hemos desarrollado permite actuar sobre diferentes datasets. De esta forma, podemos utilizar el mismo procedimiento para mejorar las recomendaciones conversacionales en diversos entornos como veremos más adelante. Esta versatilidad es una característica especialmente importante del trabajo realizado, ya que suele ser habitual que el desarrollo y la utilización de un SR esté muy limitado a los datos concretos que va a manejar. Este hecho conlleva muy poca libertad de maniobra en el caso de que se quieran introducir ciertos cambios, o simplemente que los datos sean diferentes. Nuestra propuesta supera esta barrera de adaptabilidad y longevidad del SR ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo necesitamos conocer el conjunto de atributos e implicaciones subyacente a los datos.

Finalmente, antes de llegar a las últimas líneas del capítulo que dedicaremos a establecer el contenido del documento y las contribuciones producidas, cabe resaltar ya en este punto la naturaleza dual de la tesis, en el sentido de que mantendremos una vía de investigación teórica y otra práctica de forma simultánea. De modo que vamos a tener una tarea teórica de investigación y asimilación de los conceptos teóricos y las técnicas necesarias, y otra empírica, en la que llevaremos a la práctica los mecanismos desarrollados hasta alcanzar resultados visibles en los dos campos de conocimiento mencionados anteriormente, bases de datos y sistemas de recomendación. Haremos especial hincapié en la parte práctica del estudio con la intención de seguir la idea inicial de realizar un trabajo que nos permita tanto alcanzar resultados de investigación satisfactorios, como dejar la puerta abierta a posibles oportunidades de transmisión de conocimiento en entornos diferentes del ámbito académico como el mercado empresarial. Pasamos entonces ahora a desglosar la estructura del documento y la producción conseguida.

\subsection*{Estructura de la Tesis}
\noindent
En este primer capítulo introductorio, hemos fijado los puntos fundamentales de la tesis, como son: el marco de trabajo sobre el que vamos a actuar, las técnicas que utilizaremos y los principales objetivos que se pretenden alcanzar. Concretamente, hemos estipulado la utilización de FCA y los conjuntos de implicaciones como base del estudio sobre la que aplicar técnicas basadas en la lógica para mejorar el tratamiento de la información.

A continuación, entraremos en el Capítulo \ref{cap:preliminares}, en el que hemos recopilado un conjunto de conceptos previos necesarios relacionados con: FCA, la lógica de simplificación, los sistemas de implicaciones y los operadores de cierre. Tras estos dos primeros capítulos, ya contaremos con el conocimiento previo necesario para abordar las dos partes principales de la tesis. 

La primera parte, dedicada a las claves y los generadores minimales, consta a su vez de dos capítulos principales, uno dedicado al estudio y las contribuciones relacionadas con las claves minimales \ref{cap:clavesMinimales}, y otro análogo para los generadores minimales \ref{cap:generadoresMinimales}. En cuanto a las claves minimales, se incluye sección con diferentes casos de aplicación donde su conocimiento es muy beneficioso. Además, se introduce el problema de la búsqueda de claves y los algoritmos que se han investigado al respecto. Cierran el capítulo dos secciones principales, en la primera se detalla la implementación realizada de los métodos de claves, tanto en su versión secuencial como paralela, y en la segunda, se detallan los experimentos y resultados alcanzados en ambos casos.

En el capítulo dedicado a los generadores minimales \ref{cap:generadoresMinimales} se procede de forma análoga al de claves. Así, se presenta el problema de calcular los generadores minimales y los métodos que se han investigado y desarrollado a lo largo del trabajo realizado. A continuación, se detallan los experimentos llevados a cabo para probar el rendimiento de estos métodos. Finalmente, se presentan implementaciones paralelas de los métodos y sus respectivos resultados.

La segunda parte de la tesis estará dedicada al estudio realizado sobre el campo de los sistemas de recomendación y las contribuciones alcanzadas. En este sentido, se incluye una primera introducción al campo de conocimiento y al estado del arte en el Capítulo \ref{cap:sistemasRecomendacion}. Seguidamente, en el Capítulo \ref{cap:desarrolloSR} se detalla el proceso llevado a cabo para desarrollar un SR conversacional, incluyendo explicaciones concretas sobre su funcionamiento y las medidas de evaluación utilizadas para calcular su rendimiento. Tras ello, llegaremos al Capítulo \ref{cap:experimentosSR} donde se exponen los experimentos llevados a cabo haciendo uso del SR conversacional junto con los resultados obtenidos. Podemos considerar este capítulo de importancia superior pues recoge el resultado de aplicar los conceptos teóricos en aras de obtener una aplicación real de ingeniería.

Para finalizar, cerraremos la tesis con un último capítulo \ref{cap:conclusiones} dedicado a recopilar las principales conclusiones obtenidas y a proponer caminos por los que seguir ahondando en la investigación en esta materia. Además, se incluye una relación de las referencias consultadas y los respectivos índices de figuras, términos y tablas.
