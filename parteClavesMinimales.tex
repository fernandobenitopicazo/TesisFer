% this file is called up by thesis.tex
% content in this file will be fed into the main document

\pagestyle{empty}

%: ----------------------- name of chapter  -------------------------
\chapter{Claves Minimales}
\label{cap:clavesMinimales} % top level followed by section, subsection


%: ----------------------- paths to graphics ------------------------

% change according to folder and file names
%\ifpdf
%    \graphicspath{{X/figures/PNG/}{X/figures/PDF/}{X/figures/}}
%\else
%   \graphicspath{{X/figures/EPS/}{X/figures/}}
%\fi

%: ----------------------- contents from here ------------------------


\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Identificar las claves de un esquema relacional es una tarea imprescindible dentro de numerosas áreas diferentes de la gestión de la información. Ejemplos de ello nos remontan incluso décadas atrás, donde podemos ver que las claves constituyen un elemento fundamental en acciones como pueden ser: la optimización de consultas \cite{Kemper1991}, el indexado \cite{Kemper1991} o el modelado de datos \cite{Simsion2005}, pero el concepto y sus aplicaciones son igualmente extensibles a sistemas actuales. 

Generalmente, es tarea de ingeniería establecer y elegir las claves como parte de la normalización del esquema. El reto es encontrar esos atributos del esquema que nos permitan identificar de forma única cada tupla de la relación.

% Clave
Técnicamente, una clave de un esquema relacional está compuesta por un subconjunto de atributos que actúan como el dominio de una función cuya imagen es el propio conjunto de atributos. Estas funciones se describen como dependencias funcionales (en adelante FD, por sus siglas en inglés, \textit{Functional Dependencies}), las cuales especifican una restricción entre los dos conjuntos de atributos, que denotamos como $A \rightarrow B$, y que nos asegura que para cualquier tupla de la relación, si la tupla verifica el antecedente $A$, entonces también verifica el consecuente $B$. Tal y como hemos visto en los capítulos introductorios, este definición coincide exactamente con el concepto de \textit{if-then rules} que hasta ahora hemos introducido según la denominación de implicación por hallarnos en el marco de FCA, sin embargo, esta vez, la diferencia de nombre se debe simplemente al entorno en el que ahora vamos a trabajar, esto es, las bases de datos relacionales, donde se habla de dependencias funcionales para este tipo de reglas. Se expone formalmente en las siguientes definiciones.

\begin{definicion}[Atributo]
Un atributo $A$ es un identificador para un elemento en un dominio $D$.
\end{definicion}

\begin{definicion}[Dependencia funcional]
Sea $\Omega$ un conjunto de atributos. Una dependencia funcional (FD) sobre $\Omega$ es una expresión de la forma $X \rightarrow Y$, donde $X, Y\subseteq \Omega$. Se satisface en una tabla $R$ siempre y cuando para cada dos tuplas de $R$, si verifican $X$, entonces también verifican $Y$.
\end{definicion}

\begin{definicion}[Dependencia funcional trivial]
Sea $\Omega$ un conjunto de atributos. Una dependencia funcional (FD) $X \rightarrow Y$ sobre $\Omega$, se denomina trivial si $Y\subseteq X$.
\end{definicion}

\begin{definicion}[Dependencia funcional unitaria]
Sea $\Omega$ un conjunto de atributos. Una dependencia funcional (FD) $X \rightarrow Y$ sobre $\Omega$, se denomina unitaria si $Y$ es un conjunto unitario.
\end{definicion}

Las claves nos permiten identificar de forma unívoca cada una de las tuplas que existan en una relación y podemos definirlas por medio de FD como sigue:

\begin{definicion}[Clave]
Dada un tabla $R$ sobre un conjunto de atributos $\Omega$, decimos que $K$ es un clave de $R$ si se verifica la dependencia funcional $K\rightarrow \Omega$ en $R$.
\end{definicion}

Una característica muy importante de la claves es su minimalidad. Una clave se considerará minimal cuando todos y cada uno de los atributos que la forman son imprescindibles para mantener su naturaleza de clave, es decir, no contiene ningún atributo superfluo. Formalmente:

\begin{definicion}[Clave minimal]
Dada un tabla $R$ sobre un conjunto de atributos $\Omega$, decimos que $K$ es una clave minimal de $R$ si se verifica la dependencia funcional $K\rightarrow \Omega$ en $R$ y $\not\exists a \in K$ tal que $K \smallsetminus \{a\} \rightarrow \Omega$.
\end{definicion}

\begin{remark}
Es de vital importancia mencionar que aunque para averiguar el conjunto de claves vamos a trabajar sobre FDs, no es objetivo de esta Tesis extraerlas a partir de un esquema relacional. Para ello, existen desde hace tiempo numerosas técnicas en la literatura que se ocupan de realizar esta tarea \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Por tanto, haremos uso de estas técnicas a la hora de enfrentarnos con un esquema relacional para obtener el conjunto de FD que se verifican sobre esos datos de forma que tengamos la semilla sobre la que aplicar las técnicas de búsqueda de claves que son motivo de estudio en esta tesis.
\end{remark}

Para ilustrar de forma básica el concepto de clave, vamos a utilizar el siguiente Ejemplo \ref{ejemplo:basicoClaves}.

\begin{ejemplo}
\label{ejemplo:basicoClaves}
Supongamos que disponemos de la Tabla \ref{tabla:ejemploPeliculas}. Es una pequeña tabla donde se refleja información que relaciona títulos de películas, actores, países, directores, nacionalidad y años de estreno. 

De esta información, utilizando los métodos comentados anteriormente, podemos extraer el siguiente conjunto de FDs: 

$\Gamma = \{Titulo, A\tilde{n}o \rightarrow Pais$;  $Titulo, A\tilde{n}o \rightarrow Director$; $Director\rightarrow Nacionalidad$\}. 

\begin{table*}[htbp]
\caption{Tabla de películas}
\label{tabla:ejemploPeliculas}
\centering
{\scriptsize
\begin{tabular}{cccccc}
 \hline
 Título & Año & País & Director & Nacionalidad & Actor\\
 \hline
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & John Travolta\\
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & Uma Thurman\\
  Pulp Fiction & 1994 & USA & Quentin Tarantino & USA &Samuel Jackson \\
 King Kong & 2005 & NZ & Peter Jackson& NZ & Naomi Watts\\
 King Kong & 2005 & NZ & Peter Jackson & NZ & Jack Black\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jessica Lange\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jeff Bridges\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Jamie Foxx\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Samuel Jackson\\\hline
\end{tabular}
}
\end{table*}

Esta tabla tiene una sola clave minimal: $\{Titulo, A\tilde{n}o, Actor\}$ que corresponde con el conjunto de atributos necesario para identificar cualquier tupla de la relación.
\end{ejemplo}

Aquellos lectores que no estén familiarizados con las nociones formales de FD, claves y tablas relacionales pueden consultar uno de los trabajos más citados al respecto en \cite{Elmasri2010}.

Es conveniente aclarar que el trabajo llevado a cabo no es una cuestión de minería de datos \cite{Fay96}. De forma muy general podríamos decir que la minería de datos puede considerarse un proceso computacional para descubrir patrones en grandes volúmenes de datos con el objetivo de extraer información de un conjunto de datos y transformarla en estructuras para diversos usos \cite{witten2011}. Sin embargo, nuestra labor consiste en desarrollar, a partir de la información ya extraída de los datos, los mecanismos y algoritmos necesarios para encontrar las claves de los conjuntos de datos, y en general, descubrir el conocimiento implícito, que nos permitan realizar un tratamiento más inteligente y eficiente de la información. 

En este capítulo, analizaremos el problema de la búsqueda de claves \ref{sec:problemaBusquedaClaves} y el estado del arte \ref{sec:metodosSSTCK}, pero antes vamos a mostrar algunas situaciones donde la existencia de este problema es relevante \ref{sec:aplicaciones}. Cerrarán este capítulo dos secciones más; la primera es realmente importante ya que presenta la implementación de los métodos y la filosofía de computación paralela que se ha utilizado \ref{seccion:implementacion} y en la última se recogen en diversas tablas los resultados obtenidos por cada uno de los métodos \ref{seccion:experimentosResultados}.



\section{Aplicaciones}
\label{sec:aplicaciones}
Antes de seguir avanzando, es momento de apoyar el valor intrínseco de los datos mostrando una situaciones de ejemplo donde conocer las claves del sistema es fundamental.

\begin{ejemplo}
\label{ejemplo:basicoAplicacionClaves}
Sea el caso en que tenemos almacenados los datos personales de los empleados de una determinada empresa (e.g. nombre, edad, DNI, teléfono, etc.). Se podría pensar que una forma de identificar a uno de los empleados podría ser a través de su nombre y apellidos. Sin embargo, esto no sería correcto puesto que hay personas diferentes cuyos nombres y apellidos pueden coincidir. Tal podría ser el caso de nombres más habituales tanto dentro del territorio nacional como Antonio Fernández o fuera, como Peter Williams. Otra alternativa podría ser elegir el número de teléfono, pero también podría ser un fallo en tanto en cuanto una misma familia o compañeros de trabajo pueden compartir el mismo número de línea fija de teléfono. 
\end{ejemplo}

En este ejemplo, el elemento que nos permite identificar de forma única a un empleado es el DNI, y por tanto, ésa debe ser la clave de nuestro esquema. Y además, puesto que solamente contiene la información necesaria para ser clave, estamos ante una clave minimal. Sin embargo, no es la única clave que existe. Pensemos que si tomamos combinaciones de valores como \textit{DNI} y \textit{Apellidos}, también estaríamos obteniendo una clave válida del esquema, pero en este caso, puesto que tenemos información que no es imprescindible (e.g. \textit{Apellidos}), no estaríamos ante una clave minimal.


\begin{ejemplo}[Optimización de consultas]
Dependiendo del sistema de información con el que estemos tratando, la complejidad que pueden alcanzar algunas consultas puede ser tal que su tiempo de respuesta no sea admisible. Un ejemplo de una consulta de cierta complejidad dentro de un hipotético modelo sanitario a nivel nacional, podría consistir en solicitar al sistema la lista de los varones mayores de 40, que hayan tenido al menos una intervención quirúrgica, estén casados, posean vivienda propia, tengan al menos dos hijos, ... 
\end{ejemplo}

Este tipo de consultas pueden necesitar acceder a tal cantidad de recursos que a la hora de obtener una respuesta del sistema, el tiempo de espera puede ser intratable. Por tanto, tener el sistema bien diseñado de manera que las consultas se puedan hacer de forma eficiente será un aspecto fundamental del sistema; una de las formas principales de conseguir esto es mediante un buen diseño que nos permita decidir a través de qué elementos dirigir la búsqueda, es decir, conocer las claves del modelo de datos \cite{Elmagarmid2007}.

\vspace{0.3cm}
Un aspecto muy importante en las tecnologías de la información es tener mecanismos que nos permitan detectar errores que pueden producirse en la gestión y almacenamiento de la información. Estos errores pueden deberse a multitud de causas y no sólo se producen a la hora de diseñar un nuevo sistema, sino que pueden aparecer a lo largo del tiempo de vida de un sistema ya sea por el uso prolongado, cambios en las tecnologías, la intervención de diferentes personas, etc. En estos casos, es imprescindible tener mecanismos para encauzar la búsqueda del error y para ello, es fundamental conocer las claves del sistema \cite{Atencia2012}. Para mostrar una situación en la que conocer las claves nos puede ayudar en la detección de errores supongamos la siguiente situación.

\begin{ejemplo}[Detección de errores]
Suele ser común que se produzcan errores en bases de datos debido a las peculiaridades de algunos nombres personales. Los más comunes suelen ser la repetición de registros. Por ejemplo, pensemos una editorial que tenga una base de datos enorme de artículos científicos donde se almacena simplemente el nombre de los autores y el de las publicaciones. Ahora, pongamos por caso que en una publicación el nombre de uno de los autores es Aurora Manjón Ramos, mientras que en otra perteneciente a la misma autora, el nombre registrado es Aurora Manjón-Ramos. Si queremos obtener las aportaciones de este autor en este sistema, habrá que tener en cuenta la sutil diferencia en la forma de los apellidos pues de lo contrario la lista de artículos no será consistente. 
\end{ejemplo}

Esto ocurre porque el sistema está abierto al fallo humano al no tener mecanismos para bloquear la posible duplicidad de registros. Actualmente y en relación a este caso particular, el sistema de control DOI elaborado por la Corporación Nacional para Iniciativas de Investigación (CNRI) es un claro ejemplo de una forma de clave con la que identificar los trabajos. En definitiva, poseer una clave que nos determine cómo se introduce nueva información en el sistema será esencial para evitar este tipo de situaciones de error.



\section{El problema de la búsqueda de claves}
\label{sec:problemaBusquedaClaves}
% Problema de la búsqueda de claves
El problema de la búsqueda de claves consiste en encontrar todos los subconjuntos de atributos que componen una clave mínimal a partir de un conjunto de FD que se verifican en un esquema de una tabla de de datos relacional. Es un campo de estudio con décadas de antigüedad en el que podemos remontarnos a un primer estudio preliminar en \cite{Fadous75}, donde las claves se estudiaron dentro del ámbito de la matriz de implicaciones u otros tantos trabajos como \cite{Sali2004,Giannella99} que se centran en averiguar estas claves minimales.

% Claves como elemento crucial
Las claves constituyen un elemento crucial en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}. Sin embargo, la dificultad al enfrentarnos con el problema de la búsqueda de claves surge debido a que, dado un conjunto de atributos $A$, la cardinalidad del conjunto $2^A$ hace que haya que abordar el problema aplicando técnicas que guíen la búsqueda de los conjuntos candidatos a ser claves minimales. 

% problema NP
El cálculo de todas las llaves minimales representa un problema complejo. En \cite{Lucchesi78,Yu76} se incluyen resultados interesantes acerca de la complejidad del problema; los autores demuestran que el número de claves minimales para un sistema relacional puede ser exponencial respecto al número de atributos, o factorial respecto al número de dependencias. Además, establecieron que el número de claves está limitado por el factorial del número de dependencias, por tanto, no existe un algoritmo que resuelva el problema en tiempo polinómico. Hay otros resultados que apoyan la complejidad del problema; es un problema NP-completo decidir si existe una clave de tamaño a lo sumo $k$ dado un conjunto de FD \cite{Lucchesi78}.

% referencias generales
Las principales referencias sobre este problema apuntan a los trabajos de Lucchesi y Osborn en \cite{Lucchesi78} que muestran un algoritmo para calcular todas las claves candidatas. Por otro lado, Saiedian y Spencer \cite{Saiedian1996} presentaron un algoritmo usando grafos con atributos para encontrar todas las claves posibles de un esquema de base de datos relacional. No obstante, demostraron que sólo podía aplicarse cuando el grafo de FDs no estuviera fuertemente conectado. Otro ejemplo lo encontramos en el trabajo de Zhang \cite{Zhang09} en el cual se utilizan mapas de Karnaugh \cite{Karnaugh1953} para calcular todas las claves. También existen trabajos más recientes sobre el cálculo de las claves minimales como son \cite{Sismanis2006,Worland2004} y otra contribución actual que aborda el problema en un estilo lógico \cite{CorderoEMG14}. Asimismo, en \cite{Levy2005,Valtchev03,Valtchev08} los autores propusieron el uso de FCA \cite{Ganter1997} para abordar problemas relacionados con la búsqueda y la gestión de las implicaciones, que pueden considerarse complementarios a nuestro trabajo.

Es significativo como el problema de la búsqueda de claves aparece en diversos campos de conocimiento. Por ejemplo, en \cite{Benito-PicazoCMMSE2015} se hace mención a la importancia de conocer las claves en áreas emergentes como el \textit{linked-data}. Por otro lado, en \cite{CorderoEMG14}, mostramos cómo el problema de las claves mínimales en las bases de datos tiene su análogo en FCA, donde el papel de las FDs se manifiesta, como ya hemos comentado, como implicaciones de atributos. En ese artículo, el problema de las claves mínimales se presentó desde un punto de vista lógico y para ello se empleó un sistema axiomático para gestionar las FDs y
las implicaciones; este sistema es el que en apartados anteriores hemos presentado como \slfde \cite{Enciso2002}.

% referencias de tableaux
En nuestro objetivo de esta parte de la Tesis nos vamos a concentrar en los algoritmos de búsqueda de claves basados en la lógica, y más específicamente, en aquellos que utilizan el paradigma de Tableaux \cite{Morgan1992} para determinar las claves de un esquema relacional utilizando un sistema de inferencia. 

%De forma muy general, podemos considerar al Tableaux como un árbol de búsqueda cuyas ramas se van formando por la acción de diferentes reglas de inferencia y cuyas hojas contienen las claves del esquema relacional original.

De forma muy general, podemos decir que los métodos tipo Tableaux representan el espacio de búsqueda como un árbol, donde sus hojas contienen las soluciones (claves). El proceso de construcción del árbol comienza con una raíz inicial y desde allí, las reglas de inferencia generan nuevas ramas etiquetadas con nodos que representan instancias más simples del nodo padre. La mayor ventaja de este proceso es su versatilidad, ya que el desarrollo de nuevos sistemas de inferencia nos permiten diseñar un nuevo método. Las comparaciones entre estos métodos se pueden realizar fácilmente ya que su eficiencia va de la mano con el tamaño del árbol generado.

Esto nos lleva a un punto de partida fundamental, los estudios de R. Wastl (Universidad de Wurzburg, Alemania) \cite{Wastl98a,Wastl98} donde se introduce por primera vez un sistema de inferencia de tipo Hilbert para averiguar todas las claves de un esquema relacional. A modo de ejemplo básico, en la Figura \ref{figura:ejemploTaleaux} podemos ver un ejemplo de árbol de búsqueda según el paradigma de Tableaux desarrollado según las reglas de inferencia del sistema de inferencia $\mathbb{K}$ de Wastl.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=.75\textwidth,height=.3\textheight]{arbol897.png}
	\end{center}
	\caption{Ejemplo de Tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
	\label{figura:ejemploTaleaux}
\end{figure}

Siguiendo esta línea, en \cite{Cordero2013} los autores abordan el problema de la búsqueda de claves utilizando un sistema de inferencia basado en la lógica de simplificación para dependencias funcionales \cite{Enciso2002}. En \cite{Mora2012} los autores muestran la equivalencia entre \slfde y los axiomas de Armstrong \cite{Armstrong74} junto con un algoritmo para calcular el cierre de un conjunto de atributos. Los autores también exponen una comparación con otros algoritmos del cierre que aparecen en la literatura para demostrar la orientación práctica del sistema lógico.

Más tarde, en \cite{CorderoEMG14}, los autores introdujeron el método SST, basado en la introducción del test de minimalidad que evita la apertura de ramas adicionales del árbol, por lo que el espacio de búsqueda se vuelve más reducido, logrando un gran rendimiento en comparación con sus predecesores.

% con el paralelo
Recordemos que para abordar el problema de la búsqueda de claves en grandes sistemas, nuestro objetivo era utilizar las técnicas lógicas sobre una implementación paralela de los métodos que, mediante el uso de recursos de supercomputación, nos permitan alcanzar resultados en un tiempo razonable. En esta línea, son varios los trabajos que han utilizado la paralelización para afrontar problemas relacionados con implicaciones o FCA. Un algoritmo paralelo para el tratamiento de implicaciones enmarcado en el campo de los hipergrafos lo podemos encontrar en \cite{Sridhar1990}. A su vez, Krajca et al. \cite{Krajca2008} presentan un algoritmo paralelo para el cálculo de conceptos formales. Por nuestra parte, una primera aproximación a la paralelización del método de Wastl \cite{Wastl98a,Wastl98} y el algoritmo de claves \cite{Cordero2013} fue presentado en \cite{Benito-Picazo2014}, donde se muestra cómo el paralelismo puede integrarse de forma natural en los métodos basados en tableaux. Fue este el punto de partida para el desarrollo de los nuevos métodos más eficientes que vemos a continuación.




\section{Algoritmos para el cálculo de claves}
\label{sec:metodosSSTCK}
% nuestra contribución
Nuestra contribución principal en relación a esta parte de la tesis se produce de la siguiente forma. Basándonos en el sistema axiómatico de la lógica \slfde \ref{sec:logicaSimplificacion}, proponemos un nuevo método llamado \textit{Closure Keys (CK)} que incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde para mejorar el método SST. El nuevo método se basa en la relación fuerte entre la noción de clave y el operador de cierre, no sólo a nivel de definición, sino también en cuanto a su construcción. 

El operador de cierre definido en \cite{Mora2012} y que hemos introducido previamente en el capítulo de Preliminares \ref{cap:preliminares} como \cierre, nos permite reducir el espacio de búsqueda realizando reducciones en el camino hacia las hojas, donde finalmente se obtienen las claves. Además, se ha desarrollado una implementación paralela tanto del método SST como del método CK junto con varios experimentos, los cuales necesitan llevarse a cabo en entornos de supercomputación, para confirmar las mejoras que se han alcanzado.


\subsection*{Método SST}
\label{subsec:metodoSST}
En \cite{CorderoEMG14} se presentó un nuevo algoritmo, denominado SST, para calcular todas las claves minimales usando una estrategia de estilo tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. SST se basa en la noción de cierre de conjunto, una noción básica en la teoría de base de datos que permite caracterizar el conjunto máximo de atributos que se puede alcanzar, desde un determinado conjunto de atributos $A$ con respecto a un conjunto de FD, utilizando el sistema axiomático. Por lo tanto, si el cierre de $A$ se denota como $A^+_\Gamma$, el sistema de inferencia para FD nos permite inferir la FD $A\to A^+_\Gamma$. El enfoque con estilo lógico para el problema de las claves minimales consiste en la enumeración de todos los conjuntos de atributos $A$ tales que se verifique la FD: $A\to\Omega$.

El método SST presenta dos mejoras fundamentales con respecto a sus predecesores. En primer lugar, consigue una reducción en el número de ramas y la segunda mejora consiste en el desarrollo de potentes reglas de inferencia que reducen la profundidad de las ramas, lo cual acelera significativamente la búsqueda de las claves.
Estas dos nuevas reglas de inferencia, denominadas {\tt{[sSimp]}} y {\tt{[lSimp]}} y que se muestran a continuación son dos extensiones concretas de la regla de simplificación de la \slfd. 

\[[sSimp] \quad \frac{A \rightarrow B \quad C \rightarrow D}{A(C \smallsetminus B) \rightarrow D \smallsetminus AB} \quad [lSimp] \quad \frac{A \rightarrow B \quad C \rightarrow D}{A(C \smallsetminus B) \rightarrow D}\]

Estas reglas guían la construcción del espacio de búsqueda para descubrir todas las claves minimales \cite{CorderoEMG14}. El método avanza paso a paso construyendo un árbol desde el problema original hasta llegar a las hojas donde encontramos las soluciones, es decir, las claves. Las reglas se aplican a cada par (conjunto de atributos, conjunto de implicaciones) correspondiente a cada nodo del árbol para producir nuevos nodos en un nivel inferior, que serán el origen de nuevas ramas del árbol de búsqueda. 

Al aplicar {\tt{[lSimp]}} sobre el subconjunto de atributos en cada nodo y la implicación en el borde correspondiente, obtenemos la nueva raíz en la rama. Por ejemplo, en la Figura \ref{figura:aplicacionAlgoritmoCK}, a partir de $\Omega$ y $A_1\to B_1$, obtenemos el nuevo subconjunto $\Omega_1$.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[height=.2\textheight,width=.95\textwidth]{aplicacionAlgoritmoCK.png}
	\end{center}
	\caption{Aplicación de las reglas {\tt{[sSimp]}} y {\tt{[lSimp]}} en el algoritmo \textit{Closure Keys}.}
	\label{figura:aplicacionAlgoritmoCK}
\end{figure}

Por otra parte, la aplicación de {\tt{[sSimp]}} sobre la implicación establecida en cada nodo se hace tomando cada implicación como pivote y aplicando la regla al resto de las implicaciones para generar nuevas ramas. Podemos verlo en la Figura \ref{figura:aplicacionAlgoritmoCK}, donde la primera rama se genera tomando $A_1\to B_1$ como pivote y aplicándolo al resto de implicaciones $A_2\to B_2,\ldots,A_n\to B_n$. Cuando el conjunto de implicaciones en un nodo es el conjunto vacío, hemos llegado a una hoja del árbol y por tanto, a una clave, que agregamos a la solución global. A continuación mostramos un ejemplo completo de aplicación.

\begin{ejemplo}
\label{ejemploTableauxGrande}
Sea un conjunto de atributos $U = \{a,b,c,d,e,g\}$ y un conjunto de implicaciones $\Gamma = \{ab \rightarrow c, bc \rightarrow d, be \rightarrow c, cg \rightarrow b, c \rightarrow a, d \rightarrow eg, ce \rightarrow g\}$. El conjunto $K$ de todas las claves minimales que se extraen es $K = \{cd, bc, ce, cg, bd, ab, be\}$ y el Tableaux que se genera pondemos verlo en la Figura \ref{figura:arbolGrande}. 

Obsérvese que el primer nivel del árbol tiene sólo cuatro hijos, que corresponden a las cuatro fórmulas minimales en $\Gamma$. En el caso del método \slfde estaríamos hablando de 7 hijos en este primer nivel. La eficaz estrategia proporcionada por el método SST y sus nuevas reglas de inferencias redunda en el éxito en la construcción de todo el árbol. De esta forma, el árbol final tiene 21 nodos mientras que en el caso del método \slfde hablamos de 244.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[height=.99\textheight,width=.9\textwidth]{ejemploSLFDMod.png}
	\end{center}
	\caption{Tableaux completo para los datos del ejemplo \ref{ejemploTableauxGrande}.}
	\label{figura:arbolGrande}
\end{figure}
\end{ejemplo}


SST muestra un gran rendimiento en comparación con sus predecesores como hemos visto hasta ahora y como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}. El beneficio principal en la reducción del espacio de búsqueda de debe a la introducción del test de inclusión para evitar la apertura de ramas extra. Gracias a ello, SST no abre algunas ramas que sabemos que van a producir las mismas claves que se calculan en otra rama. Poder identificar que tales ramas resultarán en hojas con información duplicada no es una tema trivial. Para abordarlo, hemos definido la noción de implicación minimal con respecto a un conjunto de implicaciones.

\begin{definicion}
Sea $\Gamma$ un conjunto de implicaciones. La implicación $A \to B \in \Gamma$ es minimal si $\forall C \to D \in \Gamma$, se cumple que $C \not\subset A$.
\end{definicion}

Podemos apreciar un ejemplo básico de cómo SST supera a su predecesor en la \slfd, comparando los resultados obtenidos entre las Figuras \ref{figura:arbolSLFD} y \ref{figura:arbolSST}, para \slfde y SST respectivamente.

\begin{figure}
	\centering
		\includegraphics*[width=.99\textwidth,height=.3\textheight]{arbolSLFD.png}
		\caption{Método \slfd. Tableaux de 3 niveles. 37 nodos. 18 hojas.}
		\label{figura:arbolSLFD} 
\end{figure}

\begin{figure}
	\centering
		\includegraphics*[width=.99\textwidth,height=.3\textheight]{arbolSLFDMod.png}
		\caption{Método SST. Tableaux de 2 niveles. 19 nodos. 12 hojas.}
		\label{figura:arbolSST}
\end{figure}




\subsection*{Método CK}
\label{subsec:metodoCK}
El objetivo principal para seguir progresando en esta línea es la reducción del espacio de búsqueda que se puede cuantificar a través del número de nodos en el árbol. Con esta intención, hemos estudiado cómo reducir aún más el tamaño del árbol al acortar su profundidad. El resultado es este nuevo método \textit{CK} cuyo núcleo es el algoritmo del cierre lógico publicado en \cite{Mora2012} y que ya hemos visto como \cierree puede considerarse un novedoso enfoque a los métodos clásicos de cierre, ya que proporciona una nueva característica: además del conjunto de atributos que constituye la salida del operador de cierre, el método proporciona un subconjunto de implicaciones del conjunto $\Gamma$ original. Este nuevo subconjunto de implicaciones reúne el conocimiento que puede considerarse como el complemento del cierre y se puede usar de una manera inteligente para encontrar claves minimales por medio del operador de cierre.

La ventaja principal del método es que recibe un conjunto de implicaciones $\Gamma$ y un subconjunto de atributos $X \subseteq \Omega$. Calcula el conjunto cierre $X^+$ respecto a $\Gamma$, y además, un nuevo conjunto $\Gamma^\prime$ que contiene el conjunto de implicaciones que guarda la semántica que queda fuera del cierre $X^+$. Si $\Gamma^\prime = \varnothing$, entonces $X^+ = \Omega$ (véase \cite{Mora2012} para más detalles).

Por tanto, proponemos aplicar el \cierree a cada implicación minimal del conjunto $\Gamma$ en cada nodo para abrir nuevas ramas con el resultado producido por el cierre \cierre. La definición de nuestro nuevo método de claves minimales se presenta en el Algoritmo \ref{algoritmo:CK}.

\begin{algorithm2e}
{\scriptsize
%\DontPrintSemicolon
\KwData{

$\Omega$ un conjunto de atributos.

$\Gamma$ un conjunto de implicaciones.

${\cal C}$ una variable acumuladora con un subconjunto de atributos.

${\mathcal K}$ una variable acumuladora con conjuntos de atributos.}
%\KwResult{${\mathcal K}$, set of all minimal keys.}
\Begin{
        ${\cal A}$ := $\Omega$\\
        %${\mathcal K}:= \varnothing $\\
        \lIf{$\Gamma=\varnothing$}{\\             
        \ \ \ \ Add(${\mathcal K}, \{{\cal A}\cup {\cal C}\})$
            %{\Return ${\cal A}$}
        }\bf else{\\
        \ \ \ \ \ForEach{$X \to Y \in \text{[IM]}(\Gamma)$}{
        	\ \ \ \ $<X^\prime,\Gamma^\prime>$ := \cierre $(X,\Gamma)$\\
          \ \ \ \ ${\cal C}:={\cal C}\cup X$\\
        	\ \ \ \ {\bf Closure\_Keys}($X^\prime,\Gamma^\prime, {\cal C}, {\mathcal K}$)\\
        				}
        }
             %   \Return  ${\scriptsize \tt{Minimal}} (\mathcal K)$\\

        /* Donde [IM]$(\Gamma)$ denota las implicaciones en $\Gamma$ que son minimales. */
}
}
\caption{Closure Keys (CK)}
\label{algoritmo:CK}
\end{algorithm2e}

La llamada principal del Algoritmo \ref{algoritmo:CK} debe ser Closure\_Keys($\Omega,\Gamma,\varnothing,\varnothing$). A continuación, para obtener todas las claves mínimales, recogemos los elementos minimales del cuarto parámetro (que actúa como un acumulador en modo de entrada/salida) de esta llamada a procedimiento con respecto a $(2^\Omega,\subseteq)$. El siguiente teorema asegura que el Algoritmo \ref{algoritmo:CK} proporciona un método válido y completo.

\begin{theorem}\label{ThGordo}
Sea $\Omega$ un conjunto de atributos y $\Gamma$ un sistema de implicaciones sobre $M$. Entonces, el Algoritmo \ref{algoritmo:CK} devuelve todas las claves minimales.
\end{theorem}

\begin{demostracion}
En primer lugar, la terminación del algoritmo está garantizada porque, en cada llamada recursiva de Closure\_Keys, la cardinalidad del sistema de implicaciones se reduce estrictamente. Además, el número de llamadas recursivas está limitado por la cardinalidad de $\Gamma$. El núcleo del método es el cierre \cierre \cite{Mora2012} que proporciona un resultado compuesto, es decir, $\cierre(X,\Gamma)= <X^+, \Gamma^\prime>$.

Este método genera todas las claves minimales porque, en cada paso produce el cierre $X^+$ y si resulta que ese cierre no es el conjunto $\Omega$ completo, procedemos seleccionando un nuevo antecedente del conjunto de implicaciones $\Gamma^\prime$. En virtud de esto, una vez que alcanzamos una hoja, la unión de los antecedentes en el camino conforma una clave y por lo tanto, el método es \emph{válido}.

Con respecto a la \emph{completitud}, cualquier clave del sistema se va a encontrar ya que el espacio de búsqueda inducido por nuestro método realiza una búsqueda exhaustiva.

\QED
\end{demostracion}

Al igual que en el apartado anterior donde mostramos un ejemplo de comparación entre el árbol generado por el método \slfde y el método SST (Figura \ref{figura:arbolSLFD} y \ref{figura:arbolSST}, respectivamente), un ejemplo ilustrativo que muestra la reducción en el árbol proporcionado por CK con respecto al método SST se muestra en las figuras \ref{figura:arbolCK} y \ref{figura:arbolSST} respectivamente.

\begin{figure}
	\centering
		\includegraphics*[width=.99\textwidth,height=.4\textheight]{arbolSST.png}
		\caption{Ejemplo completo de aplicación utilizando el método SST.}
		\label{figura:arbolSST} 
\end{figure}

\begin{figure}
	\centering
		\includegraphics*[width=.99\textwidth,height=.25\textheight]{arbolCK.png}
		\caption{Ejemplo completo de aplicación utilizando el método CK donde se aprecia la mejora obtenida por CK sobre SST en cuando a la reducción del árbol de búsqueda, pasando de 21 nodos en SST a 11 con CK.}
		\label{figura:arbolCK}
\end{figure}


\section{Implementación}
\label{seccion:implementacion}
Como conclusión de los ejemplos presentados en la sección anterior, vemos que incluso para ejemplos básicos, el árbol de búsqueda va alcanzando dimensiones considerables. De hecho, podemos consultar estudios previos donde el método \slfde llega a construir árboles de millones de nodos \cite{Benito-Picazo2014}. Por tanto, para poder salvar este escollo y utilizar los métodos sobre grandes cantidades de datos, vamos a utilizar estrategias de paralelismo sobre arquitecturas hardware con altos recursos.

Nuestra intención es aprovechar el diseño del Tableaux en nuestros métodos para dividir el problema original en instancias atómicas que pueden resolverse por separado en un tiempo razonable y con los recursos disponibles. El resultado con todas las claves minimales lo obtenemos a partir de unificar los resultados individuales obtenidos para cada una de esas instancias atómicas. 

De esta forma, las implementaciones paralelas de los algoritmos funcionarán en dos etapas:

\begin{enumerate}
	\item \textbf{Etapa de división (secuencial).} Ejecuta el método de búsqueda de claves, en vez de construir el árbol entero, se detendrá en un cierto nivel determinado según un valor de corte (ver a continuación) generando un conjunto de sub-problemas, uno por nodo del árbol en ese nivel. El resultado de esta etapa es un conjunto de problemas de búsqueda de claves más simples.
	\item \textbf{Etapa de resolución (paralela).} En esta segunda etapa, ejecutamos el algoritmo de búsqueda de claves sobre cada uno de los sub-problemas generados en la etapa anterior de forma paralelavy, al final, combinamos todas las soluciones para obtener todas las claves minimales.
\end{enumerate}

La Figura \ref{figura:diagramaParalelo} muestra un esquema conceptual de la estrategia paralela.

\begin{figure}
	\centering
		\includegraphics*[width=.75\textwidth,height=.2\textheight]{diagramaParalelo.png}
		\caption{Esquema del funcionamiento en dos etapas del código paralelo.}
		\label{figura:diagramaParalelo}
\end{figure}

Podemos utilizar este tipo de estrategia \textit{MapReduce} \cite{Dean2004} sobre la implementación paralela porque, debido a la naturaleza del árbol, cada rama será totalmente independiente de otras, por lo que cada nodo puede tratarse de manera independiente. Ese es la gran ventaja de usar el paralelismo para de estas técnicas; podemos enviar una gran cantidad de problemas a diferentes núcleos de computación de manera que pueden ser resueltos simultáneamente. En virtud de esto, el tamaño de los problemas en la entrada puede see mucho mayor de lo que la literatura nos muestra hasta ahora sin exceder las limitaciones de la máquina; hemos alejado el límite de una manera sustancial por el momento.

Como hemos mencionado, mediante la aplicación del código parcial, dividimos el problema de entrada en varios sub-problemas, sin embargo, es necesario aclarar cuál es el valor que decide ese punto de parada (BOV, en inglés, \textit{break-off value}).

\subsection*{Valor de parada}
\label{subseccion:BOV}
Decidir el BOV es una labor crucial de la investigación actual debido a las siguientes consideraciones. Por un lado, si decidimos parar en un nivel cercano a la raíz del árbol seleccionando un BOV bajo, ciertamente estamos reduciendo el tiempo de ejecución de la etapa de división y sólo se crearán unos pocos subproblemas. Dado que el árbol no habría podido expandirse todavía, entonces no contaremos con suficiente material para ser gestionado en paralelo usando diferentes núcleos. Por otro lado, si detenemos la división de la etapa parcial llegados a un nivel más profundo del árbol y lejos de la raíz, la división seguramente creará una gran cantidad de subproblemas. Esta situación nos va a permitir resolver esos subproblemas usando múltiples núcleos pero, el tiempo de ejecución de la primera etapa seguramente sea mayor. Por lo tanto, hemos tenido que seleccionar BOV de forma empírica, ya que es realmente difícil averiguar el valor más adecuado simplemente analizando la información de entrada.

En nuestro caso, como valor de parada que determina el nivel en el que la rama se considera atómica para ser tratada por la etapa paralela, usamos el cardinal del conjunto de implicaciones del nodo actual, ya que hemos observado que, cuanto mayor es, más larga \textit{suele ser} la rama. Sin embargo, en la etapa actual de la investigación, sólo podemos considerar eso como una tendencia, nunca una certeza.

Una vez que tenemos todos estos subproblemas, y debido a que todos ellos conforman un estado individual del algoritmo, pueden ser resueltos por el código paralelo. Sin embargo, si estamos tratando con problemas con una cantidad considerable de atributos e implicaciones, el número de subproblemas generados podría ser enorme (hemos realizado experimentos con más de 50,000 subproblemas \cite{Benito-Picazo2014TFM}). Por lo tanto, su gestión es una tarea que solo está al alcance de una gran cantidad de recursos, como los ofrecidos por el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga \footnote{http://www.scbi.uma.es}.



\section{Experimentos y resultados}
\label{seccion:experimentosResultados}
Llegamos a la sección donde vamos a mostrar una serie de experimentos y resultados para determinar cómo los algoritmos estudiados e implementados en la sección anterior \ref{sec:metodosSSTCK} tratan con problemas más complejos de forma que se aprovechen los beneficios del paralelismo.

Hemos elaborado de forma aleatoria un conjunto de problemas variando la cantidad de atributos e implicaciones. En primer lugar, comenzamos con problemas que contienen 100 implicaciones, cada una de ellas construida a partir de 100 atributos diferentes posibles. Y en segundo lugar, avanzamos un paso más, considerando problemas con 150 implicaciones y 150 atributos. Hay que tener en cuenta que estos números van más allá de las capacidades de la máquina, como se ha demostrado en estudios previos de este
trabajo \cite{Cordero2013}, e incluso mejoran sustancialmente los resultados dados en \cite{Benito-Picazo2014}, donde ya se aplicaron técnicas paralelas.

Obviamente, ambos métodos obtienen las mismas claves de forma que se valida experimentalmente el método. Por lo tanto, hemos omitido este parámetro en tablas de resultados para mejorar la legibilidad ya que en gran parte de las ocasiones el conjunto de claves puede llegar a ser considerablemente grande. Ahora bien, para comparar los números alcanzados mediante el uso de SST y de CK, nos centramos en dos parámetros fundamentales, tiempos de ejecución y la cantidad de nodos del árbol. La elección de estos dos parámetros se debe al siguiente razonamiento.  

Cuando nos planteamos la idea de la comparación de resultados, lo primero que consideramos fueron los tiempos que necesitaba cada uno de los métodos para obtener los resultados: era lo más evidente. No obstante, este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el algoritmo, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En este sentido, se oscurecía la utilidad teórica de los resultados obtenidos. Esto nos llevó a añadir como segundo parámetro de la comparación, el número de nodos del árbol como medida de la dimensión alcanzada por el problema. Además, si alguien hiciera otro método, utilizara otro código o empleara recursos hardware diferentes que desembocaran en una mejora
del tiempo, siempre podríamos atenernos al tamaño del árbol pudiendo defender si realmente es una mejora en el método o en la ejecución debido a la arquitectura.

El número de nodos siempre será el mismo para un experimento individual sin importar la cantidad de veces que realicemos el test, sin embargo, los tiempos de ejecución no coincidirán exactamente con esta circunstancia; podrían ser ligeramente diferentes debido a su naturaleza intrínseca. Las diferencias con respecto al número de nodos mostrarán cómo los nuevos métodos han mejorado el algoritmo reduciendo drásticamente la profundidad del árbol, y en consecuencia, los tiempos de ejecución. Además, hemos incluido una última columna para mostrar el ratio entre el número de nodos y el tiempo de ejecución. Finalmente, cada tiempo de ejecución que se muestra es el fruto de un estudio \textit{a posteriori} de los resultados obtenidos de cada ejecución, de forma que conservamos los más fiables \cite{Zobel1998}.

Como hemos mencionado anteriormente, la arquitectura hardware que se ha utilizado para desarrollar cada prueba que se muestra en la tesis se puede visitar en \footnote{http://www.scbi.uma.es}. En particular, hemos desarrollado cada experimento usando 32 nodos cluster SL230, utilizando 32 núcleos y 64Gb de memoria RAM. Las comunicaciones se realizan a través de una red Infiniband FDR. Durante los experimentos, estos núcleos están reservados sólo para nuestro uso, de manera que podamos evaluar resultados fiables con respecto a los tiempos de ejecución.

Puede parecer que conseguir mejores resultados con la estrategia paralela descrita es simplemente un problema referente a la cantidad de recursos que tengamos disponibles, sin embargo, esto no es del todo cierto. Si bien es cierto que en la mayoría de los casos hemos obtenidos mejores resultados gracias a utilizar más recursos, algunos experimentos para los que se aumentó el número de núcleos disponibles no alcanzaron las expectativas esperadas en términos de tiempos de ejecución, como se muestra brevemente en la Tabla \ref{table:speedup}. Incrementar el número de núcleos para favorecer el paralelismo dentro de este tipo de problemas es una cuestión en la que aún queda mucho por investigar.

\begin{table}[htbp]
\scriptsize
	\centering
\caption{Intentos de mejorar los tiempos de ejecución en base a aumentar el número de núcleos disponibles para la implementación paralela.}
\label{table:speedup}
\begin{tabular}{lrrrrrr}
\hline\noalign{\smallskip}
 & Implicaciones & Attrib & Parada \\
 & 150 & 150 & 140 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Problema \& Método & División$_{t}$(s) & Total$_{t}$(s) & Nodos & Cores & Ratio\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-4-\sst & 581 & 885 & 55.211 & 32 & 62\\
150150-4-\sck & 48 & 65 & 25.477 & 32 & 391\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-4-\sst & 576 & 880 & 55.211 & 64 & 62\\
150150-4-\sck & 46 & 64 & 25.477 & 64 & 398\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Entrando ya en los experimentos realizados, el primero resuelve una batería de problemas con 100 atributos y 100 implicaciones y los resultados podemos verlos en la Tabla \ref{table:bigdataI} que está formada por dos partes. Una cabecera en la que se indica la configuración del experimento, y un cuerpo principal donde se recogen los valores alcanzados para cada problema por parte de cada método. Este cuerpo principal consta de 6 columnas que desglosamos a continuación:

\begin{enumerate}
	\item Nombre del problema y el método utilizado para resolverlo.
	\item Número de subproblemas generados en la etapa primera del algoritmo.
	\item Tiempo transcurrido para la etapa primera.
	\item Tiempo total de ejecución (etapa parcial + etapa paralela).
	\item Número de nodos del árbol.
	\item Ratio entre nodos y tiempo de ejecución.
\end{enumerate}

\begin{table}[htbp]
\scriptsize
	\centering
\caption{Métodos paralelos aplicados a problemas grandes (I)}
\label{table:bigdataI}
\begin{tabular}{lrrrrrr}
\hline\noalign{\smallskip}
 & Attrib & Implicaciones & Parada & Cores & \\
 & 100 & 100 & 90 & 32 &  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Problema \& Método  & Subp & División$_{t}$(s) & Total$_{t}$(s) & Nodos & Ratio\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-1-\sst & 14 & 0 & 1 & 33 & 33\\
100100-1-\sck & 0 & 0 & 0 & 15 & 15\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-2-\sst & 1.354 & 36 & 105 & 25.621 & 244\\
100100-2-\sck & 212 & 4 & 15 & 12.715 & 847\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-3-\sst & 8.602 & 183 & 644 & 192.574 & 299\\
100100-3-\sck & 1.286 & 37 & 99 & 94.255 & 952\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-4-\sst & 400 & 7 & 26 & 1.704 & 65\\
100100-4-\sck & 15 & 1 & 2 & 751 & 375\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-5-\sst & 39 & 0 & 2 & 119 & 59\\
100100-5-\sck & 0 & 0 & 1 & 42 & 42\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-6-\sst & 1.808 & 37 & 123 & 7.856 & 63\\
100100-6-\sck & 115 & 4 & 9 & 3.698 & 410\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-7-\sst & 6.167 & 182 & 489 & 275.429 & 563\\
100100-7-\sck & 1.378 & 24 & 90 & 118.884 & 1.320\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-8-\sst & 5.104 & 146 & 415 & 182.167 & 438\\
100100-8-\sck & 1.014 & 19 & 68 & 81.632 & 1.200\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-9-\sst & 314 & 11 & 25 & 868 & 34\\
100100-9-\sck & 0 & 1 & 1 & 341 & 341\\
\noalign{\smallskip}\hline\noalign{\smallskip}
100100-10-\sst & 1.130 & 27 & 84 & 12.541 & 149\\
100100-10-\sck & 136 & 4 & 10 & 6.128 & 612\\
\noalign{\smallskip}
\hline
\end{tabular}
\end{table}

Incluso para una cantidad tan grande de atributos y dependencias, sólo han sido necesarios alrededor de 10 minutos para que el algoritmo más lento finalice (problema 100100-3). Además, el tamaño máximo alcanzado del árbol (problema 100100-7) ronda los 300k nodos. Sería totalmente descabellado pensar en resolver estos problemas usando
versiones secuenciales de los algoritmos, ya que los tiempos de ejecución se irían de las manos. No obstante, estos son resultados son altamente alentadores ya que hubiera sido impensable tratar de reproducir estos experimentos con los métodos previos \cite{Benito-Picazo2014}. Por tanto, es evidente cómo el paralelismo es una opción muy acertada para abordar este tipo de problemas.

Se obtienen resultados especialmente notables como son los problemas 100100-\{1,5,9\}. En estos casos, hay que notar que CK no crea ningún subproblema. Por ello, la mejora que se produce es doble: 
\begin{enumerate}
	\item En algunos casos, con el mismo valor de parada, el nuevo método ni siquiera necesita avanzar en la implementación paralela, la parcial es suficiente para resolver estos problemas.
	\item Para aprovechar esta circunstancia, llegado el caso en el que tengamos que trabajar con problemas aún más complejos, podemos establecer un valor de parada más cercano a la raíz del árbol de forma que el tiempo parcial se reducirá significativamente.
\end{enumerate}

Para el segundo experimento, seguimos la misma línea que el anterior y desarrollamos una nueva batería de 10 problemas, aumentando esta vez el número de implicaciones y atributos disponibles hasta 150. Cuanto mayor es la complejidad de estos problemas, mayor es la mejora lograda por el nuevo método CK. Los tiempos de ejecución y la cantidad de nodos han sido drásticamente reducido como se muestra en la Tabla \ref{table:bigdataII}. Esta vez, decidimos agregar un experimento adicional (problema 150150-EXTRA) debido a los notables resultados que alcanzó.

\begin{table}[htbp]
\scriptsize
	\centering
\caption{Métodos paralelos aplicados a problemas grandes (II)}
\label{table:bigdataII}
\begin{tabular}{lrrrrrr}
\hline\noalign{\smallskip}
 & Attrib & Implicaciones & Parada & Cores & \\
 & 150 & 150 & 140 & 32 &  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Problema \& Método  & Subp & División$_{t}$(s) & Total$_{t}$(s) & Nodos & Ratio\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-1-\sst  & 165 & 6 & 14 & 911 & 65 \\
150150-1-\sck & 11 & 2 & 3 & 374 & 124\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-2-\sst & 2.949 & 229 & 394 & 116.517 & 295\\
150150-2-\sck & 347 & 25 & 44 & 54.375 & 1.235\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-3-\sst & 12.968 & 1.049 & 1.716 & 157.947 & 92\\
150150-3-\sck & 822 & 125 & 165 & 68.531 & 415\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-4-\sst & 5.352 & 581 & 885 & 55.211 & 62\\
150150-4-\sck & 344 & 48 & 65 & 25.477 & 391\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-5-\sst & 5.361 & 211 & 484 & 32.377 & 66 \\
150150-5-\sck & 168 & 27 & 36 & 12.522 & 347\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-6-\sst & 771 & 72 & 155 & 17.298 & 111\\
150150-6-\sck & 79 & 7 & 11 & 8.110 & 737 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-7-\sst & 9.473 & 638 & 1.252 & 576.912 & 460 \\
150150-7-\sck & 1.754 & 97 & 187 & 262.621 & 1.404 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-8-\sst & 5.466 & 424 & 857 & 510.627 & 595 \\
150150-8-\sck & 966 & 57 & 104 & 257.267 & 2.473 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-9-\sst & 235 & 25 &  45 & 3.632 & 80\\
150150-9-\sck & 24 & 3 & 4 & 1.726 & 431\\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-10-\sst & 3.403 & 348 & 555 & 102.537 & 184\\
150150-10-\sck & 277 & 31 & 46 & 45.962 & 999 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
150150-EXTRA-\sst & 31.401 & 2.950 & 30.983 & 21.404.732 & 690 \\
150150-EXTRA-\sck & 8.049 & 354  & 1.320 & 10.614.386 & 8.041 \\
\noalign{\smallskip}
\hline
\end{tabular}
\end{table}

Igual que en el caso anterior,  hay varios experimentos que merecen ser discutidos por separado. Por ejemplo, si fijamos nuestra atención en los problemas 150150-\{3,7\}, el número de subproblemas generados es mucho menor para el método CK que para SST. La diferencia, que no es ni mucho menos trivial, también propicia la gran reducción de los tiempos de ejecución de la etapa parcial. Los tiempos totales de ejecución van en la misma dirección. En cuanto al tamaño del árbol, los beneficios al introducir el cierre son bastante llamativos; en la mayoría de los casos, la cantidad de nodos se reduce en torno al 50\%. Podemos apreciar las diferencias y las mejoras obtenidas más cómodamente de forma gráfica en las Figuras \ref{figura:bigSizeIITiempos} y \ref{figura:bigSizeIINodos}.

\begin{figure}
	\centering
		\includegraphics*[width=.8\textwidth,height=.4\textheight]{totalTimesGraphics.png}
		\caption{Tiempos de ejecución de los métodos paralelos aplicados a problemas grandes (II).}
		\label{figura:bigSizeIITiempos}
\end{figure}

\begin{figure}
	\centering
		\includegraphics*[width=.8\textwidth,height=.4\textheight]{nodesGraphics.png}
		\caption{Tiempos de ejecución de los métodos paralelos aplicados a problemas grandes (II).}
		\label{figura:bigSizeIINodos}
\end{figure}


% =====================================================================
% =====================================================================
% =====================================================================