\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, podemos decir que esta Tesis ha englobado dos vertientes principales. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado dataset. Y por otro lado, de forma más extensa, se han realizado las tareas de implementación necesarias para poder llevar estos métodos teóricos a la práctica. Hemos trabajado sobre tres campos diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada uno de ellos se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado.

En esta tesis hemos podido apreciar el hecho de que contar con una sólida teoría basada en la lógica y las matemáticas nos concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. Como hemos podido comprobar, existe una gran cantidad de información implícita en los datos que solemos manejar. El descubrimiento de toda esta información y su gestión inteligente es sin duda un claro tema de investigación con fuerte actividad y repercusión en la actualidad. Esta ha sido nuestra intención a la hora de trabajar con FCA, los conjuntos de implicaciones y los operadores de cierre. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos tanto de esta tesis como lo es para el propio FCA si se pretende que se convierta en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Antes de entrar plenamente en el apartado de conclusiones, queremos indicar la manera de certificar la validez de los resultados obtenidos a lo largo de la tesis. Como hemos podido advertir en los capítulos anteriores, nuestra labor se centra en actuar sobre conjuntos de implicaciones. En ese sentido, para los experimentos realizados hemos contado con unos ficheros de entrada que contenían la información necesaria, y sobre ellos hemos obtenido unos resultados. Ahora bien, la forma de verificar que esos resultados son correctos es la siguiente. En primer lugar y con respecto a los resultados de claves y generadores minimales, se han realizado y corregido numerosos ejercicios en papel intentando buscar casos límites donde la implementación pudiera no ser precisa y se ha comprobado que los resultados obtenidos en papel coincidían exactamente con los calculados por las implementaciones en la máquina. Además, dado que para muchos de los ejemplos probados en los que se llegaban a calcular millones de nodos de un árbol no era posible comprobar si cada uno de esos cálculos era correcto, para el caso concreto de los experimentos relacionados con claves minimales, la validez de los experimentos viene dada al haber cotejado los resultados con aquellos obtenidos sobre un amplio abanico de ficheros utilizados en trabajos anteriores \cite{Benito-Picazo2013PFC,Benito-Picazo2014TFM} donde su validez quedó demostrada. Básicamente, la validez de los ejercicios más grandes se ha extrapolado de los resultados correctos obtenidos para los ejercicios más pequeños. Asimismo, los resultados se corroboran igualmente al alcanzar las mismas soluciones para diferentes métodos cuando cada uno de ellos hace un tratamiento de la información diferente con respecto al otro. En relación a los experimentos con SR conversacionales, dado que los experimentos no alcanzan números tan altos, la validez puede demostrarse de forma más asequible siguiendo un desarrollo explícito en papel.

Y el otro punto que queremos remarcar es el siguiente. A lo largo de todo el proyecto siempre hemos hablado de trabajar sobre sistemas de implicaciones. No obstante, queda fuera del ámbito de esta tesis el procedimiento mediante el cual se obtiene esos elementos para un sistema de datos. Para ello, encomendamos al avezado lector a visitar \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. En nuestro caso, nuestro cometido comienza con el tratamiento de la información una vez de ha extraído el conjunto de implicaciones de un sistema concreto.

Aclarados estos puntos, nos encontramos ahora en el último capítulo de la Tesis, en el cual vamos a recopilar la conclusiones más importantes que hemos alcanzado como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se introducen como trabajos futuros.


\section*{Conclusiones}
\noindent
Como hemos mostrado anteriormente, conocer las claves es fundamental en cualquier modelo de datos. En este sentido, hemos presentado una serie de métodos que nos permiten averiguar el conjunto de claves a partir del conjunto de implicaciones y haciendo uso de métodos de razonamiento automatizado, en concreto, la \slfd. Se ha investigado, pasando desde la teoría a la práctica, los diferentes métodos implementados haciendo uso del paradigma de Tableaux y se ha comprobado como, hasta donde sabemos, los resultados obtenidos superan las aproximaciones anteriores.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento y una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Para abordar esta tarea, se han presentado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello se ha hecho un uso intensivo de la \slfde sobre conjuntos de implicaciones. Finalmente, se han creado, analizado y probado enfoques diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras alcanzadas por cada uno.

En ambas situaciones, es decir, tanto para claves minimales como para generadores minimales, se han desarrollado los códigos necesarios para poder actuar sobre grandes cantidades de información. No obstante, para resolver problemas reales donde la cantidad de información de entrada sea considerable, es incuestionable la necesidad de unos recursos enormes tales como los que ha proporcionado el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; sin ellos habría sido inviable haber podido realizar gran parte de las pruebas. De acuerdo con esto, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de recursos de memoria. Incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse escandalosamente.

Asimismo, el parecido y la cualidad de independencia que tiene cada nodo del árbol tanto para los métodos de claves minimales como para los de generadores minimales, ponen de manifiesto que la utilización de estrategias paralelas sea la mejor opción para abordar estos problemas. No obstante, el primer punto que queremos aclarar es que el concepto de paralelismo que hemos utilizado se refiere a un paralelismo de tipo \textit{hardware}. Con esto nos referimos a que los beneficios que obtenemos del paralelismo se deben a la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, no estamos ante un caso de desarrollo de código paralelo desde una visión más purista, sino que es más acertado considerarlo como una aplicación basada en una estrategia \textit{MapReduce}\cite{Dean2004}.

Para la mayoría de los casos, en relación a claves y generadores minimales, existe un serio inconveniente. En primera instancia es prácticamente imposible, por el momento, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto nos va a obligar a realizar una serie de experimentos previos de forma que podamos aproximar las necesidades que va a tener un determinado experimento. 

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que nos permitieran evaluar el rendimiento de las pruebas de forma que pudiéramos comparar unos métodos con otros. En el caso de los experimentos relacionados con claves y generadores minimales, cuando nos planteamos la idea de la comparación de resultados, lo primero que se nos ocurrió fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, advertimos como este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar magnitud del árbol y la cantidad de resultados redundantes que se obtienen. De esta forma, si alguien hiciera otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre podríamos atenernos al tamaño del árbol pudiendo defender si realmente es una mejora en el método o en la ejecución debido a la arquitectura.

En cuanto a los SR conversacionales, también son varias las conclusiones a las que hemos llegado. La más importante es que, efectivamente, el tratamiento que realizamos de la información por medio de implicaciones y la \slfde puede aplicarse con éxito en este campo de conocimiento. Esto ya quedaba patente, como hemos mencionado con anterioridad, por la existencia de multitud de trabajos en la literatura de SR que utilizan conceptos de FCA; nuestro trabajo viene a reforzar este hecho.

Concretamente, presentamos una novedosa aplicación del algoritmo de cierre \cierree para afrontar el problema de la sobrecarga de la información que a menudo presentan los SR. Nuestra solución propone un proceso conversacional de selección de atributos por parte del usuario. Este trabajo combina características de sistemas basados en conteido con sistemas badados en conocimiento mediante una gestión inteligente de las implicaciones a través del cierre \cierre. Nuestro sistema constituye un marco que puede integrarse en diversos datasets y los resultados siguen siendo admisibles. Esto es, ciertamente, una gran contribución de este trabajo, pues ofrece la posibilidad de aplicarse a aplicaciones en diferentes áreas.

Otra conclusión importante es que existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR para evitar los diferentes problemas que pueden aparecer, por tanto, hay que tener presente con qué expectativas queremos que actúe. Además, hay que tener en cuenta que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación; lo habitual es que sean sistemas híbridos con la intención de poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. Nuestro caso en un claro ejemplo; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional.

Por otro lado, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esta es una conclusión razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SR es igualmente alto. Es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que queramos evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. Asimismo, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, aplicarle las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Finalmente, queremos remarcar que el sistema desarrollado es capaz de ir más allá del ámbito académico o de investigación. Las pruebas realizadas sobre \textit{datasets} reales demuestran la viabilidad y la utilidad que el sistema puede aportar en entornos empresariales. De esta forma, apostamos de nuevo por una eficaz transmisión de conocimiento que acerque la empresa a la academia.



\section*{Trabajos Futuros}
\noindent
Finalmente, nuestros resultados motivan una serie de direcciones importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Los iremos introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En primera instancia es prácticamente imposible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a constituir en la mayoría de los casos un serio inconveniente.

Tanto en claves minimales como en generadores minimales hemos podido ver que existe un elemento fundamental en el diseño de las implementaciones paralelas: el valor de corte o BOV. Recordamos que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. Nos encontramos entonces con que establecer un valor de corte adecuado se convierte en una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el Tableaux del experimento que estemos realizando y esto sólo lo podemos averiguar empíricamente. En definitiva, hay que buscar estrategias para encontrar un valor de corte tal que el aprovechamiento de los recursos computacionales sea óptimo.

Es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que podamos dirigir las búsquedas de forma más eficiente.

Hay que profundizar en el hecho de que aumentar el número de cores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como hemos comentado anteriormente.

Nuestra intención futura es aplicar el cálculo de claves y generadores minimales en más situaciones donde tratemos con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que podamos valorar el rendimiento alcanzado gracias al paralelismo.

Otra tarea sobre la que continuar el trabajo consiste en investigar cómo realizar un nuevo diseño de los códigos paralelos que nos permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema de forma que podamos mejorar en la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo nos serviría para obtener una versión paralela del mejor método secuencial estudiado (GenMinGen). 

En relación a los SR conversacionales, aparecen dos aspectos interesantes a tener en cuenta en el futuro próximo. En primer lugar, sería muy valioso poder identificar desde un primer momento qué elementos del dataset presentan una importancia superior entre los demás, ya sea por aparecer con mayor frecuencia, ser único, tener relación con un mayor número de elementos del conjunto, etc. En segundo lugar y siguiendo la misma idea, sería sin duda crucial saber qué características del dataset (tamaño, escasez, etc.) son las más influyentes en el rumbo que toman las conversaciones con el usuario. Tener un mayor conocimiento de estos dos factores nos permitiría poder influir de manera positiva en la conversación del sistema con el usuario y de esta forma, deparar en una mejor experiencia de usuario.

Asimismo, hay aspectos de los SR que sería recomendable investigar si sería acertado incluir en el sistema conversacional; tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario ha recibido. Este es un aspecto importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}.


%-BOV
%-becnhmark
%-no saber de principio nada
%-real data sets
%-numer of cores
%-sw and hw para paralelismo sw
%-calculos redundantes
%
%-Trying to discover which characteristics concerning the dataset (dimensionality, sparsity, etc.) are relevant
%-identifying elements within the dataset, which play a more significant role









