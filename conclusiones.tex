\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\setlength{\epigraphwidth}{8cm}
\epigraph{\textit{Si una conclusión no está poéticamente equilibrada, no puede ser científicamente cierta.
}{\\\textit{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Los robots del amanecer}\\\textup{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ I. Asimov}}
}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, la naturaleza dual de esta esta tesis ha conllevado dos grandes grupos de tareas. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado \textit{dataset}. Y por otro lado, se han realizado las tareas de investigación e implementación necesarias para poder contrastar la validez de estos métodos teóricos en la práctica. 

Se ha investigado sobre tres áreas diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada una de ellas se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado. Asimismo, se hace un especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico, como el mercado empresarial.

A lo largo de la tesis se puede apreciar el hecho de que contar con una sólida teoría basada en la Lógica y las Matemáticas concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. De esta forma, se ha comprobado que existe una gran cantidad de información implícita en los datos que se suelen utilizar en dichas aplicaciones. El descubrimiento de toda esta información y su gestión inteligente es sin duda una clara oportunidad de investigación con una fuerte actividad y repercusión en la actualidad. Esta ha sido la intención principal a la hora de trabajar con FCA, los conjuntos de implicaciones y los conjuntos cerrados. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos de esta tesis al pretender que estos conceptos se conviertan en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Se alcanza ahora el último capítulo de la tesis, en el cual se recopilan las conclusiones más importantes alcanzadas como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se presentan como trabajos futuros.


\section{Conclusiones}
\noindent
En primer lugar, y dada la relación manifiesta en términos de algoritmos y computación paralela, se muestran las conclusiones referentes a claves y generadores minimales. Más adelante y para terminar la sección, se muestran aquellas conclusiones obtenidas en torno a SRs conversacionales.

Como se ha mencionado con anterioridad, conocer las claves es una tarea crucial para muchas áreas de gestión de la información. Se recuerda que una clave es un conjunto de atributos de un esquema relacional que nos permite distinguir inequívocamente cada objeto del \textit{dataset}. El problema surge debido a la complejidad exponencial del algoritmo de búsqueda de claves a partir de un conjunto de DFs que se cumplen en un esquema del modelo relacional \cite{Lucchesi78}.

Para abordar este problema, a partir de los métodos que los autores presentaron en \cite{Cordero2013}, se han diseñado una serie de nuevos métodos que permiten averiguar el conjunto de claves a partir del conjunto de implicaciones haciendo uso de métodos de razonamiento automático basados en la lógica \slfd. Se han diseñado e implementado, pasando desde la teoría a la práctica, los diferentes métodos basados en el paradigma de tableaux, y se ha verificado como, hasta donde se ha podido comprobar, los resultados obtenidos mejoran los de las aproximaciones anteriores \cite{Cordero2013} reduciendo tres aspectos fundamentales: el tiempo de cómputo, los cálculos redundantes y el tamaño del problema \cite{Benito-Picazo2016}. Además, en este trabajo se manifiesta como la introducción del paralelismo y los recursos de supercomputación han permitido que las limitaciones que se encontraban en el trabajo original \cite{Cordero2013} hayan podido solventarse, abriendo la puerta a poder trabajar con cantidades mayores de información.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento, constituyendo una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Junto con los conjuntos cerrados, los generadores minimales, son esenciales
para obtener una representación completa del conocimiento en FCA \cite{Poelmans2013}.

El punto de partida para trabajar sobre generadores minimales en esta tesis ha sido el método presentado en \cite{Cordero2012}, donde se utilizó la lógica \slfde\index{\slfde} como herramienta para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. La propuesta que se ha realizado en esta tesis ha consistido en el diseño y la implementación de métodos que nos permitan identificar los generadores minimales como representaciones canónicas de cada conjunto cerrado para un \textit{dataset}. 

Desafortunadamente, la dificultad que aparece al utilizar estos métodos es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial.

Con la intención de afrontar esta tarea, se han diseñado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello, se ha hecho un uso intensivo de la lógica \slfde sobre conjuntos de implicaciones. Finalmente, se han diseñado, analizado y probado algoritmos diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras aportadas por cada uno. Así, se han alcanzado reducciones de más del 50\% en el número de nodos del árbol de búsqueda que dibuja el método más básico, MinGen, frente al resto, MinGenPr y GenMinGen, como se ha detallado en \cite{Benito-Picazo2018}.

Asimismo, la utilización de estrategias paralelas se alza como la mejor alternativa en la resolución tanto de los problemas de claves minimales como de generadores minimales. Este hecho se debe a que cada uno de los subproblemas que se generan en la resolución de estos problemas (cada nodo del árbol del tableaux) es una instancia equivalente del problema original pero reducida, y por tanto, pueden tratarse de manera paralela asignando cada uno de ellos a un procesador diferente. 

No obstante, el primer punto que es necesario aclarar es que se ha aplicado un \textit{paralelismo} de tipo \textit{hardware}. Es decir, ha consistido en la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, estas implementaciones no son un caso de desarrollo de código paralelo desde una visión más purista de programación, sino que es más acertado considerarlas como aplicaciones basadas en una estrategia \textit{MapReduce}\cite{Dean2004}, que se ejecutan de forma paralela con la ayuda de recursos \textit{hardware}.

Al igual que para el problema de las claves minimales, se ha desarrollado el código necesario para poder trabajar con grandes cantidades de información para identificar los generadores minimales. Para resolver los problemas de tiempo de ejecución cuando la cantidad de información de entrada sea considerable, el desarrollo se ha optimizado para ejecuciones que sean capaces de aprovechar grandes recursos computacionales, tales como los proporcionados por el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; gracias a ellos ha sido viable realizar la gran mayoría de las pruebas. Aun contando con dichos recursos, se ha llegado a la conclusión de que, para ambos casos, claves y generadores minimales, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de los recursos de memoria; incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse sustancialmente.

Sin embargo, para la mayoría de los casos, existe un serio inconveniente. A la hora de afrontar la resolución de un problema de claves o generadores minimales, no es posible, en primera instancia, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema (en términos de número de nodos y tiempo de ejecución) a la vista únicamente de la información de entrada. Esto obliga a realizar una serie de pruebas previas para configurar adecuadamente el entorno de ejecución en cuanto a número de procesadores, cantidad de memoria, espacio de almacenamiento (véase \cite{Benito-Picazo2014TFM}). Eliminar la necesidad de dichas pruebas previas constituye sin duda un gran reto de cara a futuras investigaciones, como se mencionará más adelante en el apartado de trabajos futuros. 

Para evaluar las implementaciones realizadas (partiendo del hecho de que son implementaciones de algoritmos con el mismo orden de complejidad), no es suficiente con la comparación de los tiempos de ejecución de los métodos ya que este parámetro va a venir condicionado por la arquitectura \textit{hardware} utilizada para llevar a cabo los experimentos. En este sentido, se han utilizado métricas adicionales: el número de nodos y el número de claves o generadores redundantes, que reflejan el tamaño del problema y la cantidad de cálculo redundante realizado, respectivamente. Como muestran los experimentos presentados en \cite{Benito-Picazo2014,Benito-Picazo2013PFC,Benito-Picazo2014TFM,Benito-Picazo2016,Benito-Picazo2018}, gracias a estas métricas se ha podido comprobar que, con el trabajo realizado en esta tesis, se ha conseguido en diferentes experimentos una reducción del número de nodos y/o del número de cálculos redundantes entorno al 10-70\%.

En cuanto a los SRs conversacionales\index{sistemas de recomendación!conversacionales}, el trabajo realizado se ha enfocado en el uso de implicaciones y la Lógica para subsanar determinados problemas que aparecen en este tipo de SR. En concreto, se ha abordado el denominado problema de la dimensionalidad que surge cuando el \textit{dataset} sobre el que trabaja el SR contiene una cantidad muy elevada de atributos, lo que dificulta la interacción del sistema con el usuario.

En este campo también son varias las conclusiones alcanzadas. A alto nivel, la más importante es que, efectivamente, el tratamiento de la información realizado por medio de implicaciones y la lógica \slfde puede aplicarse con éxito al campo de los SRs. Este hecho ya era auspiciado por la existencia de trabajos en la literatura de SRs que utilizan conceptos de FCA \cite{Senatore2013,LeivaERCMG13,LeivaERCMG13a,Zou2017}; el trabajo en esta tesis refuerza este hecho, en concreto para los SRs conversacionales, proponiendo nuevos métodos con los que abordar problemas comunes de esos sistemas y mejorar las aproximaciones existentes.

Más específicamente, se ha aportado una novedosa aplicación del algoritmo del cierre \cierree para afrontar el problema de la sobrecarga de la información. La solución propuesta propone un proceso conversacional de selección de elementos por parte del usuario a partir de los atributos de éstos. Este trabajo combina además características de sistemas basados en contenido con sistemas de recomendación basados en conocimiento mediante una gestión inteligente de las implicaciones teniendo como base el cierre \cierre.

%Otra conclusión es que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación, lo habitual es que sean sistemas híbridos buscando poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. %La propuesta de esta tesis es un claro ejemplo de ello; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional, consiguiendo una beneficiosa sinergia.

%existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, se ha comprobado que es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR con la intención de evitar los diferentes problemas que pueden aparecer; es por tanto crucial, tener presente qué expectativas se pretenden cumplir. Del mismo modo, hay que tener en cuenta que 

Para concretar las conclusiones obtenidas, debe tenerse en cuenta que, tal y como se introdujo anteriormente en la Sección \ref{seccion:evaluacionSistemasRecomendacion}, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esto es razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SRs es igualmente alto. Por tanto, es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que se desee evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. 

Dada la naturaleza del sistema desarrollado, las métricas que se han utilizado para evaluar el rendimiento están directamente relacionadas con el proceso de diálogo, como son el número de pasos de la conversación o el filtrado de atributos que se produce. Estas métricas devuelven resultados muy prometedores en ambos casos: 
\begin{enumerate}
	\item En cuanto al número de pasos del diálogo, se puede ver como en gran parte de los experimentos realizados, la conversación necesita menos de 3 ó 4 pasos para alcanzar recomendaciones adecuadas aun cuando el número total de atributos sea considerable (más de 30 atributos). Estos buenos resultados quedan contrastados en \cite{Benito-Picazo2017}, donde se han hecho pruebas con \textit{datasets} con 100 atributos diferentes y en las que la mayoría de las conversaciones finalizan en 2 ó 3 pasos. 
	\item En cuanto al filtrado de atributos, los resultados son igualmente notables. Las pruebas mostradas en ese mismo artículo, demuestran cómo el uso del sistema conversacional ha evitado que el usuario tenga que interactuar a lo largo del diálogo con cerca del 5-20\% de atributos, reduciendo de esta forma la sobrecarga de información y mejorando la experiencia de usuario.
\end{enumerate}

Estos resultados superan con creces los encontrados en la literatura con métricas de evaluación comparables, como \cite{TrabelsiWBR11} donde, incluso en pruebas con \textit{datasets} con un menor número de atributos, se puede ver que por un lado, necesitan un número de pasos mayor, y por otro, que la reducción del número de atributos en la conversación es menor.
  

%En consecuencia, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas que existen. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, discernir las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Para finalizar, se quiere destacar que las aplicaciones y métodos desarrollados para los tres campos son capaces de ir más allá del ámbito académico o de investigación. Las pruebas realizadas demuestran la viabilidad y la utilidad que las propuestas pueden aportar en entornos empresariales. Como muestras de ello, se pueden considerar como referencias los experimentos satisfactorios llevados a cabo sobre datos reales, como el caso de MovieLens y los repositorios de la UCI\footnote{Universidad de California, Irvine (https://archive.ics.uci.edu/ml/index.php)} para claves y generadores minimales, o el caso de la información real sobre enfermedades y fenotipos extraída de HPO\footnote{Human Phenotype Ontology Consortium (https://hpo.jax.org/app/)} y OMIM\footnote{Online Mendelian Inheritance in Man (https://https://www.omim.org)} en el caso de los sistemas de recomendación conversacional.



%\section{Trabajos Futuros}
%\noindent
%Los resultados obtenidos motivan una serie de líneas de trabajo importantes para futuras investigaciones.
%
%Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.
%
%En el momento actual no es posible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada, en términos de tiempo de cómputo y de recursos computacionales. Esto va a constituir en la mayoría de los casos un serio inconveniente ya que, para la realización de aplicaciones o pruebas, esta circunstancia no permite adelantar los requisitos de tiempo y recursos computacionales que serán necesarios. A pesar de ello, gracias a la gran cantidad de pruebas realizadas, se ha observado que ciertas características, como el cardinal de la premisa o la conclusión en la implicación, suelen vaticinar patrones de comportamiento similares. En este sentido, el trabajo que se está llevando a cabo es investigar la motivación teórica de estos hechos empíricos, con la intención de poder identificar características que pronostiquen la complejidad que alcanzará una determinada ejecución del proceso.
%
%Otro camino muy importante para continuar la investigación tanto para claves minimales como para generadores minimales es ahondar en la optimización del valor de corte o BOV. Recuérdese que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. El hecho de establecer un valor de corte adecuado es una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el tableaux del experimento que se esté realizando y esto sólo se puede averiguar empíricamente. Es este sentido, se está trabajando en optimizar el valor de corte calculándolo como un porcentaje del tamaño de la entrada de manera que equilibre el trabajo realizado por cada procesador.
%
%%Como consecuencia de lo anterior, es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se pueda optimizar el funcionamiento de los métodos.
%
%Otra línea de investigación sobre la que se está trabajando busca profundizar en el hecho de que aumentar el número de procesadores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}. En otras palabras, los tiempos de ejecución de los experimentos pueden incrementarse al aumentar los recursos \textit{hardware}. Este efecto se debe, generalmente, a que el problema original se disemina de manera excesiva entre los procesadores disponibles provocando que el tiempo requerido por las comunicaciones para combinar los resultados parciales y así construir el resultado final contrarresta la ganancia en rendimiento que ofrece la capacidad de cómputo adicional. Para abordar este problema, se está trabajando en descubrir aquellas cotas de recursos a partir de las cuales el beneficio decrece. Para ello, se están investigando estrategias de optimización de recursos \textit{hardware} en entornos de HPC \cite{Wienke2016,AlAli2016}. Así, se están llevando a cabo nuevos experimentos generando de manera sistemática datos de prueba con diferentes caracterizaciones para intentar inferir correlaciones entre éstas y las configuraciones \textit{hardware} necesarias de cara a optimizar el rendimiento. 
%
%%También se pretende en el futuro aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda maximizar el rendimiento alcanzado gracias al paralelismo. Para ello, se va a utilizar información de repositorios conocidos, como el comentado de la Universidad de California, sobre los que aplicar los algoritmos, comparar resultados y verificar la viabilidad en tales situaciones reales.
%
%Otra tarea por la que continuar es realizar un nuevo diseño de los algoritmos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema, de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo es muy importante y la razón es la siguiente. Como se ha mencionado en el Capítulo \ref{cap:introduccion}, el método que mejores resultados obtiene en cuanto a número de nodos y tiempos de ejecución es GenMinGen, sin embargo, hay que recordar que, por el momento, es un método secuencial debido al requisito de establecer comunicación entre las diferentes soluciones parciales. Consecuentemente, las nuevas tareas de paralelización que se están llevando a cabo abren la puerta a poder obtener una versión paralela de GenMinGen. Según el curso actual de la investigación, va a ser necesario considerar la aplicación del paralelismo a nivel de \textit{software} y ver cómo puede combinarse con la estrategia actual, centrada en el paralelismo \textit{hardware}. Por tanto, se ha comenzado a investigar 
%la literatura sobre este tema \cite{Moraes2016}, además de otros aspectos de más bajo nivel como lenguajes de programación, librerías, etc.
%
%En relación a los SRs conversacionales, aparecen varios aspectos interesantes a tener en cuenta en el futuro próximo. 
%
%%En primer lugar, sería muy valioso poder identificar qué objetos del \textit{dataset} presentan una importancia superior entre los demás teniendo en cuenta que aparezcan con mayor frecuencia en las recomendaciones (tienen una mayor popularidad). Teniendo en cuenta que en el sistema actual la posición que ocupa cada elemento en la lista de resultados es indiferente, incluir la característica mencionada puede ser útil de cara a proponer un determinado orden en que se muestren los resultados en la lista de recomendación. En este sentido se está investigando la literatura referente a ordenación de resultados en SRs \cite{Shalom2016,Shashkin2017,Wang2018}.
%
%%Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen una menor presencia en las recomendaciones, lo cual puede deberse a diversas circunstancias como: ser más antiguos, estar descritos en menor detalle, etc. Ejemplos de este tipo de características ya están siendo investigadas en SRs \cite{Nguyen2014,Arnaboldi2016,Bressan2016,Abdollahpouri2017}; el reto consiste en introducirlas en el modelo concreto de SR conversacional propuesto en esta tesis, y para ello, por el momento se está investigando la literatura existente \cite{Llorente2012,Felfernig2015,Salem2014} en aras de encontrar un nexo que permita tener introducir estas características en la propuesta actual.
%
%En primer lugar, sería esencial averiguar qué características del \textit{dataset}\index{dataset} (tamaño, sobreespecialización, dispersión, sinónimos, etc.) son las más influyentes en el rendimiento del proceso de recomendación. En busca de este objetivo, en estos momentos se ha comenzado por investigar la literatura al respecto \cite{Rajput2015,Verstrepen2017,Pan2016}.
%
%%En primer lugar, sería muy valioso poder identificar desde un primer momento qué objetos del \textit{dataset} presentan una importancia superior entre los demás teniendo en cuenta que aparezcan con mayor frecuencia en las recomendaciones (tienen una mayor popularidad). Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen una menor presencia en las recomendaciones, lo cual puede deberse a diversas circunstancias como: ser más antiguos, estar descritos en menor detalle, etc. Ejemplos de este tipo de características ya están siendo investigadas en SRs \cite{Nguyen2014,Arnaboldi2016,Bressan2016,Abdollahpouri2017}; el reto consiste en introducirlas en el modelo concreto de SR conversacional propuesto en esta tesis, y para ello, por el momento se está investigando la literatura existente \cite{Llorente2012,Felfernig2015,Salem2014} en aras de encontrar un nexo que permita tener introducir estas características en la propuesta actual.
%
%%Entre ellas, serviría para incrementar la reducción de los pasos del diálogo o proponer un determinado orden en que se muestran los resultados en la lista de recomendación \cite{Shalom2016}. En definitiva, deparar en una mejor experiencia de usuario. 
%
%Existen más aspectos de los SRs que sería recomendable investigar para mejorar el sistema conversacional desarrollado. Tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario recibe. Este es un aspecto muy importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}. De hecho, la aceptación de un SR mejora cuando los usuarios comprenden las fortalezas y limitaciones del SR \cite{Papadimitriou2012}.
%
%En este sentido, es relevante considerar que al tratar con implicaciones, nuestro SR garantiza que los resultados cumplen plenamente con lo que pide el usuario. Sin embargo, consideramos interesante la generación de explicaciones para justificar la reducción de atributos que se lleva a cabo a lo largo del proceso conversacional, al margen de los solicitados explícitamente por el usuario, por la acción del algoritmo del cierre. %De esta forma, se podría indicar al usuario la razón de que se reduzcan los atributos disponibles más allá de sus elecciones explícitas. 
%
%Por todo ello, en este momento se está investigando a partir de la literatura los tres diferentes estilos de explicaciones habituales en los SRs actuales como son las basadas en: usuario y sus preferencias, como es el caso de Recommender Widget \cite{Guy2009} en el que los elementos recomendados se eligen de acuerdo con la similitud en la navegación web del usuario \cite{Yulong2011}; en objetos, donde las recomendaciones están basadas en las valoraciones que el usuario ha proporcionado al sistema; en características, donde se utiliza los atributos de los objetos como guía para realizar las recomendaciones \cite{Mooney2000}. En el futuro próximo, el trabajo se centrará en el estudio del tercer tipo de explicaciones ya que, por el momento, los dos primeros tipos no tienen cabida en el sistema desarrollado, ya que no se utilizan ni preferencias de usuario ni valoraciones.
%
%Finalmente, un punto muy importante que se está investigando actualmente para mejorar el SR conversacional propuesto es la inclusión de reglas de asociación. De manera general, las reglas de asociación  \cite{Han2000,Agrawal1993} pueden considerarse también como reglas \textit{si-entonces} que ayudan a descubrir relaciones entre datos aparentemente no relacionados en los \textit{datasets}. Estas reglas son un elemento ampliamente utilizado en el campo de los SRs desde hace años. Así por ejemplo en \cite{Kardan2013} los autores proponen un sistema en el que se aplican reglas de asociación para descubrir usuarios con características similares, en \cite{Mican2010} se utilizan las reglas de asociación para el descubrimiento de patrones entre elementos frecuentes e infrecuentes y en \cite{Cakir2012}, a partir del historial de navegación del usuario, se extraen las reglas de asociación que son la base para proponer productos recomendados para un usuario.
%
%En concreto, con el objetivo de reducir la sobrecarga de información, se está investigando cómo enriquecer el proceso conversacional mediante la introducción de reglas de asociación, para lo cual hay varios aspectos a tener en consideración. En primer lugar, un ejemplo de regla de asociación podría ser: \textit{``Si un cliente compra pan y leche, tiene una probabilidad del 80\% de comprar también mantequilla''}; por tanto, al contrario que las implicaciones, puede ocurrir que aunque se cumpla la premisa, la conclusión no lo haga; es cierta sólo para un porcentaje de todos los objetos que están cubiertos por la premisa de la regla. En casos en los que este porcentaje sea coonsiderable, puede ser acertado incluir estas relaciones en el proceso conversacional ya que aumentaría el conocimiento del sistema. De este modo, por ejemplo, para la información de la Tabla \ref{tabla:ejemploContextoFormal}, se podría extraer la regla de asociación \textit{\{Estados Unidos $\overset{91\%}{\Longrightarrow}$ Europa, Asia\}}, que indica que de las aerolíneas que proveen vuelo a Estados Unidos (11 de las 13 aerolíneas), el 91\% también lo hacen a Europa y Asia (10 en total). Una regla de estas características que engloba conocimiento referente a tan elevado porcentaje de elementos del \textit{dataset}, puede ser recomendable incluirla en el proceso conversacional aunque no se verifique en el 100\% de los casos.
%
%Para profundizar en este tema, se ha empezado por investigar los conceptos de confianza y soporte de una regla de asociación, con el especial interés de identificar aquellas asociaciones con grado de confianza más alto, de manera que el sistema, sobrecargándolas, las añada al conjunto de implicaciones. A partir de esta inclusión, el sistema podría actuar como lo hace hasta el momento sin mayor impacto. Sin embargo, hay que ser enormemente cuidadoso con esta inclusión, ya que desde primera hora, se está perdiendo el 100\% de cumplimiento del sistema con respecto a las solicitudes del usuario que se tiene al trabajar con implicaciones. No obstante, se pretende analizar si el aumento de conocimiento que obtiene el sistema al incluir estas asociaciones en el conjunto de implicaciones es admisible frente a la pérdida de exactitud en los resultados.
%
%%De esta forma aparece la otra tarea, en la que se está investigando cómo obtener un valor de ratio entre el número de objetos del \textit{dataset} que verifican una premisa (en el ejemplo anterior, 11) y el número de objetos que verifican la premisa y la conclusión (en el ejemplo anterior, 10). Con ello se pretende analizar si el aumento de conocimiento que obtiene el sistema al incluir estas asociaciones en el conjunto de implicaciones es admisible frente a la pérdida de exactitud en los resultados.
%
%
%%Para profundizar en este tema, se están llevando a cabo dos tareas en paralelo. Por un lado, se ha empezado por investigar los conceptos de confianza y soporte de una regla de asociación, con el especial interés de identificar aquellas asociaciones con grado de confianza más alto de manera que el sistema, sobrecargándolas, las incluya en el conjunto de implicaciones. A partir de esta inclusión, el sistema propuesto, y en especial el algoritmo del cierre, podría actuar como lo hace hasta el momento sin mayor impacto. Sin embargo, hay que ser enormemente cuidadoso con esta inclusión, ya que desde primera hora se está perdiendo el 100\% de cumplimiento del sistema con respecto a las solicitudes del usuario que se tiene al trabajar con implicaciones. De esta forma aparece la otra tarea, en la que se está investigando cómo obtener un valor de ratio entre el número de objetos del \textit{dataset} que verifican una premisa (en el ejemplo anterior, 11) y el número de objetos que verifican la premisa y la conclusión (en el ejemplo anterior, 10). Con ello se pretende analizar si el aumento de conocimiento que obtiene el sistema al incluir estas asociaciones en el conjunto de implicaciones es admisible frente a la pérdida de exactitud en los resultados.



\section{Trabajos Futuros}
\noindent
Los resultados obtenidos motivan una serie de líneas de trabajo importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En el momento actual no es posible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada, en términos de tiempo de cómputo y de recursos computacionales. Esto va a constituir en la mayoría de los casos un serio inconveniente ya que, para la realización de aplicaciones o pruebas, esta circunstancia no permite adelantar los requisitos de tiempo y recursos computacionales que serán necesarios. A pesar de ello, se ha observado que ciertas características, como el cardinal de la premisa o la conclusión en la implicación, suelen vaticinar patrones de comportamiento similares. En este sentido, el trabajo que se está llevando a cabo es investigar la motivación teórica de estos hechos empíricos, con la intención de poder identificar características que pronostiquen la complejidad que alcanzará una determinada ejecución del proceso.

Otro camino muy importante para continuar la investigación tanto para claves como para generadores minimales consiste en ahondar en la optimización del valor de corte o BOV. Recuérdese que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. El hecho de establecer un valor de corte adecuado es una tarea realmente compleja. En estos momentos se está estudiando la forma de expansión que tiene el tableaux con la intención de optimizar el valor de corte de manera que equilibre el trabajo realizado por cada procesador.

%Como consecuencia de lo anterior, es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se pueda optimizar el funcionamiento de los métodos.

Otra línea de investigación sobre la que se está trabajando busca profundizar en el hecho de que aumentar el número de procesadores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}. En otras palabras, los tiempos de ejecución de los experimentos pueden incrementarse al aumentar los recursos \textit{hardware}. Este efecto se debe, generalmente, a que el problema original se disemina de manera excesiva entre los procesadores disponibles provocando que el tiempo requerido por las comunicaciones para combinar los resultados parciales y así construir el resultado final contrarresta la ganancia en rendimiento que ofrece la capacidad de cómputo adicional. Para abordar este problema, se está trabajando en descubrir aquellas cotas de recursos a partir de las cuales el beneficio decrece. Para ello, se están investigando estrategias de optimización de recursos \textit{hardware} en entornos de HPC \cite{Wienke2016,AlAli2016}. %Así, se están llevando a cabo nuevos experimentos generando de manera sistemática datos de prueba con diferentes caracterizaciones para intentar inferir correlaciones entre éstas y las configuraciones \textit{hardware} necesarias de cara a optimizar el rendimiento. 

%También se pretende en el futuro aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda maximizar el rendimiento alcanzado gracias al paralelismo. Para ello, se va a utilizar información de repositorios conocidos, como el comentado de la Universidad de California, sobre los que aplicar los algoritmos, comparar resultados y verificar la viabilidad en tales situaciones reales.

Otra tarea por la que continuar es realizar un nuevo diseño de los algoritmos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema, de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este objetivo es muy importante y la razón es la siguiente. Como se ha mencionado en el Capítulo \ref{cap:introduccion}, el método que mejores resultados obtiene en cuanto a número de nodos y tiempos de ejecución es GenMinGen, sin embargo, hay que recordar que, por el momento, es un método secuencial debido al requisito de establecer comunicación entre las diferentes soluciones parciales. Según el curso actual de la investigación, va a ser necesario considerar la aplicación del paralelismo a nivel de \textit{software} y analizar cómo puede combinarse con la estrategia actual, centrada en el paralelismo \textit{hardware}. Por tanto, se ha comenzado por investigar trabajos relacionados en la literatura sobre este tema \cite{Moraes2016}.%, además de otros aspectos de más bajo nivel como lenguajes de programación, librerías, etc.

En relación a los SRs conversacionales, aparecen varios aspectos interesantes a tener en cuenta en el futuro próximo. 

%En primer lugar, sería muy valioso poder identificar qué objetos del \textit{dataset} presentan una importancia superior entre los demás teniendo en cuenta que aparezcan con mayor frecuencia en las recomendaciones (tienen una mayor popularidad). Teniendo en cuenta que en el sistema actual la posición que ocupa cada elemento en la lista de resultados es indiferente, incluir la característica mencionada puede ser útil de cara a proponer un determinado orden en que se muestren los resultados en la lista de recomendación. En este sentido se está investigando la literatura referente a ordenación de resultados en SRs \cite{Shalom2016,Shashkin2017,Wang2018}.

%Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen una menor presencia en las recomendaciones, lo cual puede deberse a diversas circunstancias como: ser más antiguos, estar descritos en menor detalle, etc. Ejemplos de este tipo de características ya están siendo investigadas en SRs \cite{Nguyen2014,Arnaboldi2016,Bressan2016,Abdollahpouri2017}; el reto consiste en introducirlas en el modelo concreto de SR conversacional propuesto en esta tesis, y para ello, por el momento se está investigando la literatura existente \cite{Llorente2012,Felfernig2015,Salem2014} en aras de encontrar un nexo que permita tener introducir estas características en la propuesta actual.

En primer lugar, sería esencial averiguar qué características del \textit{dataset}\index{dataset} (tamaño, sobreespecialización, dispersión, sinónimos, etc.) son las más influyentes en el rendimiento del proceso de recomendación. En busca de este objetivo, en estos momentos se ha comenzado por investigar la literatura al respecto \cite{Rajput2015,Verstrepen2017,Pan2016}.

%En primer lugar, sería muy valioso poder identificar desde un primer momento qué objetos del \textit{dataset} presentan una importancia superior entre los demás teniendo en cuenta que aparezcan con mayor frecuencia en las recomendaciones (tienen una mayor popularidad). Análogamente, sería igualmente beneficioso detectar aquellos otros objetos que tienen una menor presencia en las recomendaciones, lo cual puede deberse a diversas circunstancias como: ser más antiguos, estar descritos en menor detalle, etc. Ejemplos de este tipo de características ya están siendo investigadas en SRs \cite{Nguyen2014,Arnaboldi2016,Bressan2016,Abdollahpouri2017}; el reto consiste en introducirlas en el modelo concreto de SR conversacional propuesto en esta tesis, y para ello, por el momento se está investigando la literatura existente \cite{Llorente2012,Felfernig2015,Salem2014} en aras de encontrar un nexo que permita tener introducir estas características en la propuesta actual.

%Entre ellas, serviría para incrementar la reducción de los pasos del diálogo o proponer un determinado orden en que se muestran los resultados en la lista de recomendación \cite{Shalom2016}. En definitiva, deparar en una mejor experiencia de usuario. 

Existen más aspectos de los SRs que sería recomendable investigar para mejorar el sistema conversacional desarrollado. Tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario recibe. Este es un aspecto muy importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}. De hecho, la aceptación de un SR mejora cuando los usuarios comprenden las fortalezas y limitaciones del SR \cite{Papadimitriou2012}.

En este sentido, es relevante considerar que al tratar con implicaciones, nuestro SR garantiza que los resultados cumplen plenamente con lo que pide el usuario. Sin embargo, consideramos interesante la generación de explicaciones para justificar la reducción de atributos que se lleva a cabo a lo largo del proceso conversacional, al margen de los solicitados explícitamente por el usuario, por la acción del algoritmo del cierre. %De esta forma, se podría indicar al usuario la razón de que se reduzcan los atributos disponibles más allá de sus elecciones explícitas. 

Por todo ello, en este momento se está investigando a partir de la literatura los tres diferentes estilos de explicaciones habituales en los SRs actuales como son las basadas en: usuario y sus preferencias \cite{Guy2009}, en objetos \cite{Yulong2011}, en características \cite{Mooney2000}. En el futuro próximo, el trabajo se centrará en el estudio del tercer tipo de explicaciones ya que, por el momento, los dos primeros tipos no tienen cabida en el sistema desarrollado, ya que no se utilizan ni preferencias de usuario ni valoraciones.

Finalmente, un punto muy importante que se está investigando actualmente para mejorar el SR conversacional propuesto es la inclusión de reglas de asociación. De manera general, las reglas de asociación  \cite{Han2000,Agrawal1993} pueden considerarse también como reglas \textit{si-entonces} que ayudan a descubrir relaciones entre datos aparentemente no relacionados en los \textit{datasets}. Estas reglas son un elemento ampliamente utilizado en el campo de los SRs desde hace años. Así por ejemplo en \cite{Kardan2013} los autores proponen un sistema en el que se aplican reglas de asociación para descubrir usuarios con características similares, en \cite{Mican2010} se utilizan las reglas de asociación para el descubrimiento de patrones entre elementos frecuentes e infrecuentes y en \cite{Cakir2012}, a partir del historial de navegación del usuario, se extraen las reglas de asociación que son la base para proponer productos recomendados para un usuario.

En concreto, con el objetivo de reducir la sobrecarga de información, se está investigando cómo enriquecer el proceso conversacional mediante la introducción de reglas de asociación. Para avanzar en esta línea, se ha empezado por investigar los conceptos de confianza y soporte de una regla de asociación, con el especial interés de identificar aquellas asociaciones con grado de confianza más alto que puedan utilizarse para aumentar el rendimiento del proceso conversacional.

% =====================================================================
% =====================================================================
% =====================================================================








