\pagestyle{empty}
\chapter{Introducción}\label{cap:introduccion}
%\epigraphfontsize{\small\itshape}
\setlength{\epigraphwidth}{8.7cm}
\epigraph{\textit{---Empieza por el principio ---dijo el Rey con gravedad--- y sigue hasta llegar al final; allí, te paras.
}{\\\textit{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Alicia en el país de las maravillas}\\\textup{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ L. Carroll}}
}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}La gestión de la información es uno de los pilares esenciales de la Ingeniería Informática. No es de extrañar, por tanto, que conforme un amplio campo de investigación y conocimiento donde diversas disciplinas como las Matemáticas, la Lógica y la Computación actúen conjuntamente para alcanzar mejores sinergias.

Dentro de este ámbito y con la intención de hacer aportaciones en campos de la Ingeniería Informática como son las bases de datos\index{bases de datos} y los sistemas de recomendación\index{sistemas de recomendación}, esta tesis doctoral toma como principal base teórica el Análisis Formal de Conceptos \index{Análisis Formal de Conceptos} (FCA, por sus siglas en inglés: \textit{Formal Concept Analysis}), y más concretamente, una de sus herramientas fundamentales: los conjuntos de implicaciones\index{conjuntos de implicaciones}. La gestión inteligente de estos elementos mediante técnicas lógicas y computacionales confieren una alternativa para superar obstáculos en los campos mencionados.

FCA es una teoría matemática y una metodología que permite derivar una jerarquía de conceptos a partir de una colección de objetos, sus atributos y las relaciones entre ellos. De esta forma, el propósito es poder representar y organizar la información de manera más cercana al pensamiento humano sin perder rigor científico. En este sentido se enmarca la cita de Rudolf Wille: ``\textit{El objetivo y el significado del FCA como teoría matemática sobre conceptos y sus jerarquías es apoyar la comunicación racional entre seres humanos mediante el desarrollo matemático de estructuras conceptuales apropiadas que se puedan manipular con la lógica.}'' \cite{Wille2005}.

El término FCA fue acuñado por Wille en 1984 culminando años más tarde con la publicación más citada al respecto en colaboración con Bernhard Ganter \cite{Ganter1997}. Desde entonces, FCA se ha aplicado con éxito en diferentes disciplinas, como por ejemplo: biología celular \cite{Endres2012}, genética \cite{Xudong2015}, ingeniería del software \cite{Poelmans2012,Kester2013}, medicina \cite{Sacarea2017}, derecho \cite{Mimouni2015}, etc.

FCA parte de una representación de conjuntos de objetos y atributos por medio de tablas de datos. Estas tablas se denominan contextos formales y representan las relaciones binarias entre esos objetos y atributos. A partir de ahí, se generan dos herramientas básicas para representar el conocimiento: los retículos de conceptos y los conjuntos de implicaciones. Dichas herramientas además, son representaciones equivalentes del conocimiento descrito en el contexto formal.

Desde hace años, existen en la literatura estudios \cite{Kuznetsov2002,Outrata2012} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir de un conjunto de datos (en adelante \textit{dataset} por su nomenclatura habitual en el campo)\index{dataset}. Muchos de ellos toman como base uno de los algoritmos más conocidos a tal efecto, el denominado por Wille y Ganter como \textit{NextClosure} \cite{Ganter1997}.
%Sin embargo, debido a que el tamaño del retículo de conceptos es, en el peor de los casos, $2^{min(|G|,|M|)}$, siendo $G$ el conjunto de objetos y $M$ el conjunto de atributos, el coste computacional de los métodos encargados de su construcción constituye una limitación para la aplicación de FCA.

Por otro lado está el conjunto de implicaciones. Las implicaciones pueden considerarse \textit{grosso modo} como reglas del tipo \textit{si-entonces}, que representan un concepto muy intuitivo: cuando se verifica una premisa, entonces se cumple una conclusión. Esta idea básica se utiliza con diferentes interpretaciones en numerosos campos de conocimiento. Así, en la teoría relacional se interpretan como dependencias funcionales (DFs)\index{dependencias funcionales} \cite{Codd1971b}, en FCA como implicaciones\index{implicaciones} \cite{Ganter1997}, etcétera.

No obstante, también existen ciertas desventajas a la hora de trabajar con retículos e implicaciones, de hecho, la propia extracción del conjunto completo de implicaciones de un \textit{dataset} es una tarea que presenta una complejidad exponencial, sin embargo, es conveniente destacar que no es competencia de este trabajo el estudio de técnicas de extracción de implicaciones (lo cual es más una tarea de minería de datos), sino que la intención es partir del conjunto de implicaciones para trabajar con él. A este respecto, se pueden consultar trabajos ampliamente citados en la literatura en relación a la extracción de implicaciones a partir de \textit{datasets}\index{dataset} \cite{HuhtalaKPT99,YaoHB2002}.

Trabajar con conjuntos de implicaciones permite utilizar técnicas de razonamiento automático basadas en la lógica. Este hecho fundamenta el objetivo de esta tesis doctoral, que principalmente consiste en, utilizando los conjuntos de implicaciones, aplicar mecanismos lógicos para realizar un tratamiento eficiente de la información.

Como se verá en el Capítulo \ref{cap:preliminares}, la aproximación a través de la lógica es posible gracias a sistemas axiomáticos correctos y completos como los axiomas de Armstrong\index{Axiomas de Armstrong} \cite{Armstrong74} y la Lógica de Simplificación\index{Lógica de Simplificación} \cite{Enciso2002} (SL, por sus siglas en inglés: \textit{Simplification Logic}). Estos métodos aplicados sobre conjuntos de implicaciones se utilizan en esta tesis doctoral sobre tres áreas de investigación: claves minimales\index{clave!claves minimales}, generadores minimales\index{generadores minimales} y sistemas de recomendación conversacionales.

Se anticipa que, en cada uno de los casos, se va a aprovechar la información subyacente al conjunto de implicaciones para realizar novedosas aproximaciones que permitan abordar problemas presentes en esos ámbitos. Los resultados obtenidos se sustentan por una amplia gama de experimentos, en los cuales se ha utilizado tanto información real como sintética (información generada de forma aleatoria) y donde la computación paralela llevada a cabo en entornos de supercomputación\index{supercomputación} ha desempeñado un papel crucial. Como se verá más adelante, para el primer caso, el trabajo se centra en el uso de DFs mientras que para el segundo y tercero el núcleo son implicaciones.

%Antes de describir con mayor detalle las tres áreas anteriores, es necesario hacer una importante declaración previa. Tal y como se ha mencionado anteriormente, en esta tesis se trabaja con el conjunto de implicaciones que se deriva de un \textit{dataset}.  

%Son trabajos de suma importancia desde el punto de vista teórico, pero también desde el punto de visto práctico, pues incluyen las implementaciones de las aplicaciones que realizan la extracción de las implicaciones. 

Dicho esto, se retoma el texto pasando a introducir los tres campos de aplicación donde se han utilizado los conjuntos de implicaciones.

%Sin perjuicio de lo anterior, dado que hemos llegado a crear datasets propios sobre los que poder realizar experimentos como veremos más adelante, si bien no entramos en las técnicas de extracción de implicaciones que utilizan esas aplicaciones, sí hemos tenido que convertirnos en usuarios de estas aplicaciones para poder obtener el conjunto atributos e implicaciones que se verifican. Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

\section{Claves Minimales}
\noindent
El concepto de clave\index{clave} es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1990}. Una clave de un esquema relacional está compuesta por un subconjunto de atributos\index{atributo} que identifican a cada uno de los elementos de una relación. Representan el \textit{dominio} de una determinada función cuya \textit{imagen} es la totalidad del conjunto de atributos. Así, en un esquema de bases de datos relacional, una clave permite identificar cada fila de una tabla, impidiendo que exista más de una fila con la misma información y puede representarse por medio de una DF \cite{Codd1990} hacia todo el conjunto de atributos. Debido a ello en los sistemas gestores de bases de datos, las restricciones de clave son implementadas usando restricciones de unicidad (\textit{unique}) sobre el subconjunto de atributos que forman la clave.

Las DFs especifican una relación entre dos subconjuntos de atributos, e.g. $A$ y $B$, representada como $A \to B$, que asegura que para cualesquiera dos tuplas\index{tupla} de una tabla de datos, si los valores de sus atributos de $A$ coinciden, entonces también han de coincidir los de $B$. Si bien la noción de DF se verá con mayor detalle en la Sección \ref{sec:basesDatosRelacionales}, se adelanta el siguiente ejemplo básico \ref{ejemplo:basicoClaves} tanto para mostrar ejemplos de DFs como para ilustrar el concepto de clave.

\begin{ejemplo}
\label{ejemplo:basicoClaves}
Supongamos que disponemos de la siguiente tabla con información que relaciona títulos de películas, actores, países, directores, nacionalidad y años de estreno:

\vspace{0.3cm}
\noindent
{\scriptsize
\begin{tabular}{lcllll}
 \hline
 Título & Año & País & Director & Nacionalidad & Actor\\
 \hline
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & John Travolta\\
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & Uma Thurman\\
  Pulp Fiction & 1994 & USA & Quentin Tarantino & USA &Samuel Jackson \\
 King Kong & 2005 & NZ & Peter Jackson& NZ & Naomi Watts\\
 King Kong & 2005 & NZ & Peter Jackson & NZ & Jack Black\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jessica Lange\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jeff Bridges\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Jamie Foxx\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Samuel Jackson\\
 Blade Runner & 1982 & USA & Ridley Scott & UK & Harrison Ford\\
 Blade Runner & 2017 & USA & Denis Villeneuve  & CAN & Harrison Ford\\
% El dictador & 2012 & USA & Larry Charles  & USA & Megan Fox\\
% This is 40 & 2012 & USA & Judd Apatow  & USA & Megan Fox\\
 \hline
\end{tabular}
}

\vspace{0.3cm}

De esta información, podemos extraer el siguiente conjunto de DFs: 

$\Sigma = \{Titulo, A\tilde{n}o \rightarrow Pais, Director$; $Director\rightarrow Nacionalidad$\}. 

Esta tabla tiene una única clave: $\{A\tilde{n}o, Actor\}$ que corresponde con el conjunto de atributos necesario para identificar cualquier tupla de la relación.
\end{ejemplo}

La identificación de las claves de una determinada relación es una tarea crucial para muchas áreas de tratamiento de la información: modelos de datos \cite{Simsion2005}, optimización de consultas \cite{Kemper1991}, indexado \cite{Manolopoulos1999}, enlazado de datos \cite{Nikolov2011}, etc. Como muestras de esta importancia, es posible encontrar numerosas citas en la literatura, entre las que se pueden destacar las siguientes. En \cite{Sismanis2006}, los autores afirman que: ``\textit{la identificación de claves es una tarea fundamental en muchas áreas de la gestión moderna de datos, incluyendo modelado de datos, optimización de consultas (proporciona un optimizador de consultas con nuevas rutas de acceso que pueden conducir a mejoras sustanciales en el procesado de consultas), indexación (permite al administrador de la base de datos mejorar la eficiencia del acceso a los datos a través de técnicas como la partición de datos o la creación de índices y vistas), detección de anomalías e integración de datos}''. En \cite{Pernelle2013} los autores delimitan el problema manifestando: \textit{``establecer enlaces semánticos entre los elementos de datos puede ser realmente útil, ya que permite a los rastreadores, navegadores y aplicaciones combinar información de diferentes fuentes.''}.

Como refleja el contenido de esta sección, es evidente la importancia manifiesta de averiguar las claves de una relación, sin embargo, este labor no está exenta de dificultades, por ello, el trabajo realizado en esta parte de la tesis ha consistido en proponer, diseñar e implementar métodos para afrontar el problema de la búsqueda de claves, el cual se presenta a continuación.


% El problema de la búsqueda de claves
\section*{El Problema de la Búsqueda de Claves}
\label{sec:problemaBusquedaClaves}
\noindent
% Problema de la búsqueda de claves
El problema de la búsqueda de claves consiste en encontrar todos los subconjuntos de atributos que componen una clave minimal\footnote{Se acuña el término \textit{minimal} para referirnos a una clave en la que todos y cada uno de los atributos que la forman son imprescindibles para mantener su naturaleza de clave, es decir, no contiene ningún atributo superfluo.} a partir de un conjunto de DFs. Es un campo de estudio con décadas de antigüedad como puede observarse en \cite{Sali2004}, o en \cite{Fadous75}, donde las claves se estudiaron dentro del ámbito de la matriz de implicaciones.

% Claves como elemento crucial
%La dificultad al enfrentarse al problema de la búsqueda de claves surge debido a que, dado un conjunto de atributos $A$, la cardinalidad del conjunto $2^A$ hace que haya que abordar el problema aplicando técnicas que guíen la búsqueda de los conjuntos candidatos a ser claves minimales de forma que se pueda subsanar la complejidad exponencial de este tipo de problemas. 

% problema NP
El cálculo de todas las claves minimales representa un problema complejo. En \cite{Lucchesi78,Yu76} se incluyen resultados interesantes acerca de la complejidad del problema; los autores demuestran que el número de claves está limitado por el factorial del número de dependencias, por tanto, no existe un algoritmo que resuelva el problema en tiempo polinómico. En definitiva, es un problema NP-completo decidir si existe una clave de tamaño a lo sumo $k$ dado un conjunto de DFs.

Por otro lado, en \cite{CorderoEMG14}, los autores muestran cómo el problema de las claves minimales en las bases de datos tiene su análogo en FCA, donde el papel de las DFs se trata como implicaciones de atributos. En ese artículo, el problema de las claves mínimales se presentó desde un punto de vista lógico y para ello, se empleó un sistema axiomático, que los autores denominaron \slfde\index{\slfde} (por sus siglas en inglés: \textit{Simplification Logic for Functional Dependencies}) \cite{Enciso2002}, para gestionar las DFs y las implicaciones.

% referencias generales
Las principales referencias sobre el problema de la búsqueda de claves apuntan al trabajo de Lucchesi y Osborn en \cite{Lucchesi78} donde presentan un algoritmo para calcular todas las claves. Por otro lado, Saiedian y Spencer \cite{Saiedian1996} presentaron un algoritmo usando grafos con atributos para encontrar todas las claves posibles de un esquema de base de datos relacional. No obstante, demostraron que sólo podía aplicarse cuando el grafo de DFs no estuviera fuertemente conectado. Es reseñable también el trabajo de Zhang \cite{Zhang09} en el cual se utilizan mapas de Karnaugh \cite{Karnaugh1953} para calcular todas las claves. Existen  más trabajos sobre el problema del cálculo de las claves minimales como son \cite{Sismanis2006,Worland2004}. %Asimismo, en \cite{Levy2005,Valtchev03,Valtchev08} los autores propusieron el uso de FCA \cite{Ganter1997} para abordar problemas relacionados con la búsqueda y la gestión de las implicaciones, que pueden considerarse complementarios a nuestro trabajo.




\section*{Algoritmos para el Cálculo de Claves}
\label{sec:algoritmosCalculoClaves}
\noindent
% referencias de tableaux
El objetivo de esta parte de la tesis se centra en los algoritmos de búsqueda de claves basados en la lógica, y más específicamente, en aquellos que utilizan el paradigma de tableaux\index{Tableaux} \cite{Morgan1992,Risch1992} como sistema de inferencia. 

De forma muy general, se puede decir que los métodos tipo tableaux representan el espacio de búsqueda como un árbol, donde sus hojas contienen las soluciones (claves). El proceso de construcción del árbol comienza con una raíz inicial y desde allí, mediante la utilización de unas reglas de inferencia, se generan nuevas ramas del árbol etiquetadas con nodos que representan instancias más simples del nodo padre. Debido a esta característica, las comparaciones entre estos métodos se pueden realizar fácilmente ya que su eficiencia va de la mano del tamaño del árbol de búsqueda generado. La mayor ventaja de este proceso es su versatilidad, ya que el desarrollo de nuevos métodos se reduce en gran parte a cambiar las reglas de inferencia.

Esto conduce a un punto de partida fundamental, los estudios de R. Wastl (Universidad de Wurzburg, Alemania) \cite{Wastl98a,Wastl98} donde se introduce por primera vez un sistema de inferencia de tipo Hilbert para averiguar todas las claves de un esquema relacional. %A modo de ejemplo básico, en la Figura \ref{figura:ejemploTaleaux} se presenta un árbol de búsqueda según el paradigma de tableaux desarrollado utilizando el sistema de inferencia $\mathbb{K}$ de Wastl. 
Básicamente, se parte de una raíz para cuyo cálculo se ha aplicado una regla de inferencia \uno y a partir de ahí, se van construyendo las diferentes ramas del árbol mediante la aplicación de una segunda regla de inferencia \dos al conjunto de DFs (véase \cite{Wastl98a,Wastl98} para más detalles).

%\begin{figure}[htbp]
%	\begin{center}
%		\includegraphics*[width=.75\textwidth,height=.25\textheight]{arbol897.png}
%	\end{center}
%	\caption{Ejemplo de tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
%	\label{figura:ejemploTaleaux}
%\end{figure}

%\begin{figure}[htbp]
%	\begin{center}
%\begin{tikzpicture}%[every node/.style={rectangle, fill=blue!20!white}]
%\node {ABCD} [sibling distance=2.5cm]
%child {node {BCD}
%child {node {BC}}
%}
%child {node {ACD}
%child {node {CD}}
%}
%child {node {BCD}
%child {node {BC}}
%}
%child {node {ABC}
%child {node {BC}}
%child {node {AC}}
%};
%\end{tikzpicture}
%\end{center}
%\caption{Ejemplo de tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
%\label{figura:ejemploTaleaux}
%\end{figure}

Siguiendo esta línea, en \cite{Cordero2013} los autores abordan el problema de la búsqueda de claves utilizando un sistema de inferencia basado en la lógica \slfde \cite{Enciso2002}, demostrando como el árbol del espacio de búsqueda que se genera lleva a sobrepasar las capacidades computacionales de ordenadores corrientes hoy día, incluso para problemas pequeños. En \cite{Enciso2002} los autores muestran la equivalencia entre \slfde\index{\slfde} y los axiomas de Armstrong \cite{Armstrong74} junto con un algoritmo para calcular el cierre de un conjunto de atributos. Más tarde, en \cite{CorderoEMG14}, los autores introdujeron el método SST (por sus siglas en inglés, \textit{Strong Simplification Tableaux})\index{SST} para calcular todas las claves minimales usando una estrategia de estilo tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. 

El método SST está basado en la lógica de simplificación y sus equivalencias, añadiendo además, el test de minimalidad para aumentar la eficiencia. De esta forma, SST evita la apertura de ramas adicionales del árbol, por lo que el espacio de búsqueda se vuelve más reducido, logrando un gran rendimiento en comparación con sus predecesores como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}.


%\section*{Métodos \textit{SST} y \textit{CK}}
%\label{sec:metodosSSTyCK}
%\noindent
%En \cite{CorderoEMG14} se presentó un nuevo algoritmo, denominado SST\index{SST}, para calcular todas las claves minimales usando una estrategia de estilo tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. SST se basa en la noción de cierre de conjunto; una noción básica en la teoría de bases de datos que, utilizando el sistema axiomático\index{sistema axiomático} de la lógica \slfde (ver Sección \ref{subsec:logicaSLFD}), permite caracterizar el conjunto máximo de atributos que se puede alcanzar, desde un determinado conjunto de atributos $A$ con respecto a un conjunto de DFs. Por lo tanto, si el cierre de $A$ se denota como $A^+_\Sigma$, el sistema de inferencia para DFs permite inferir la DF $A\to A^+_\Sigma$. En este sentido, el enfoque con estilo lógico para el problema de las claves minimales consiste en la enumeración de todos los conjuntos de atributos $A$ tales que se verifique la DF: $A\to M$.

%SST muestra un gran rendimiento en comparación con sus predecesores como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}. El beneficio principal en la reducción del espacio de búsqueda se debe a la introducción de un test de inclusión para evitar la apertura de la totalidad de las ramas del árbol. Gracias a ello, SST no abre aquellas ramas de las que se tiene el conocimiento de que van a producir las mismas claves que se calculan en otra rama.

Por otro lado, el nuevo operador de cierre\index{operador de cierre} definido en \cite{Mora2012} tiene una característica fundamental que lo convierte en una novedosa alternativa frente a los métodos clásicos \cite{Maier1983} y es la siguiente. Además del conjunto de atributos que se deriva de la aplicación del operador de cierre al conjunto de implicaciones, el método proporciona un subconjunto $\Sigma^\prime$ de implicaciones del conjunto $\Sigma$ original que engloba la información que ha quedado fuera del cierre.

Tomando como base esos trabajos anteriores y con el apoyo del sistema axiómatico de la lógica \slfde\index{\slfde} (véase Sección \ref{subsec:logicaSLFD}), en esta tesis se presenta un nuevo método llamado \textit{Closure Keys (CK)}\index{CK}. Este nuevo método incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde (ver Sección \ref{subsec:algoritmosCalculoCierre}) para mejorar el rendimiento del método SST. 

%El método CK tiene una característica fundamental que lo convierte en una novedosa alternativa frente a los métodos clásicos y es la siguiente. Además del conjunto de atributos que se deriva de la aplicación del operador de cierre al conjunto de implicaciones, el método proporciona un subconjunto $\Sigma$ de implicaciones del conjunto $\Sigma$ original que engloba la información que ha quedado fuera del cierre. 

%Más formalmente, el método CK recibe un conjunto de implicaciones $\Sigma$ y un subconjunto de atributos $X \subseteq \Omega$, calcula el conjunto cierre $X^+$ respecto a $\Sigma$, y además, un nuevo conjunto $\Sigma$ que contiene el conjunto de implicaciones que guarda la semántica que queda fuera del cierre $X^+$. Si $\Sigma = \varnothing$, entonces $X^+ = \Omega$ (véase \cite{Mora2012} para más detalles).
%Con esto, se tienen los elementos para presentar la principal contribución de la tesis en la resolución del problema de la búsqueda de claves. Se han diseñado e implementado algoritmos para las versiones paralelas tanto del método SST como del método CK basándose en el paradigma \textit{MapReduce}\index{MapReduce} \cite{Dean2004}.


% con el paralelo
Una propiedad muy interesante de los métodos basados en tableaux (como lo son los métodos SST y CK) es la generación de subproblemas independientes los unos de los otros a partir del problema original. De esta forma, se alcanza otro objetivo fundamental de esta tesis, que consiste en utilizar las técnicas lógicas sobre una implementación paralela de los métodos de búsqueda de claves que, mediante el uso de recursos de supercomputación\index{supercomputación}, permitan alcanzar resultados en un tiempo razonable.


%En esta línea, son varios los trabajos que han utilizado la paralelización para afrontar problemas relacionados con implicaciones o FCA. Un algoritmo paralelo para el tratamiento de implicaciones enmarcado en el campo de los hipergrafos lo podemos encontrar en \cite{Sridhar1990}. A su vez, Krajca et al. \cite{Krajca2008} presentan un algoritmo paralelo para el cálculo de conceptos formales. 

Por nuestra parte, en \cite{Benito-Picazo2014} ya se presentó una primera aproximación a la paralelización del método de Wastl \cite{Wastl98a,Wastl98} y el algoritmo de claves \cite{Cordero2013}, donde se muestra cómo el paralelismo\index{paralelismo} puede integrarse de forma natural en los métodos basados en tableaux. Siguiendo la línea de estos trabajos, en esta tesis se ha llevado a cabo el estudio y diseño de los métodos SST y CK, y posteriormente, se han desarrollado también las implementaciones de los algoritmos en sus versiones secuenciales y paralelas, basándose estas últimas en el paradigma \textit{MapReduce}\index{MapReduce} \cite{Dean2004}.

Para la labor de computación de alto rendimiento, se ha trabajado intensamente con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{http://www.scbi.uma.es/}. La posibilidad de tratar con este centro ha proporcionado dos beneficios fundamentales: por un lado, se ha alcanzado una elevada pericia para trabajar en entornos de computación de alto rendimiento (HPC, por sus siglas en inglés: \textit{High Performance Computing}) y para realizar implementaciones que aprovechen una alta cantidad de recursos, y por otro lado, ha permitido obtener resultados empíricos sobre experimentos utilizando estrategias paralelas\index{paralelismo} que han desembocado en contribuciones científicas \cite{Benito-Picazo2014,Benito-Picazo2016} y que habría sido imposible conseguir en la actualidad sin contar con tales recursos computacionales.

Básicamente, el algoritmo paralelo de búsqueda de claves se divide en dos partes principales. Utiliza una primera fase en la que se realiza una expansión del árbol de búsqueda trabajando sobre el problema original y aplicando sucesivamente las reglas de inferencia y el algoritmo del cierre lógico, pero llegando únicamente hasta un cierto nivel de árbol, es decir sin alcanzar todavía las claves en las hojas del árbol. A partir de ese momento, se tiene un árbol de búsqueda parcial en el que cada nodo constituye un problema equivalente al original pero simplificado. A continuación interviene la segunda etapa del algoritmo y la computación paralela en la que cada nodo de ese nivel del árbol, se resuelve en paralelo mediante el uso de un elevado número de procesadores, es decir, aplica el mismo algoritmo de búsqueda de claves, pero ahora ya sí, hasta alcanzar las hojas del árbol, es decir, las soluciones del problema.

Existen numerosos factores a tener en cuenta a la hora de aplicar el algoritmo paralelo, de entre los cuales, el más importante es el valor de corte o parada de la primera etapa (en adelante \textit{BOV} por sus siglas en inglés, \textit{Break-Off Value}). Determinar este valor es un punto muy sensible del problema, pues de él depende el aprovechamiento general de los recursos en la aplicación del paralelismo \cite{Benito-Picazo2016}. Esto se debe a que existe la necesidad de elegir un \textit{BOV} de forma que la primera fase del algoritmo no requiera una cantidad excesiva de tiempo de ejecución, pero al mismo tiempo, que se genere la suficiente información para poder maximizar el rendimiento de la segunda fase, la de computación paralela.

Para contrastar la aportación del algoritmo, se han realizado una considerable cantidad de pruebas de rendimiento, las cuales necesitan llevarse a cabo en entornos de supercomputación\index{supercomputación} y cuyos resultados pueden consultarse en \cite{Benito-Picazo2016}. Así, se ha demostrado que el algoritmo diseñado es claramente susceptible de ejecutarse utilizando una implementación paralela. Se puede comprobar como se consiguen resultados en tiempos razonables incluso en los casos en los que la cantidad de información de entrada es considerable y en los que los métodos secuenciales no son capaces de finalizar.


\section{Generadores Minimales}
\label{sec:generadoresMinimales}
\noindent
Como se ha mencionado anteriormente, una forma de representar en FCA el conocimiento es el retículo de conceptos. Esta representación otorga una visión global de la información con un formalismo muy sólido, abriendo la puerta para utilizar la teoría de retículos como una metateoría para gestionar la información \cite{Bertet2016}.

Los conjuntos cerrados\index{retículo de conceptos!conjuntos cerrados} son la base para la generación del retículo de conceptos ya que éste puede ser construido a partir de aquellos, considerando la relación de subconjuntos como la relación de orden. En este punto nace el concepto de generadores minimales como representaciones canónicas de cada conjunto cerrado \cite{Ganter1997}.

Los generadores minimales junto con los conjuntos cerrados son esenciales para obtener una representación completa del conocimiento en FCA. Su relevancia puede apreciarse a través de importantes estudios como \cite{Poelmans2013,Qu2007}. Además, los generadores minimales se han usado como punto clave para generar bases\index{bases}, las cuales constituyen una representación compacta del conocimiento que facilita un mejor rendimiento de los métodos de razonamiento basados en reglas. Missaoui et al. \cite{Missaoui2010,Missaoui2012} presentan el uso de generadores minimales para calcular bases que impliquen atributos positivos y negativos cuyas premisas son generadores minimales.

En este aspecto, el trabajo de esta parte de la tesis ha consistido en el estudio y diseño de métodos para la enumeración de todos los conjuntos cerrados y sus generadores minimales a partir del conjunto de implicaciones. El proceso se desarrolla a partir de esta información, y no del \textit{dataset}\index{dataset} original, lo cual, hasta donde se ha investigado, no se había hecho previamente.

%Cualquier operador de cierre $c$ sobre un conjunto de atributos $M$ puede asociarse con un sistema de implicaciones. Esta conexión establece una forma de gestionar el trabajo del operador de cierre $c$ por medio de su derivación sintáctica y, como consecuencia, se puede elaborar un método para realizar esta gestión. Pero, ¿qué ocurre con la conexión inversa? Es decir, dado un conjunto de implicaciones, ¿es posible generar el operador de cierre $c$ asociado a él? Tal pregunta es el núcleo de esta parte de la tesis y su solución implica enumerar todos los conjuntos cerrados.

%Si se tiene que $X,Y \subseteq M$ satisfacen que $X = Y^+_\Sigma$, es habitual decir que $Y$ es un generador del conjunto cerrado $X$. Obsérvese que cualquier subconjunto de $X$ que contiene $Y$ es también un generador de $X$. Dado que se trabaja con conjuntos finitos de atributos, el conjunto de los generadores de un conjunto cerrado se pueden caracterizar por sus generadores minimales.


\section*{Métodos para el Cálculo de Generadores Minimales}
\label{seccion:metodosGeneradoresMinimales}
\noindent
Los métodos propuestos en esta tesis es una evolución del presentado en \cite{Cordero2012}, donde se utilizó la lógica \slfde\index{\slfde} como medio para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. Este método trabaja sobre el conjunto de implicaciones aplicando unas reglas de inferencia\index{reglas de inferencia} y construyendo árbol de búsqueda de aspecto similar a los árboles del caso de las claves minimales. No obstante, hay una diferencia esencial en el caso de los generadores minimales y es la siguiente. 

Al igual que el algoritmo \textit{CK} para claves minimales, los métodos propuestos utilizan el cierre \slfd, con lo cual, en cada paso también obtienen un conjunto $\Sigma^\prime$ reducido de implicaciones, pero ahora además, cada nodo del tableaux que se genera es una solución parcial del problema, la cual se combinará con el resto de soluciones al término de la ejecución del algoritmo para obtener el resultado final, mientras que en el caso de claves minimales, las soluciones se encontraban únicamente en las hojas.


%De manera específica, dado un conjunto de atributos $M$ y un sistema de implicaciones $\Sigma$, el método realiza un mapeo $mg_\Sigma\colon 2^M\to 2^{2^M}$\index{mapeo $mg_\Sigma$} que satisface la siguiente condición.

%$\forall X,Y\subseteq M$, $X\in mg_\Sigma(C)$ si y sólo si $C$ es cerrado para $(\ )^+_\Sigma$ y $X$ es un generador minimal para $C$.

%\begin{ejemplo}
%Sea $\Sigma=\{a\to c, bc\to d, c\to ae, d\to e\}$, el mapeo $mg_\Sigma$ se describe como:
%\begin{center}
%\begin{tabular}{p{1.6cm}|p{.4cm}p{.3cm}p{.3cm}p{.5cm}p{.5cm}p{.6cm}p{.6cm}p{.8cm}p{.9cm}}
%$X$ & $\varnothing$ & $b$ & $e$ & $be$ & $de$ & $ace$ & $bde$ &  $acde$ &   $abcde$   \\
%\hline
%$mg_\Sigma(X)$ &$\varnothing$ & $b$ & $e$ & $be$ & $d$  & $a$ & $bd$ & $ad$ & $ab$ 
%\\
%& & & & & & $c$& &$cd$ & $bc$    
%\end{tabular}
%\end{center}
%
%En otro caso, $X$ no es cerrado y $mg_\Sigma(X)=\varnothing$. Nótese que $\varnothing$ es cerrado y $mg_{\Sigma}(\varnothing)=\{\varnothing\}$, i.e. $\varnothing$ es un generador minimal del conjunto cerrado $\varnothing$.
%\end{ejemplo}

Tras el método presentado en \cite{Cordero2012} (que los autores denominaron MinGen)\index{MinGen} se presenta ahora un nuevo método, MinGenPr\index{MinGenPr}, que aplica una importante mejora con respecto al anterior. Fundamentalmente, consiste en incorporar un mecanismo de poda, basada en un test de inclusión de conjuntos, que involucra a todos los nodos del mismo nivel, para evitar la enumeración de generadores minimales y cierres redundantes. El propósito de esta poda es verificar la información de cada nodo en el espacio de búsqueda, evitando la apertura de una rama completa. Con la intención de ilustrar de forma general el efecto de esta poda se presenta el siguiente ejemplo.

\begin{ejemplo}
Sea $\Sigma=\{a\to c, bc\to d, c\to ae, d\to e\}$. El tableaux generado al aplicar el método MinGenPr sobre $\Sigma$ es:

%\begin{figure}
$$
\centerline{\scriptsize\xymatrix@C=0.6cm@R=0.5cm{
&&*{
\begin{array}{|llll}
\varnothing & b &e &be\\
\hline
\varnothing & b &e &be
\end{array}}
\ar@{-}[dll]_{a}\ar@{.}[dl]_{\color{Gray}bc}\ar@{-}[d]_{c}\ar@{-}[dr]^d&
\\
*+[o]{
\begin{array}{|ll}
ace & acde \\
\hline
a & ad 
\end{array}
}\ar@{-}[d]^{ab}
&
{\color{Gray}%*[Gray]{
\begin{array}{|l}
abcde \\
\hline
 bc 
\end{array}
}&
*{
\begin{array}{|ll}
ace & acde \\
\hline
c & cd 
\end{array}
}
\ar@{-}[d]^{bc}
&
*{
\begin{array}{|ll}
de & bde \\
\hline
d & bd 
\end{array}
}
\ar@{-}[d]^{ad}
\ar@{-}[dr]^{cd}
\\
*{
\begin{array}{|l}
abcde \\
\hline
 ab 
\end{array}
}&&
*{
\begin{array}{|l}
abcde \\
\hline
 bc 
\end{array}
}&
*{
\begin{array}{|ll}
acde & abcde \\
\hline
ad & abd 
\end{array}
}
&
*{
\begin{array}{|ll}
acde & abcde \\
\hline
cd & cbd 
\end{array}
}
}}
%\caption{Search tree for MinGenPr}\label{branch:fig}
%\end{figure}
$$

De entre toda la información contenida en el árbol, se puede apreciar cómo el método aplica una estrategia de poda en profundidad evitando abrir la rama cuya etiqueta es un superconjunto de otra rama en el mismo nivel. En concreto, la rama etiquetada como \emph{bc} (en color gris) no se abre dado que no es minimal respecto al conjunto \{a,bc,c,d\}.
\label{ejemplo:MinGenPr}
\end{ejemplo}

Finalmente, se propone un último método, GenMinGen\index{GenMinGen}, que generaliza la estrategia de poda anterior al considerar el test de inclusión del subconjunto no sólo con la información de los nodos del mismo nivel, sino también con todos los generadores minimales calculados antes de la apertura de cada rama. Al igual que en el caso anterior, se presenta el siguiente ejemplo para ilustrar el efecto de esta nueva poda.

\begin{ejemplo}
Sea el mismo conjunto $\Sigma$ del Ejemplo \ref{ejemplo:MinGenPr}. El tableaux generado al aplicar el método GenMinGen sobre $\Sigma$ es:

%\begin{figure}
$$
\centerline{\scriptsize\xymatrix@C=0.6cm@R=0.5cm{
&&*{
\begin{array}{|llll}
\varnothing & b &e &be\\
\hline
\varnothing & b &e &be
\end{array}}
\ar@{-}[dll]_{a}\ar@{.}[dl]_{\color{Gray}bc}\ar@{-}[d]_{c}\ar@{-}[dr]^d&
\\
*+[o]{
\begin{array}{|ll}
ace & acde \\
\hline
a & ad 
\end{array}
}\ar@{-}[d]^{ab}
&
{\color{Gray}%*[Gray]{
\begin{array}{|l}
abcde \\
\hline
 bc 
\end{array}
}&
*{
\begin{array}{|ll}
ace & acde \\
\hline
c & cd 
\end{array}
}
\ar@{-}[d]^{bc}
&
*{
\begin{array}{|ll}
de & bde \\
\hline
d & bd 
\end{array}
}
\ar@{.}[d]^{\color{Gray}ad}
\ar@{.}[dr]^{\color{Gray}cd}
\\
*{
\begin{array}{|l}
abcde \\
\hline
 ab 
\end{array}
}&&
*{
\begin{array}{|l}
abcde \\
\hline
 bc 
\end{array}
}&
{\color{Gray}%*[Gray]{
\begin{array}{|ll}
acde & abcde \\
\hline
ad & abd 
\end{array}
}
&
{\color{Gray}%*[Gray]{
\begin{array}{|ll}
acde & abcde \\
\hline
cd & cbd 
\end{array}
}
}}
%\caption{Search tree for MinGenPr}\label{branch:fig}
%\end{figure}
$$

En este caso, además de la misma poda en profundidad del ejemplo del método MinGenPr, esta vez, GenMinGen aplica también la poda teniendo en cuenta los resultados alcanzados hasta el momento. Así, las ramas etiquetadas con \emph{ad} y \emph{cd} no se abren debido a que previamente ya se abrieron las ramas etiquetadas como \emph{a} y \emph{c}.
\end{ejemplo}


En definitiva, se han estudiado, diseñado e implementado cada uno de estos métodos en su versión secuencial. Para evaluar el rendimiento e ilustrar las mejoras obtenidas al pasar de un método a otro, se han realizado un gran número de pruebas utilizando información sintética e información real procedente de repositorios de datos utilizados comúnmente en investigación, como son los de la Universidad de California, Irvine (UCI)\footnote{https://archive.ics.uci.edu/ml/datasets.html}.

A la luz de los resultados obtenidos en \cite{Benito-Picazo2018}, se aprecia claramente como la estrategia de poda del método MinGenPr\index{MinGenPr} hace que su rendimiento supere con creces al anterior MinGen. Estas mejoras pueden verse reflejadas en la reducción del número de nodos del árbol de búsqueda y su consiguiente disminución de los tiempos de ejecución del algoritmo. Respecto al último método, GenMinGen, los resultados de los experimentos son aún más notables, alcanzando reducciones superiores al 75\% en ambas métricas (i.e. número de nodos y tiempos de ejecución) en muchos de los casos. 

%La contrapartida que aparece al utilizar estos métodos es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial. Sin embargo, dado que nuestro objetivo es explotar las posibilidades de operar con conjuntos de implicaciones, en este trabajo se va a combinar ambos aspectos enumerando todos los conjuntos cerrados a partir de un conjunto dado de implicaciones. De hecho, se va a ir un paso más allá, calculando no sólo todos los conjuntos cerrados, sino que para cada uno de ellos se producirán sus respectivos generadores minimales.



\section*{Generadores Minimales y Paralelismo}
\label{seccion:generadoresMinimalesParalelismo}
\noindent
Si se pretende trabajar sobre conjuntos de implicaciones con una cantidad de información substancial, surge el mismo problema que en la enumeración de las claves minimales, la capacidad computacional de una máquina convencional actual no es suficiente para solucionar estos problemas en un tiempo razonable. Por tanto, se vuelve a utilizar el paralelismo como estrategia para abordar el problema.

%No obstante, aparecerá un problema similar al que sucedía con las claves minimales, y es que, al tratar con grandes cantidades de información, los métodos realizados para producir los generadores minimales, conllevan unas necesidades de cómputo que sobrepasan los límites de las máquinas convencionales actualmente. Por tanto, una vez más, hay que trasladar las implementaciones secuenciales de los métodos a versiones paralelas que permitan funcionar bajo arquitecturas de supercomputación\index{supercomputación}. 

%En este sentido, en esta tesis, se han diseñado los métodos de búsqueda de generadores minimales y se han realizado las implementaciones secuenciales de MinGen\index{MinGen}, MinGenPr\index{MinGenPr} y GenMinGen\index{GenMinGen}. Respecto al paralelismo, se ha realizado la implementación paralela de MinGenPr, que se ha denominado MinGenPar\index{MinGenPar}. La razón por la cual sólo se ha paralelizado el método MinGenPr es la siguiente.

%En primer lugar y como se ha mencionado anteriormente, MinGenPr es superior en rendimiento al método MinGen. No obstante
Aunque GenMinGen ha demostrado tener un mejor rendimiento que MinGenPr (y ambos a su vez un mejor rendimiento que MinGen) como demuestran lo resultados obtenidos en \cite{Benito-Picazo2018}, sólo se va a desarrollar una versión paralela del método MinGenPr, denominada \textit{MinGenPar}. Esto se debe a que, cuando se usa el método MinGenPr, no existe necesidad de comunicación entre los nodos del árbol, y por tanto, se puede utilizar la misma filosofía paralela de implementación \textit{MapReduce} en dos etapas que se utiliza en el caso de las claves minimales. Sin embargo, cuando se usa el método GenMinGen, es necesario comparar los resultados obtenidos en cada nodo con el conjunto actual de generadores minimales generados hasta el momento. Esto rompe esa filosofía de implementación, donde cada nodo del árbol está destinado a ser resuelto de forma independiente y sin existir comunicación entre cada uno de ellos. No obstante, esta circunstancia es el objetivo de estudio de uno de los trabajos futuros que se proponen en la Sección \ref{trabajosFuturos}.

Finalmente, para verificar el rendimiento y la idoneidad de los métodos en relación a la aplicación de estrategias paralelas, se ha realizado una amplia batería de pruebas tanto sobre información sintética como información real, tal y como se ha explicado anteriormente para el tema de las claves minimales. Además, las pruebas han incluido tareas de estimación del número óptimo de cores a utilizar, así como del valor de corte más apropiado en la etapa primera de los métodos paralelos. Los resultados obtenidos respecto a esta parte de la investigación pueden consultarse en \cite{Benito-PicazoCMMSE2017} y, especialmente en \cite{Benito-Picazo2018}, que constituye uno de los trabajos que avalan esta tesis doctoral.




\section{Sistemas de Recomendación Conversacionales}
\noindent
La tercera aportación de esta tesis doctoral haciendo uso de los conjuntos de implicaciones\index{conjuntos de implicaciones} se enmarca en el campo de los sistemas de recomendación\index{sistemas de recomendación} (SRs). 

%El objetivo principal de los SR es ayudar al usuario a elegir entre un número alto de alternativas.  

De forma muy simplificada, se podría considerar que un SR es un sistema inteligente que proporciona a los usuarios una serie de sugerencias personalizadas (recomendaciones) seleccionadas de un conjunto de elementos (ítems). Comúnmente, los SRs estudian las características de cada usuario e ítem del sistema, y a partir de ahí, mediante un procesamiento de los datos, encuentra un subconjunto de ítems que pueden resultar de interés para el usuario. Una recopilación de las referencias más notables en el campo de los SRs la encontramos en \cite{Ricci2015}.

% Historia
Desde los primeros trabajos sobre SRs \cite{Hill1995,Resnick1997}, éstos han estado en continua evolución durante los últimos años \cite{Adomavicius2005}. Sin embargo, es con la expansión de las nuevas tecnologías cuando han tenido un acercamiento más directo a la mayor parte de la sociedad debido a su capacidad para realizar todo tipo de recomendaciones sobre productos muy populares (libros \cite{Crespo2011}, documentos \cite{Porcel2012}, música \cite{LampropoulosLT12}, turismo \cite{BorrasFPMVIORC11}, películas \cite{Feng2015789,Harper2016}, etc.).

% Importancia
Los SRs constituyen tanto un importante campo de investigación \cite{Son2018,Eirinaki2018}, como un elemento indispensable para sólidos entornos comerciales a nivel mundial (Amazon \cite{linden2003}, LinkedIn \cite{Metaphor2012}, Facebook \cite{Tiroshi11}), lo cual pone de manifiesto la importancia de estos sistemas en la sociedad actual.

%En la actualidad, los SRs constituyen un claro campo de investigación y estudio como demuestran el gran número de trabajos que se están realizando \cite{Son2018,Eirinaki2018} y cuya cantidad continúa aumentando día a día. Además, la relevancia de estos sistemas no se limita al ámbito investigador. Actualmente, muchos SRs ya han sido implantados con éxito en fuertes entornos comerciales a nivel mundial. Este es el caso de empresas líderes en el sector como pueden ser Amazon \cite{linden2003}, LinkedIn \cite{Metaphor2012} o Facebook \cite{Tiroshi11}, que han realizado fuertes inversiones con el fin de generar mejores SRs. Estas situaciones ponen de manifiesto la gran importancia de estos sistemas en ambas vertientes de la sociedad actual.

% FCA y SR
Abordar la generación de recomendaciones haciendo uso de FCA es una aproximación existente en la literatura desde hace años. En \cite{duBoucherRyan2006}, los autores utilizan FCA para agrupar elementos y usuarios en conceptos para posteriormente, realizar recomendaciones colaborativas según la afinidad con los elementos vecinos. Más tarde, en \cite{Senatore2013}, se introducen un modelo para el filtrado colaborativo basado en FCA para generar correlaciones entre datos a través de un diseño del retículo. Zhang et al. \cite{Zhang2015} propusieron un sistema basado en similitud agrupando la información contextual en grafos mediante el cual llevar a cabo recomendaciones sobre las interacciones sociales entre usuarios. En \cite{LeivaERCMG13,LeivaERCMG13a}, se utilizan relaciones difusas e implicaciones ponderadas para especificar el contexto y \slfde\index{\slfde} para desarrollar un proceso lineal de filtrado que permite a los SRs podar el conjunto original de elementos y así mejorar su eficiencia. Recientemente, en \cite{Zou2017} se propone y utiliza un novedoso SR personalizado basado en el retículo de conceptos para descubrir información valiosa de acuerdo con los requisitos e intereses de los usuarios de forma rápida y eficiente. Todos estos trabajos subrayan claramente cómo FCA puede aplicarse con éxito en el campo de los SRs.


%\section*{Técnicas de recomendación}
%\label{seccion:tecnicasRecomendacion}
%\noindent
% Tipos
Existen numerosos tipos de SRs atendiendo a cómo se generan las recomendaciones. Los más extendidos son los de filtrado colaborativo\index{sistemas de recomendación!colaborativos} \cite{Medina-Moreira2017} que basan su funcionamiento en las valoraciones de los usuarios a los elementos disponibles; y los sistemas basados en contenido\index{sistemas de recomendación!basados en contenido} \cite{Zamani2018} que proporcionan resultados que tengan características similares a otros valorados anteriormente por el usuario. Por otro lado, los SRs basados en conocimiento\index{sistemas de recomendación!basados en conocimiento} \cite{Mandl2011} utilizan un método de razonamiento para inferir la relación entre una necesidad y una posible recomendación.

%En los últimos años ha habido un gran crecimiento de los SRs contextuales\index{sistemas de recomendación!contextuales} \cite{BenSassi2017}, capaces de tener en cuenta información relevante para la recomendación como puede ser la hora, el lugar, la compañía, la ubicación, etc. Existen también los conocidos como SRs demográficos\index{sistemas de recomendación!demográficos} \cite{BeelLNG13} que clasifican a los usuarios según diferentes parámetros personales (edad, localización, etc.) y de acuerdo con esto generan las recomendaciones. Por otro lado aparecen los denominados SRs basados en conocimiento\index{sistemas de recomendación!basados en conocimiento} \cite{Mandl2011}. Estos sistemas modelan y gestionan el conocimiento inherente a los datos y revelan cómo un ítem puede satisfacer la necesidad del usuario, es decir, utilizan un método de razonamiento para inferir la relación entre una necesidad y una posible recomendación. 

Los SRs más importantes desde el punto de vista de esta tesis son los denominados conversacionales\index{sistemas de recomendación!conversacionales} \cite{Griol2018,Lee2017}. Estos SRs se diferencian de los anteriores en el flujo de trabajo que se sigue para generar la recomendación. Este tipo de SR es la estrategia principal para el SR realizado y que ha dado lugar a una de las contribuciones que avalan esta tesis \cite{Benito-Picazo2017}. Se puede consultar una clasificación más detallada en \cite{AdomaviciusBook11,Bobadilla2013}.

%No se tendrá en cuenta únicamente la estrategia de recomendación sino también el proceso de cómo obtener una recomendación, lo cual casa con el concepto de Recuperación de Información\index{Recuperación de Información} que numerosos trabajos utilizan junto con FCA para generar recomendaciones \cite{Codocedo2015,Ignatov2015}. 

No obstante, la mejor alternativa consiste en combinar características de diferentes tipos de SRs para generar híbridos que se beneficien de las ventajas de cada uno de ellos \cite{DeCampos2010}. Tal es el caso de este trabajo, en el que se ha realizado un SR híbrido que combina características de los SRs basados en conocimiento, contenido y, principalmente, conversacionales.


%\begin{itemize}
%	\item \textbf{SRs basados en conocimiento.\index{sistemas de recomendación!basados en conocimiento}} Ya que se utilizan mecanismos de extracción de conocimiento de los datos utilizando FCA, los conjuntos de implicaciones y los operadores de cierre.
%	\item \textbf{SRs basados en contenido.\index{sistemas de recomendación!basados en contenido}} Puesto que las técnicas lógicas empleadas se basan en información sobre los ítems a recomendar y sus atributos. 
%	\item \textbf{SRs conversacionales.\index{sistemas de recomendación!conversacionales}} Ya que es precisamente un proceso de diálogo el que se utiliza para generar
%las recomendaciones.
%\end{itemize}

%Pero además, no se tendrá en cuenta únicamente de la estrategia de recomendación sino también del proceso de cómo obtener una recomendación. En este sentido, se introduce el concepto de Recuperación de Información\index{Recuperación de Información}. Este concepto se basa en obtener recursos relevantes del sistema de información a partir de la realización de consultas. Numerosos trabajos en la literatura relacionan el uso de FCA en modelos basados en Recuperación de Información \cite{Codocedo2015,Ignatov2015}.

%\section*{Evaluación de los sistemas de recomendación}
%\label{seccion:evaluacionSistemasRecomendacion}
%\noindent
La evaluación de las predicciones y recomendaciones es un aspecto fundamental en los SRs \cite{Herlocker2004,Reusens2018}. Los SRs requieren medidas de calidad y métricas de evaluación \cite{Gunawardana2009} para conocer la calidad de las técnicas, métodos y algoritmos para las predicciones y recomendaciones. %Las métricas de evaluación \cite{HernandezdelOlmo2008} y los \textit{frameworks} de evaluación \cite{Bobadilla2011} facilitan la comparación de varias soluciones para el mismo problema.

No obstante, dependiendo del SR con el que se trabaje, la evaluación se debe llevar a cabo utilizando aquellas métricas, que por su naturaleza y significado, sean coherentes con el SR que se desea evaluar. En el caso de estudio de esta tesis, dado el SR conversacional desarrollado, una medida adecuada de rendimiento consiste en calcular el número de pasos que se producen en la conversación \cite{McSherry01}. Por contra, otras métricas tan populares como son \textit{Precision} y \textit{Recall} \cite{Gunawardana2015} no son adecuadas de aplicar en el trabajo de esta tesis porque se obtendría siempre valores máximos en ambas métricas, y la razón es la siguiente. En primer lugar, cualquier ítem de la lista de resultados, verifica los atributos seleccionados ya que la consulta para obtener los ítems resultado contiene esas restricciones. Y en segundo lugar, a cada paso del diálogo, el sistema devuelve todos los ítems que verifiquen la selección de atributos establecida por el usuario.

Por la misma razón, no existe necesidad de considerar métricas referentes a la exactitud de los resultados ya que el sistema desarrollado no es un modelo de predicción, su funcionamiento está basado en implicaciones y eso asegura la completa de exactitud en las respuestas.


\section*{Problemas Comunes y la Maldición de la Dimensión}
\label{seccion:problemasRecomendacion}
\noindent
Si bien es cierto que los SRs están alcanzando una enorme importancia, existen numerosas dificultades que han de afrontarse a la hora de diseñarlos e implementarlos. En la lista de problemas\index{problemas SR} relacionados con los SRs \cite{Shah2016} se pueden destacar: el arranque en frío\index{problemas SR!arranque en frío} \cite{Feil2016,Son201687}, privacidad \cite{Friedman2015}\index{problemas SR!privacidad}, oveja-negra \cite{Gras2016}\index{problemas SR!oveja-negra}, escasez \cite{Guo2012}\index{problemas SR!escasez}, ataques maliciosos \cite{Zhou2015,Yang2016}\index{problemas SR!ataques maliciosos}, sobreespecialización \cite{LopsGS11}\index{problemas SR!sobreespecialización}, escalabilidad \cite{Isinkaye2015}\index{problemas SR!escalabilidad}, postergación \cite{Sundaresan2011}\index{problemas SR!postergación}, dimensionalidad \cite{Salimi2017}\index{problemas SR!dimensionalidad}, etc.


%Si bien es cierto que los SRs están alcanzando una enorme importancia, existen numerosas dificultades que han de afrontarse a la hora de diseñarlos e implementarlos. En la lista de problemas\index{problemas SR} relacionados con los SRs \cite{Shah2016} se pueden destacar: el arranque en frío o \textit{cold-start}\index{problemas SR!arranque en frío} (aparece cuando un nuevo elemento se incluye en el sistema careciendo de la información necesaria para participar en las recomendaciones) \cite{Feil2016,Son201687,Benito-PicazoCMMSE2016}, privacidad (puede surgir un problema si el sistema necesita información sensible con respecto al usuario) \cite{Friedman2015}\index{problemas SR!privacidad}, oveja-negra \cite{Gras2016}\index{problemas SR!oveja-negra} (existen elementos que no manifiestan similitud suficiente con ningunos otros, estos elementos tan singulares pueden verse olvidados en la mayoría de las recomendaciones), escasez \cite{Guo2012}\index{problemas SR!escasez} (pueden existir elementos del SR que sean difíciles de recomendar si la información que se tiene sobre ellos es insuficiente), ataques maliciosos \cite{Zhou2015,Yang2016}\index{problemas SR!ataques maliciosos} (son acciones que se pueden realizar sobre un SR para romper su fiabilidad, rendimiento, confianza), sobreespecialización \cite{LopsGS11}\index{problemas SR!sobreespecialización}, escalabilidad \cite{Isinkaye2015}\index{problemas SR!escalabilidad}, postergación \cite{Sundaresan2011}\index{problemas SR!postergación} (elementos que no son muy populares quedan relegados por aquellos otros que gozan de mayor solicitud), dimensionalidad \cite{Salimi2017}\index{problemas SR!dimensionalidad} (aparece cuando el la cantidad de información y su nivel de detalle en el sistema es tal que dificulta la interacción con el usuario), etc.

En concreto, en esta tesis se ha orientado el trabajo a abordar este último problema de la dimensionalidad en los SRs. Este problema, también conocido como \textit{the curse of dimensionality phenomenon} \cite{Salimi2017,Nagler2016} aparece cuando es necesario trabajar sobre \textit{datasets}\index{dataset} con un alto número de características (variables o atributos). De forma intuitiva, se puede describir de la siguiente manera: cuando hay pocas columnas de datos, los algoritmos de tratamiento inteligente de la información (aprendizaje automático, \textit{clustering}, clasificación, etc.) suelen tener un buen comportamiento. Sin embargo, a medida que aumentan las columnas o características de nuestros ítems, se vuelve más difícil hacer labores predictivas con un buen nivel de precisión. El número de filas de datos necesarias para realizar cualquier modelado útil aumenta exponencialmente a medida que agregamos más columnas a una tabla \cite{McEneaney2008}.

Para abordar este problema, se pueden encontrar numerosos trabajos en la literatura \cite{Salimi2018,KIM2018}, especialmente mediante selección de características, que pueden ayudar a descartar aquellas características que no son relevantes de cara al objetivo buscado. De hecho, estas técnicas ya se aplican en otras áreas como son: algoritmos genéticos o redes neuronales, normalmente centrándose en la aplicación de un proceso automatizado por lotes \cite{Viegas2018}.

%Suele ser habitual que para realizar una selección de características por parte del SR, el usuario tenga que introducir y seleccionar información del sistema una y otra vez. Esto constituye un problema dado que pueden existir artículos para los cuales el número de características que los definen sea muy elevado y en consecuencia, incomode la correcta interacción del usuario con el sistema, poniendo de manifiesto de nuevo cómo la alta dimensionalidad constituye un problema para los SR.

Un trabajo interesante en esta área es \cite{Jannach2009}, que establece la idoneidad de los enfoques basados en el conocimiento para los procesos conversacionales.
En particular, estos autores utilizan el razonamiento basado en restricciones, en lugar de nuestro enfoque basado en la lógica. Además, este trabajo trata sobre
concepto de optimización de consultas, análogo al aplicado en la propuesta de esta tesis. Otro trabajo notable es \cite{TrabelsiWBR11}, que comparte el objetivo de disminuir el número de pasos de la conversación. Los autores proponen métricas acerca del número de pasos y tasas de poda, ambas muy similares a las utilizados en este trabajo de tesis. Por otro lado, en \cite{Chen2007}, los autores demuestran cómo la posibilidad de que sea el usuario el encargado de la selección de atributos genera una ventaja con respecto al hecho de que sea el sistema mismo el encargado de dicha selección. Este hecho respalda el enfoque buscado en esta tesis, en el cual el experto humano guía la conversación y el proceso de selección de características.



%\begin{itemize}
%	\item En primer lugar, cualquier ítem de la lista de resultados, verifica los atributos seleccionados ya que la consulta para obtener los ítems resultado contiene esas restricciones.
%	\item Y en segundo lugar, a cada paso del diálogo, el sistema devuelve todos los ítems que verifiquen la selección de atributos establecida por el usuario.
%\end{itemize}
 
%Sucede una situación similar al hablar de otras métricas muy utilizadas como son MAE o RMSE. Estas métricas basadas en valoraciones no tienen cabida en el sistema desarrollado en esta tesis puesto que no existen valoraciones con la que el sistema trabaje. 


\section*{Propuesta Desarrollada}
\label{seccion:aplicacionDesarrollada}
\noindent
El objetivo ha sido abordar el problema de la alta dimensionalidad\index{problemas SR!dimensionalidad} en los SRs haciendo uso de los conjuntos de implicaciones\index{conjuntos de implicaciones}, a través de un proceso de selección de atributos por parte del usuario mediante el SR híbrido mencionado anteriormente. De esta forma, se ha conseguido reducir el número de pasos necesarios en el diálogo y gestionar favorablemente el problema de la dimensionalidad \cite{Benito-Picazo2017}.

Así, el sistema desarrollado se va a centrar principalmente en la primera fase de la recomendación, el filtrado. Para ello, partiendo del conjunto de implicaciones, utiliza la lógica \slfde\index{\slfde} \cite{Enciso2002} y, especialmente, el algoritmo del cierre \slfde \cite{Mora2012} como motor para facilitar y acelerar la recomendación. Gracias a la aplicación del cierre, el sistema reduce la sobrecarga de información a cada paso del diálogo filtrando aquellos atributos que resulten de la aplicación del cierre a las solicitudes del usuario, consiguiendo una reducción del número de pasos necesarios en el diálogo.

Se han realizado numerosas pruebas de aplicación para contrastar su validez. Entre ellas, destacan las pruebas utilizando información real sobre enfermedades y fenotipos, como se puede apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Además, la propuesta desarrollada, al igual que la gran mayoría de los SRs, casa con los conceptos de adaptabilidad y longevidad de los SRs ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo es necesario conocer el conjunto de atributos e implicaciones subyacente a los datos.

%La propuesta de solución realizada consiste en reducir los efectos del problema de la alta dimensionalidad mediante un proceso de selección de características guiado por el usuario (experto humano) dentro de un sistema conversacional \cite{Yu2016}. 
%el sistema se va a centrar principalmente en la primera fase de la recomendación, el filtrado. Para ello, una vez más se hará uso de una gestión inteligente de las implicaciones y de los conjuntos cerrados que va a permitir reducir sustancialmente el número de pasos necesarios en el diálogo entre el usuario y la aplicación para conseguir una recomendación adecuada en tiempo y forma. Para ello, se cuenta con el apoyo de la lógica \slfde\index{\slfde} introducida por los autores en \cite{Enciso2002} y en particular, se utilizará el algoritmo del cierre de atributos \slfde \cite{Mora2012} como núcleo de un marco de selección de atributos que será el que permita reducir el número de etapas del diálogo.

%Además, se han realizado numerosas pruebas de aplicación sobre la propuesta que se ha desarrollado para contrastar su validez. En concreto, se han realizado pruebas utilizando información real sobre enfermedades y fenotipos como se puede apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Además, la propuesta desarrollada, al igual que la gran mayoría de los SRs, permite actuar sobre diferentes \textit{datasets}\index{dataset} de forma que se puede utilizar el mismo procedimiento para mejorar las recomendaciones conversacionales en diversos entornos. Esta versatilidad es una característica notoria, ya que permite libertad de maniobra en el caso de que se quieran introducir ciertos cambios, o simplemente que los datos sean diferentes. En ese sentido, la propuesta de este trabajo casa con los conceptos de adaptabilidad y longevidad de los SRs ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo es necesario conocer el conjunto de atributos e implicaciones subyacente a los datos.


\section{Verificación de los Resultados}
\label{sec:verificacionResultados}
\noindent
Antes de entrar plenamente en el cuerpo de la tesis es necesario hacer una importante aclaración previa con la intención de indicar la manera de certificar la validez de los resultados obtenidos a lo largo de la tesis. 

Como se verá en los siguientes capítulos, la labor de investigación se ha centrado en actuar sobre conjuntos de implicaciones, y en ese sentido, para los experimentos realizados se ha contado con unos ficheros de entrada que contenían la información necesaria, y sobre ellos se han obtenido unos resultados. Ahora bien, la forma de verificar que esos resultados son correctos es la siguiente. 

En primer lugar y con respecto a los resultados que se presentan en cuanto a claves y generadores minimales, se han realizado numerosos ejercicios en papel intentando buscar casos límites donde la implementación pudiera no ser precisa y se ha comprobado que los resultados obtenidos en papel coincidían exactamente con los calculados por la máquina. Además, dado que para muchos de los experimentos, en los que se llegaban a calcular millones de nodos de un árbol, no era posible comprobar si cada uno de esos cálculos era correcto, para el caso concreto de los experimentos relacionados con claves minimales, la validez de los experimentos viene dada al haber cotejado los resultados con aquellos obtenidos sobre un amplio abanico de experimentos en trabajos anteriores \cite{Benito-Picazo2013PFC,Benito-Picazo2014TFM} donde su validez quedó demostrada. Además, la validez de los resultados se corrobora igualmente al alcanzar las mismas soluciones para diferentes métodos cuando cada uno de ellos hace un tratamiento de la información diferente con respecto al otro. En relación a los experimentos con SRs conversacionales, dado que los experimentos no alcanzan números tan costosos de verificar, la validez de los resultados puede demostrarse de forma más asequible siguiendo un desarrollo explícito en papel.

Adicionalmente, y con mayor énfasis en relación a los experimentos que han conllevado la utilización de recursos de supercomputación, cada uno de los experimentos se ha reproducido entre 30 y 50 veces, de forma que los resultados mostrados son fruto de un estudio estadístico posterior más amplio que permite identificar los resultados más fiables, tal y como se sugiere en \cite{Zobel1998,Goh10,Yang2012}.

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que permitieran evaluar el rendimiento de las pruebas de forma que se pudieran comparar unos métodos con otros. En el caso de los experimentos con claves y generadores minimales, cuando se plantea la idea de la comparación de resultados, lo primero que se pensó fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, se advirtió que este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar la magnitud del árbol y la cantidad de resultados redundantes que se obtienen (véase \cite{Benito-Picazo2014,Benito-Picazo2013PFC,Benito-Picazo2014TFM}. De esta forma, en el momento de que exista otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre se puede atender al tamaño del árbol y al número de cálculos redundantes, pudiendo defender si realmente es una mejora en el método o bien, en la ejecución debido a la arquitectura.

%\vspace{0.4cm}
A modo de resumen gráfico, la Figura \ref{figura:esquemaConceptual} muestra un esquema del camino que investigación que se ha seguido en el desarrollo de esta tesis doctoral, apoyado por las principales nociones y referencias.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.75\textheight]{esquemaConceptual.png}
	\end{center}
	\caption{Esquema del estado del arte y las contribuciones generadas.}
	\label{figura:esquemaConceptual}
\end{figure}

Para finalizar este capítulo, se incluye un último apartado donde se describe brevemente la estructura que presenta el documento incluyendo las publicaciones que avalan esta tesis.

%\vspace{2cm}
%\newpage{}
\section{Estructura de la Tesis}
\noindent
En este primer capítulo de introducción, se han fijado los puntos fundamentales de la tesis, como son: el marco de trabajo sobre el que se va a actuar, las técnicas que se utilizarán y los principales objetivos que se pretenden alcanzar. Concretamente, se ha estipulado la utilización de los conjuntos de implicaciones y las DFs como base del estudio sobre la que aplicar técnicas basadas en la lógica para mejorar el tratamiento de la información.

Tras la introducción, aparece el Capítulo \ref{cap:preliminares}, en el que se presentan: el conjunto de nociones principales de FCA para el trabajo realizado sobre generadores minimales y SRs, los aspectos fundamentales de bases de datos y DFs para la investigación sobre claves minimales, las lógicas de implicaciones y los métodos de razonamiento automático utilizados. %Tras estos dos primeros capítulos, ya se contará con el conocimiento previo necesario para abordar el resto de capítulos de la tesis.

A continuación, se presenta el Capítulo \ref{cap:clavesMinimales} en el que se presenta la primera contribución que avala este trabajo de investigación y que corresponde con el trabajo realizado en el campo de la búsqueda de las claves minimales. De forma general, este artículo presenta nuevos métodos para resolver el problema de la inferencia de claves minimales en esquema de datos basándose en la lógica \slfde y el uso de implicaciones. Además, se muestra el funcionamiento de esos métodos y las ventajas obtenidas al aplicar técnicas de computación paralela para poder aplicar los métodos sobre conjuntos de información de un tamaño tal que las técnicas secuenciales no son capaces de gestionar en cuanto a tiempo y recursos necesarios. %Para ello se presentan los resultados obtenidos para los experimentos en entornos de supercomputación\index{supercomputación}.

Seguidamente, se presenta el Capítulo \ref{cap:generadoresMinimales}, análogo al anterior pero esta vez para el tema referente a los generadores minimales \ref{cap:clavesMinimales}. Este capítulo presenta un segundo artículo en el cual se lleva a cabo un estudio de los métodos de producción de generadores minimales basados en la lógica y el tratamiento de implicaciones. Se comprueba las mejoras de rendimiento de los métodos al aplicar reducciones en el espacio de búsqueda basadas en estrategias de poda. Al igual que en el caso de las claves minimales, se presenta el funcionamiento de los métodos paralelos para poder tratar con conjuntos de información de tamaño considerable y se incluyen las pruebas realizadas en entornos de supercomputación.

Como último capítulo dedicado a las aplicaciones desarrolladas mediante la gestión de implicaciones se incluye el Capítulo \ref{cap:sistemasRecomendacion}. En este capítulo se presenta un novedoso trabajo en el que se desarrolla una aproximación al tratamiento del problema de la dimensionalidad en los SR. Mediante el uso de las implicaciones, la lógica \slfde y el algoritmo del cierre \slfd, se desarrolla un modelo de SR conversacional. Este sistema, es capaz de gestionar el problema de la dimensionalidad reduciendo la sobrecarga de información con la que el usuario debe enfrentarse a la hora de obtener una recomendación. Esta reducción se consigue mediante un filtrado de atributos guiado por la aplicación del cierre. Se demuestra su buen comportamiento mediante su evaluación sobre información real.

Finalmente, la tesis se cierra con el Capítulo \ref{cap:conclusiones} dedicado a recopilar las principales conclusiones obtenidas y a proponer caminos por los que seguir ahondando en la investigación en esta materia. Además, se incluye una relación de las referencias consultadas y los respectivos índices de términos, figuras y tablas.

En aras de la completitud, se incluyen como anexos finales aquellos artículos que han sido publicados a lo largo de este periodo de investigación, que si bien no se utilizan como respaldo para esta tesis doctoral, han sido la semilla y experiencia inicial a partir de la cual se han desarrollado los trabajos que actúan como aval.

En la Figura \ref{figura:esquemaTesis} se muestra de forma gráfica el contenido de la tesis y se contextualizan las contribuciones publicadas.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.8\textheight]{esquemaTesis.png}
	\end{center}
	\caption{Esquema de la estructura de la tesis y las publicaciones.}
	\label{figura:esquemaTesis}
\end{figure}


% =====================================================================
% =====================================================================
% =====================================================================