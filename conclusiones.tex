\pagestyle{empty}
\chapter{Conclusiones y Trabajos Futuros}
\label{cap:conclusiones}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}Fundamentalmente, se puede decir que esta tesis ha englobado dos vertientes principales dada su naturaleza dual. Por un lado, se ha realizado un profundo estudio de los métodos basados en la lógica para el tratamiento eficiente de la información utilizando los conjuntos de implicaciones que se verifican en un determinado dataset. Y por otro lado, se han realizado las tareas de investigación e implementación necesarias para poder llevar estos métodos teóricos a la práctica. 

Se ha investigado sobre tres áreas diferentes: claves minimales, generadores minimales y sistemas de recomendación. Para cada uno de ellos se han realizado multitud de experimentos que demuestran la utilidad y la validez del trabajo realizado. Asimismo, se hace un especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico, como el mercado empresarial.

A lo larog de la tesis se puede apreciar el hecho de que contar con una sólida teoría basada en la lógica y las matemáticas concede la base para la creación de métodos automatizados con los que poder afrontar el desarrollo de aplicaciones de ingeniería. De esta forma, se ha comprobado que existe una gran cantidad de información implícita en los datos que se suelen utilizar. El descubrimiento de toda esta información y su gestión inteligente es sin duda una clara oportunidad de investigación con una fuerte actividad y repercusión en la actualidad. Esta ha sido la intención principal a la hora de trabajar con FCA, los conjuntos de implicaciones y los conjuntos cerrados. Pasar de la teoría a la práctica y viceversa ha sido uno de los principales desafíos de esta tesis al pretender que estos conceptos se conviertan en una herramienta fructífera para la representación, gestión y análisis del conocimiento en situaciones reales.

Antes de entrar plenamente en el apartado de conclusiones, es necesario indicar la manera de certificar la validez de los resultados obtenidos a lo largo de la tesis. Como se ha mostrado en los capítulos anteriores, la labor de investigación se ha centrado en actuar sobre conjuntos de implicaciones, y en ese sentido, para los experimentos realizados se ha contado con unos ficheros de entrada que contenían la información necesaria, y sobre ellos se han obtenido unos resultados. 

Ahora bien, la forma de verificar que esos resultados son correctos es la siguiente. En primer lugar y con respecto a los resultados de claves y generadores minimales, se han realizado numerosos ejercicios en papel intentando buscar casos límites donde la implementación pudiera no ser precisa y se ha comprobado que los resultados obtenidos en papel coincidían exactamente con los calculados por la máquina. Además, dado que para muchos de los experimentos, en los que se llegaban a calcular millones de nodos de un árbol, no era posible comprobar si cada uno de esos cálculos era correcto, para el caso concreto de los experimentos relacionados con claves minimales, la validez de los experimentos viene dada al haber cotejado los resultados con aquellos obtenidos sobre un amplio abanico de ficheros utilizados en trabajos anteriores \cite{Benito-Picazo2013PFC,Benito-Picazo2014TFM} donde su validez quedó demostrada. Básicamente, la validez de los ejercicios más grandes se ha extrapolado de los resultados correctos obtenidos para los ejercicios más pequeños. Pero además, la validez de los resultados se corrobora igualmente al alcanzar las mismas soluciones para diferentes métodos cuando cada uno de ellos hace un tratamiento de la información diferente con respecto al otro. En relación a los experimentos con SR conversacionales, dado que los experimentos no alcanzan números tan costosos de verificar, la validez de los resultados puede demostrarse de forma más asequible siguiendo un desarrollo explícito en papel.

Y el otro punto que se quiere destacar es el siguiente. A lo largo de toda la tesis se ha hablado de trabajar sobre sistemas de implicaciones. No obstante, queda fuera del ámbito de esta tesis el procedimiento mediante el cual se obtiene esos elementos para un sistema de datos. Para ello, se recomienda al lector visitar \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. En el caso particular de esta tesis, la tarea del tratamiento de la información comienza una vez se ha extraído el conjunto de implicaciones de un sistema concreto.

Aclarados estos puntos, se alcanza ahora el último capítulo de la tesis, en el cual se recopilan las conclusiones más importantes alcanzadas como resultado del trabajo de investigación realizado. Seguidamente, cerrarán el capítulo una serie de tareas con las que continuar a partir de este punto y que se presentan como trabajos futuros.


\section*{Conclusiones}
\noindent
Como se ha mostrado anteriormente, conocer las claves es fundamental en cualquier modelo de datos. En este sentido, se ha presentado una serie de métodos que permiten averiguar el conjunto de claves a partir del conjunto de implicaciones y haciendo uso de métodos de razonamiento automatizado, en concreto, la lógica \slfd. Se ha investigado, pasando desde la teoría a la práctica, los diferentes métodos implementados haciendo uso del paradigma de Tableaux y se ha comprobado como, hasta donde se ha podido comprobar, los resultados obtenidos superan las aproximaciones existentes.

Por otro lado, enumerar todos los conjuntos cerrados y sus generadores minimales es un problema muy complejo pero esencial en varias áreas de conocimiento y una oportunidad para mostrar los beneficios de FCA cuando trabajamos para aplicaciones reales. Para abordar esta tarea, se han presentado dos métodos de poda para mejorar el rendimiento de la enumeración de los generadores minimales. Para ello, se ha hecho un uso intensivo de la lógica \slfde sobre conjuntos de implicaciones. Finalmente, se han diseñado, analizado y comprobado algoritmos diferentes (MinGen, MinGenPr, GenMinGen), mostrando claramente las mejoras alcanzadas por cada uno.

Tanto para claves minimales como para generadores minimales, se han desarrollado los códigos necesarios para poder actuar sobre grandes cantidades de información. No obstante, para resolver problemas reales donde la cantidad de información de entrada sea considerable, es incuestionable la necesidad de contar con unos recursos enormes tales como los que ha proporcionado el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga; sin ellos habría sido inviable haber podido realizar gran parte de las pruebas. De acuerdo con esto, es absolutamente necesario que las implementaciones tengan en cuenta el correcto uso de recursos de memoria; incluso para problemas pequeños, la cantidad de memoria que se puede necesitar puede dispararse sustancialmente.

Asimismo, el parecido y la cualidad de independencia que tiene cada nodo del árbol del Tableaux tanto para los métodos de claves minimales como para los de generadores minimales, ponen de manifiesto que la utilización de estrategias paralelas sea la mejor opción para abordar estos problemas. No obstante, el primer punto que se desea aclarar es que el concepto de \textit{paralelismo} que se ha utilizado se refiere a un paralelismo de tipo \textit{hardware}. Esto quiere decir que los beneficios obtenidos gracias al paralelismo se deben a la utilización de un conjunto de procesadores que se encargan de ir resolviendo cada uno de los subproblemas de forma simultánea. Por lo tanto, estas implementaciones no son un caso de desarrollo de código paralelo desde una visión más purista de programación, sino que es más acertado considerarlas como una aplicaciones basadas en una estrategia \textit{MapReduce}\cite{Dean2004}, que se ejecutan de forma paralela con la ayuda de recursos \textit{hardware}.

Para la mayoría de los casos, en relación a claves y generadores minimales, existe un serio inconveniente. En primera instancia es prácticamente imposible, por el momento, prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a obligar a realizar una serie de pruebas previas de forma que se pueda aproximar las necesidades que va a tener un determinado experimento. 

Para cada una de las implementaciones realizadas, existía la necesidad de establecer criterios que permitieran evaluar el rendimiento de las pruebas de forma que se pudieran comparar unos métodos con otros. En el caso de los experimentos con claves y generadores minimales, cuando se plantea la idea de la comparación de resultados, lo primero que se pensó fue la medición de los tiempos que necesitaba cada uno de los métodos para obtener los resultados. No obstante, se advirtió que este parámetro está íntimamente ligado a la arquitectura que estemos utilizando para ejecutar el experimento, lo cual hace que el resultado dependa en gran medida de los recursos que se están utilizando y no tanto de la calidad o eficiencia del propio algoritmo. En consecuencia, se oscurecía la utilidad teórica de los resultados obtenidos. Por tanto, se decidió contabilizar la magnitud del árbol y la cantidad de resultados redundantes que se obtienen. De esta forma, en el momento de que exista otro método con un código en cualquier otro lenguaje o utilizase recursos \textit{hardware} diferentes que desembocaran en una mejora del tiempo, siempre se puede atender al tamaño del árbol y al número de cálculos redundantes, pudiendo defender si realmente es una mejora en el método o bien, en la ejecución debido a la arquitectura.

En cuanto a los SR conversacionales, también son varias las conclusiones alcanzadas. La más importante es que, efectivamente, el tratamiento de la información realizado por medio de implicaciones y la lógica \slfde puede aplicarse con éxito en este campo de conocimiento. Este hecho ya quedaba patente por la existencia de trabajos en la literatura de SR que utilizan conceptos de FCA; el trabajo en esta tesis refuerza este hecho proponiendo además nuevos métodos con los que abordar problemas comunes en el campo y mejorar las aproximaciones existentes.

Concretamente, se presenta una novedosa aplicación del algoritmo de cierre \cierree para afrontar el problema de la sobrecarga de la información que a menudo presentan los SR. La solución propuesta propone un proceso conversacional de selección de atributos por parte del usuario. Este trabajo combina características de sistemas basados en contenido con sistemas basados en conocimiento mediante una gestión inteligente de las implicaciones teniendo como base el cierre \cierre. El sistema constituye un marco que puede utilizarse sobre diversos datasets y los resultados siguen siendo admisibles. Esta circunstancia constituye una gran contribución de este trabajo, pues ofrece la posibilidad de adaptarse a aplicaciones en diferentes áreas.

Otra conclusión a considerar es que existen numerosas técnicas de recomendación. De esta forma, una tarea fundamental en el desarrollo o en el análisis de un SR será identificar qué técnica es la más adecuada para su funcionamiento según el contexto de uso esperado. Del mismo modo, se ha comprobado que es excepcionalmente difícil abarcar todos los aspectos que involucran a un SR con la intención de evitar los diferentes problemas que pueden aparecer; es por tanto crucial, tener presente qué expectativas se pretenden cumplir. Del mismo modo, hay que tener en cuenta que son muy pocos los SR que basan su funcionamiento en una única técnica de recomendación, lo habitual es que sean sistemas híbridos buscando poder beneficiarse de las ventajas que ofrecen unas estrategias de recomendación y otras. La propuesta de esta tesis es un claro ejemplo de ello; el tipo de SR desarrollado mezcla las estrategias de recomendación basada en contenido, basada en conocimiento y conversacional, consiguiendo una beneficiosa sinergia.

Por otro lado, existen numerosas opciones a la hora de evaluar el funcionamiento de un SR. Esta es una conclusión razonable en tanto en cuanto el número de técnicas diferentes con las que trabajan los SR es igualmente alto. Por tanto, es fundamental decidir qué métricas son oportunas de aplicar dependiendo del tipo de SR que se desee evaluar, pues evidentemente habrá casos en los que una métrica no tenga cabida para un tipo de SR determinado. En consecuencia, es recomendable no pretender obtener valores óptimos en la mayoría de las métricas que existen. Hay que ser realista y darse cuenta de que es extremadamente difícil cumplir plenamente con unas y otras simultáneamente. La clave está en decidir cuál es el cometido principal de nuestro sistema, discernir las métricas de evaluación apropiadas y sacar las conclusiones pertinentes dentro de ese marco de actuación.

Para finalizar, se quiere destacar que el sistema desarrollado es capaz de ir más allá del ámbito académico o de investigación. Las pruebas realizadas sobre datasets reales demuestran la viabilidad y la utilidad que el sistema puede aportar en entornos empresariales. De esta forma, se apuesta de nuevo por una eficaz transmisión de conocimiento que acerque la empresa a la academia.



\section*{Trabajos Futuros}
\noindent
Los resultados obtenidos motivan una serie de direcciones importantes para futuras investigaciones.

Respecto a las aportaciones referentes al tema de claves y generadores minimales existen varios aspectos con los que continuar a partir de este trabajo de investigación. Se irán introduciendo uno a uno sin perjuicio de que el orden de aparición denote una mayor o menos importancia.

En el momento actual es prácticamente imposible prever cuál va a ser la magnitud que va a alcanzar la resolución del problema a la vista de la información de entrada. Esto va a constituir en la mayoría de los casos un serio inconveniente. A pesar de ello, gracias a la gran cantidad de pruebas realizadas, sí es posible aventurar que hay ciertas características, como el cardinal de la premisa o la conclusión en la implicación, que suelen vaticinar comportamientos similares. 

Tanto en claves minimales como en generadores minimales se ha podido apreciar que existe un elemento fundamental en el diseño de las implementaciones paralelas: el valor de corte o BOV. Recordemos que el BOV es el valor a partir del cual la ejecución secuencial del código paralelo termina y se forman los diferentes subproblemas que serán resueltos en paralelo. Aparece entonces el hecho de que establecer un valor de corte adecuado se convierte en una tarea realmente compleja. En primera instancia, hay que tener en cuenta la forma de expansión que tiene el Tableaux del experimento que estemos realizando y esto sólo se puede averiguar empíricamente. En definitiva, hay que buscar estrategias para encontrar un valor de corte tal que el aprovechamiento de los recursos computacionales sea óptimo.

Es necesario avanzar en el diseño de un \textit{benchmark} que tenga en cuenta los diferentes aspectos y la naturaleza de estos problemas y algoritmos de manera que se puedan dirigir las búsquedas de forma más eficiente.

Hay que profundizar en el hecho de que aumentar el número de cores para la resolución de un problema no siempre redunda en una mejora del rendimiento. Hay casos en los que aumentar los recursos utilizados puede ser incluso contraproducente como se ha demostrado en las publicaciones que avalan esta tesis \cite{Benito-Picazo2016,Benito-Picazo2018}.

La intención en el futuro es aplicar el cálculo de claves y generadores minimales en más situaciones donde se trate con datos reales, en especial a casos donde la información de entrada sea considerable, de forma que se pueda maximizar el rendimiento alcanzado gracias al paralelismo.

Otra tarea por la que continuar consiste en investigar cómo realizar un nuevo diseño de los códigos paralelos que permita establecer comunicación entre las diferentes resoluciones paralelas de un mismo problema de forma que se pueda mejorar la reducción de los cálculos redundantes. Para el caso concreto del cálculo de los generadores minimales, este mismo objetivo es muy importante, pues serviría para obtener una versión paralela del mejor método secuencial estudiado (GenMinGen). 

En relación a los SR conversacionales, aparecen dos aspectos interesantes a tener en cuenta en el futuro próximo. En primer lugar, sería muy valioso poder identificar desde un primer momento qué elementos del dataset presentan una importancia superior entre los demás, ya sea por aparecer con mayor frecuencia, ser único, tener relación con un mayor número de elementos del conjunto, etc. En segundo lugar y siguiendo la misma idea, sería esencial saber qué características del dataset (tamaño, escasez, etc.) son las más influyentes en el rumbo que toman las conversaciones con el usuario. Tener un mayor conocimiento de estos dos factores permitiría poder influir de manera positiva en la conversación y de esta forma, deparar en una mejor experiencia de usuario.

Hay aspectos de los SR que sería recomendable investigar si sería acertado incluir en el sistema conversacional; tal puede ser el caso de proporcionar explicaciones que justifiquen las recomendaciones que el usuario ha recibido. Este es un aspecto importante en un SR, ya que ayuda a mantener un mayor grado de confianza del usuario en los resultados generados por el sistema \cite{Sharma2016}.


% =====================================================================
% =====================================================================
% =====================================================================








